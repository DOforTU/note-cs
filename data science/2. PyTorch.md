# 2. PyTorch

## Content

- **2.1 Tensor (텐서)**
- **2.2 In-place 연산 및 메모리**
- **2.3 Polynomial regression (예제)**
- **2.4 Broadcasting (브로드캐스팅)**
- **2.5 신경망 구성 요소 및 분류**
- **2.6 Stable Softmax**
- **2.7 Gradient Accumulation (메모리 제약)**
- **2.8 Reproducibility (재현성)**

---

## Prerequisites

이 문서를 효과적으로 공부하기 위해 다음 개념들을 먼저 이해하고 오는 것을 권장합니다:

## Prerequisites
이 문서를 효과적으로 공부하기 위해 다음 개념들을 먼저 이해하고 오는 것을 권장합니다:

### [[Machine Learning Basics]]
- [[Loss Function]] - 손실 함수가 무엇이며 왜 필요한지
- [[Gradient]] - 그래디언트의 의미와 경사하강법
- [[Polynomial Regression]] - 다항 회귀의 개념과 원리

### [[Deep Learning Core Concepts]]
- [[Forward Pass]] - 순전파의 개념과 역할
- [[Backpropagation]] - 역전파 알고리즘의 원리
- [[Chain rule]] - 연쇄 법칙 (역전파의 수학적 기초)
- [[Autograd]] - PyTorch의 자동 미분 시스템

### Optimization Algorithms
- [[Optimizer(Data Science)]] - SGD, Adam 등 파라미터 업데이트 알고리즘

### [[Loss Function]] Types
- [[MSELoss]] - 평균 제곱 오차 (회귀 문제)
- [[Softmax]] - Softmax 함수의 역할 (분류 문제)
- [[LogSoftmax]] - Log-Softmax
- [[NegativeLogLikelihood]] - Negative Log Likelihood

### Neural Network Components
- [[Activation Function]] - 활성화 함수의 역할과 종류
- [[ReLU]] - ReLU 활성화 함수
- [[Sigmoid]] - Sigmoid 활성화 함수
- [[Dropout]] - 드롭아웃의 목적과 원리
- [[BatchNormalization]] - 배치 정규화

---

## 2.1 Tensor (텐서)

### 2.1.1 텐서의 정의 및 특징

텐서(Tensor)는 PyTorch에서 데이터를 표현하는 기본 단위로, 다차원 배열 형태의 자료구조입니다. NumPy의 `ndarray`와 유사하지만 두 가지 중요한 차이점이 있습니다. 첫째, 텐서는 GPU에서 실행될 수 있어 대규모 연산을 빠르게 처리합니다. 둘째, 텐서는 자동 미분([[Autograd]])을 지원하여 역전파를 자동으로 수행합니다.

### 2.1.2 텐서 초기화 방법

텐서를 생성하는 방법은 다양합니다:

```python
import torch

# 0~1 균등 분포 랜덤 값
x = torch.rand(2, 3)

# 모두 1로 채우기
x = torch.ones(2, 3)

# 모두 0으로 채우기
x = torch.zeros(2, 3)

# 표준 정규 분포 (평균 0, 분산 1)
x = torch.randn(2, 3)
```

`torch.randn(*size)`는 파라미터 초기화에 자주 사용되며, 표준 정규 분포에서 랜덤 값을 추출합니다. 괄호 안의 숫자들은 텐서의 shape을 결정하며, 원하는 차원 수만큼 인자를 전달할 수 있습니다.

### 2.1.3 텐서 속성

모든 텐서는 세 가지 중요한 속성을 가집니다:

```python
tensor = torch.rand(3, 4)

print(tensor.shape)   # torch.Size([3, 4]) - 텐서의 형태
print(tensor.dtype)   # torch.float32 - 데이터 타입
print(tensor.device)  # cpu 또는 cuda - 저장 위치
```

`.shape`는 텐서의 차원 크기를 나타내고, `.dtype`은 저장된 데이터의 자료형을, `.device`는 텐서가 CPU와 GPU 중 어디에 있는지를 알려줍니다.

### 2.1.4 인덱싱과 슬라이싱

텐서는 NumPy와 동일한 방식으로 인덱싱과 슬라이싱을 지원합니다:

```python
tensor = torch.rand(4, 4)

first_row = tensor[0]        # 첫 번째 행
first_col = tensor[:, 0]     # 첫 번째 열
last_col = tensor[..., -1]   # 마지막 열
```

콜론(`:`)은 해당 차원의 모든 요소를, 음수 인덱스는 뒤에서부터 세는 위치를 의미합니다.

### 2.1.5 `.item()` 메서드

`.item()` 메서드는 하나의 요소만 가진 텐서를 Python 숫자로 변환합니다:

```python
x = torch.tensor([3.14])
value = x.item()  # 3.14 (Python float)

# 여러 요소를 가진 텐서에는 사용 불가
y = torch.tensor([1, 2, 3])
# y.item()  # ValueError 발생!
```

주로 손실값(loss)처럼 단일 스칼라 값을 출력할 때 사용됩니다.

### 2.1.6 NumPy 연동

PyTorch 텐서와 NumPy 배열은 서로 쉽게 변환할 수 있습니다:

```python
import numpy as np

# NumPy → Tensor
n = np.array([1, 2, 3])
t = torch.from_numpy(n)

# Tensor → NumPy  
t = torch.ones(3)
n = t.numpy()
```

중요한 점은 `torch.from_numpy()`와 `.numpy()`가 데이터를 복사하지 않고 같은 메모리를 공유한다는 것입니다. 따라서 한쪽을 수정하면 다른 쪽도 영향을 받을 수 있습니다.

---

## 2.2 In-place 연산 및 메모리

### 2.2.1 In-place vs Out-of-place 연산

연산 방식에 따라 메모리 동작이 완전히 달라집니다. **In-place 연산**은 기존 메모리를 직접 수정하는 반면, **out-of-place 연산**은 새로운 메모리에 결과를 저장합니다. PyTorch에서 in-place 연산은 메서드 이름 끝에 언더스코어(`_`)가 붙거나 복합 할당 연산자(`+=`, `-=` 등)를 사용합니다.

```python
# In-place 연산
x += 1        # 기존 x 수정
x.add_(1)     # 기존 x 수정

# Out-of-place 연산
y = x + 1     # 새로운 텐서 y 생성
y = x.add(1)  # 새로운 텐서 y 생성
```

### 2.2.2 메모리 공유 시 연산의 차이

NumPy 배열과 텐서가 메모리를 공유할 때 연산 방식에 따라 결과가 달라집니다:

**In-place 연산의 경우:**

```python
n = np.array([1, 2, 3])
t = torch.from_numpy(n)

t += 1  # In-place 연산

print(n)  # [2 3 4] ← n도 변경됨!
print(t)  # tensor([2, 3, 4])
```

`t += 1`은 기존 메모리를 직접 수정하므로, 같은 메모리를 공유하는 `n`도 함께 변경됩니다.

**Out-of-place 연산의 경우:**

```python
n = np.array([1, 2, 3])
t = torch.from_numpy(n)

t = t + 1  # Out-of-place 연산

print(n)  # [1 2 3] ← n은 그대로!
print(t)  # tensor([2, 3, 4])
```

`t = t + 1`은 새로운 텐서를 생성하여 `t`가 그것을 가리키게 하므로, 원래 메모리는 변경되지 않습니다.

### 2.2.3 `requires_grad=True`일 때 In-place 연산 제약

그래디언트 계산이 필요한 텐서에 in-place 연산을 수행하면 에러가 발생합니다:

```python
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# x += 1  # RuntimeError 발생!
x = x + 1  # 정상 작동
```

**역전파를 위해서는 중간 값들이 보존되어야 하는데, in-place 연산은 이를 덮어쓰기 때문에 런타임 에러가 발생**합니다. PyTorch는 이러한 문제를 방지하기 위해 런타임 에러를 발생시킵니다.

### 2.2.4 `torch.no_grad()` 컨텍스트에서의 안전한 사용

`torch.no_grad()` 블록 내에서는 in-place 연산을 안전하게 사용할 수 있습니다:

```python
with torch.no_grad():
    param -= learning_rate * param.grad  # 안전함
```

이 블록 안에서는 그래디언트 추적이 비활성화되므로, 파라미터 업데이트 시 in-place 연산을 사용해도 문제가 없습니다. 경사 하강법에서 가중치를 업데이트할 때 이 패턴이 자주 사용됩니다.


---
## 2.3 Polynomial Regression (예제)

### 2.3.1 문제 정의

[[Polynomial Regression]] 예제는 PyTorch의 전체 학습 과정을 이해하는 데 매우 유용합니다. 목표는 사인 함수 $y = \sin(x)$를 3차 다항식 $\hat{y} = a + bx + cx^2 + dx^3$으로 근사하는 것입니다. 훈련 데이터는 $-\pi \leq x \leq \pi$ 범위에서 생성되며, $a, b, c, d$ 네 개의 파라미터를 학습합니다.

### 2.3.2 기본 학습 루프의 4단계

신경망 학습은 다음 네 단계를 반복하는 과정입니다:

```python
learning_rate = 1e-3

for t in range(4000):
    # 1단계: Forward pass (순전파)
    y_pred = a + b * x + c * x ** 2 + d * x ** 3
    loss = ((y_pred - y) ** 2).mean()
    
    # 2단계: Backward pass (역전파)
    loss.backward()
    
    # 3단계: Update weights (파라미터 업데이트)
    with torch.no_grad():
        a -= learning_rate * a.grad
        b -= learning_rate * b.grad
        c -= learning_rate * c.grad
        d -= learning_rate * d.grad
    
    # 4단계: Zero gradients (그래디언트 초기화)
    a.grad = None
    b.grad = None
    c.grad = None
    d.grad = None
```

[[Forward Pass]]에서 예측값과 손실을 계산하고, [[Backpropagation]]에서 그래디언트를 구한 뒤, 파라미터를 업데이트하고, 마지막으로 그래디언트를 초기화합니다.
### 2.3.3 `requires_grad=True` 설정

학습할 파라미터는 `requires_grad=True`로 설정하여 그래디언트를 계산할 수 있도록 해야 합니다:

```python
a = torch.randn((), dtype=dtype, requires_grad=True)
b = torch.randn((), dtype=dtype, requires_grad=True)
c = torch.randn((), dtype=dtype, requires_grad=True)
d = torch.randn((), dtype=dtype, requires_grad=True)
```

이 설정이 있어야 `loss.backward()`를 호출했을 때 해당 파라미터의 그래디언트가 계산됩니다. `torch.randn(())`은 스칼라 형태의 텐서를 생성합니다.

### 2.3.4 `loss.backward()` 역할 (= [[Backpropagation]])

`loss.backward()`를 호출하면 자동 미분이 작동하여 손실 함수를 각 파라미터로 미분한 값이 계산됩니다:

```python
loss.backward()  # 그래디언트 계산

# 이후 각 파라미터의 .grad에 그래디언트가 저장됨
print(a.grad)  # ∂loss/∂a
print(b.grad)  # ∂loss/∂b
```

이 메서드는 연쇄 법칙([[Chain rule]])을 자동으로 적용하여 복잡한 신경망에서도 모든 파라미터의 그래디언트를 효율적으로 계산합니다.

### 2.3.5 그래디언트 초기화의 필요성

그래디언트는 기본적으로 누적(accumulation)되므로 반드시 초기화해야 합니다:

```python
# 첫 번째 반복
loss.backward()  # a.grad = 0.5
a -= lr * a.grad

# 두 번째 반복 (초기화 안 했을 경우)
loss.backward()  # a.grad = 0.5 + 0.3 = 0.8 (누적됨!)
```

그래디언트를 초기화하지 않으면 이전 값이 계속 더해져서 잘못된 학습이 이루어집니다. `param.grad = None` 또는 `param.grad.zero_()`로 초기화할 수 있습니다.

### 2.3.6 `Optimizer` 사용

매번 수동으로 파라미터를 업데이트하는 대신 [[Optimizer(Data Science)]]를 사용하면 코드가 간결해집니다:

```python
optimizer = torch.optim.SGD([a, b, c, d], lr=1e-3)

for t in range(4000):
    y_pred = a + b * x + c * x ** 2 + d * x ** 3
    loss = ((y_pred - y) ** 2).mean()
    
    optimizer.zero_grad()  # 그래디언트 초기화
    loss.backward()        # 역전파
    optimizer.step()       # 파라미터 업데이트
```

`optimizer.zero_grad()`는 모든 파라미터의 그래디언트를 한 번에 초기화하고, `optimizer.step()`은 모든 파라미터를 자동으로 업데이트합니다.

### 2.3.7 `nn.MSELoss` 사용

PyTorch가 제공하는 [[Loss Function]]는 수치 안정성(numeric stability)이 향상되어 있습니다:

```python
criterion = torch.nn.MSELoss(reduction='mean')

for t in range(4000):
    y_pred = a + b * x + c * x ** 2 + d * x ** 3
    loss = criterion(y_pred, y)  # MSE 계산
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

직접 `((y_pred - y) ** 2).mean()`을 계산하는 것보다 라이브러리 함수를 사용하는 것이 권장됩니다. 내부적으로 오버플로우나 언더플로우를 방지하는 기법이 적용되어 있습니다.

> [[MSELoss]]란 평균 제곱 오차(Mean Squared Error)를 계산하는 손실 함수로, 회귀 문제에서 예측값과 실제값의 차이를 측정하는 데 사용됩니다.

### 2.3.8 `nn.Module` 클래스 구조

모델을 클래스로 정의하면 재사용성과 가독성이 크게 향상됩니다:

```python
class Polynomial3(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.a = torch.nn.Parameter(torch.randn(()))
        self.b = torch.nn.Parameter(torch.randn(()))
        self.c = torch.nn.Parameter(torch.randn(()))
        self.d = torch.nn.Parameter(torch.randn(()))
    
    def forward(self, x):
        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3
```

`__init__` 메서드에서는 파라미터를 `nn.Parameter`로 정의하여 학습 가능한 파라미터로 등록합니다. `forward` 메서드에서는 순전파 로직을 구현하며, 모델을 호출하면 자동으로 실행됩니다.

사용 예시:

```python
model = Polynomial3()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

for t in range(4000):
    y_pred = model(x)  # forward 자동 호출
    loss = criterion(y_pred, y)
    
    # optimizer가 gradient 초기화 및 파라미터 업데이트
    # loss function이 backpropagation 진행
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### 2.3.9 `Dataset`과 `DataLoader` 구조

대규모 데이터를 효율적으로 처리하기 위해 Dataset과 DataLoader를 사용합니다:

```python
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):
    def __init__(self, xs, ys):
        self.xs = xs
        self.ys = ys
    
    def __len__(self):
        return len(self.ys)
    
    def __getitem__(self, idx):
        return self.xs[idx], self.ys[idx]
```

Dataset 클래스는 반드시 `__len__`과 `__getitem__`을 구현해야 합니다. `__len__`은 데이터셋의 크기를, `__getitem__`은 주어진 인덱스의 샘플을 반환합니다.

DataLoader 사용:

```python
batch_size = 200
my_dataset = MyDataset(x, y)
data_loader = DataLoader(my_dataset, batch_size, shuffle=True)

for epoch in range(100):
    for _x, _y in data_loader:
        y_pred = model(_x)
        loss = criterion(y_pred, _y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

`batch_size`는 한 번에 처리할 샘플 수를 지정하고, `shuffle=True`는 매 에포크마다 데이터를 섞어서 모델이 데이터 순서를 학습하지 않도록 합니다.

---

## 2.4 Broadcasting (브로드캐스팅)

### 2.4.1 Broadcasting 규칙

Broadcasting은 서로 다른 shape을 가진 텐서 간의 연산을 자동으로 가능하게 하는 기능입니다. 두 텐서가 연산될 때 다음 규칙이 적용됩니다. 첫째, 차원 수가 다르면 작은 쪽의 앞에 크기 1인 차원을 추가합니다. 둘째, 각 차원에서 크기가 1이거나 같으면 broadcasting이 가능합니다. 셋째, 크기가 1인 차원은 다른 텐서의 해당 차원 크기로 확장됩니다.

### 2.4.2 차원 확장 예시

**예시 1: 스칼라와 벡터**

```python
x = torch.tensor([1, 2, 3])  # shape: (3,)
y = 10                        # 스칼라

result = x + y  # tensor([11, 12, 13])
# y가 [10, 10, 10]으로 확장됨
```

스칼라는 자동으로 벡터의 모든 요소에 더해집니다.

**예시 2: 2D와 1D 텐서**

```python
x = torch.ones(3, 4)  # shape: (3, 4)
y = torch.ones(4)     # shape: (4,)

result = x + y        # shape: (3, 4)
# y가 (1, 4)로 확장된 후 (3, 4)로 broadcasting
```

1차원 텐서 `y`는 먼저 `(1, 4)` 형태로 변환되고, 그 다음 첫 번째 차원이 3으로 확장되어 `(3, 4)`가 됩니다.

**예시 3: 복잡한 경우**

```python
x = torch.ones(2, 3, 4)  # shape: (2, 3, 4)
y = torch.ones(3, 1)     # shape: (3, 1)

result = x + y           # shape: (2, 3, 4)
# y가 (1, 3, 1)로 확장된 후 (2, 3, 4)로 broadcasting
```

### 2.4.3 실용 예제

**배치 연산:**

```python
# 배치 데이터: 32개 샘플, 각 10차원
batch = torch.randn(32, 10)

# 각 차원에 다른 가중치 적용
weights = torch.randn(10)

result = batch * weights  # shape: (32, 10)
# weights가 각 샘플에 자동으로 적용됨
```

Broadcasting을 사용하면 **반복문 없이 배치 전체에 연산을 효율적으로 적용**할 수 있습니다.

**정규화:**

```python
data = torch.randn(100, 5)  # 100개 샘플, 5개 특징

# 각 특징별 평균과 표준편차
mean = data.mean(dim=0)     # shape: (5,)
std = data.std(dim=0)       # shape: (5,)

# 정규화
normalized = (data - mean) / std  # shape: (100, 5)
# mean과 std가 자동으로 (100, 5)로 broadcasting
```

`mean`과 `std`는 `(5,)` shape이지만 broadcasting을 통해 `(100, 5)`로 확장되어 모든 샘플에 적용됩니다.

---
## 2.5 신경망 구성 요소 및 분류

### 2.5.1 Fashion MNIST 예제

Fashion MNIST는 10개 카테고리의 의류 이미지를 분류하는 데이터셋입니다. 각 이미지는 28×28 크기의 흑백 이미지이며, 티셔츠, 바지, 가방 등의 클래스로 구성됩니다. PyTorch는 `torchvision` 패키지를 통해 이 데이터셋을 쉽게 다운로드하고 사용할 수 있습니다.

```python
from torchvision import datasets
from torchvision.transforms import ToTensor

training_data = datasets.FashionMNIST(
    root="data", train=True, download=True, transform=ToTensor()
)
test_data = datasets.FashionMNIST(
    root="data", train=False, download=True, transform=ToTensor()
)

img, label = training_data[0]
print(img.shape)  # torch.Size([1, 28, 28])
print(label)      # 9 (Ankle boot)
```

### 2.5.2 `nn.Flatten` (차원 변환)

`nn.Flatten`은 다차원 텐서를 1차원으로 펼치는 레이어입니다. 이미지 데이터를 fully connected layer에 입력하기 전에 사용됩니다. `start_dim`과 `end_dim` 파라미터로 어느 차원부터 펼칠지 지정할 수 있으며, 기본값은 배치 차원을 제외한 모든 차원을 펼칩니다.

```python
m = nn.Flatten()

# 배치 크기 64, 채널 1, 28x28 이미지
X = torch.rand(64, 1, 28, 28)
output = m(X)
print(output.shape)  # torch.Size([64, 784])
# 1 * 28 * 28 = 784
```

### 2.5.3 `nn.Linear` (Fully Connected Layer)

`nn.Linear`는 완전 연결층(fully connected layer)으로, 선형 변환 $y = Wx + b$를 수행합니다. 첫 번째 인자 `in_features`는 입력 차원, 두 번째 인자 `out_features`는 출력 차원을 의미합니다. 내부적으로 가중치 $W$와 편향 $b$를 학습 가능한 파라미터로 가지고 있습니다.

```python
# 입력 20차원 → 출력 30차원
m = nn.Linear(20, 30)

# 배치 크기 128, 각 샘플은 20차원
input = torch.randn(128, 20)
output = m(input)
print(output.shape)  # torch.Size([128, 30])
```

### 2.5.4 `nn.Sequential` (층 연결)

`nn.Sequential`은 여러 레이어를 순차적으로 연결하는 컨테이너입니다. 코드를 간결하게 만들어주며, 입력이 순서대로 각 레이어를 통과합니다. `forward` 메서드를 따로 정의할 필요 없이 자동으로 순전파가 수행됩니다.

```python
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )
    
    def forward(self, x):
        x = self.flatten(x)
        return self.linear_relu_stack(x)
```

### 2.5.5 `nn.ReLU` 등 활성화 함수

활성화 함수는 신경망에 비선형성을 부여하여 복잡한 패턴을 학습할 수 있게 합니다. `nn.ReLU`는 가장 널리 사용되는 활성화 함수로, $f(x) = \max(0, x)$를 계산합니다. 음수는 0으로 만들고 양수는 그대로 통과시킵니다.

> ReLU 설명은 [[ReLU]] 문서를 참고

```python
# ReLU 활성화 함수
m = nn.ReLU()
input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
output = m(input)
print(output)  # tensor([0., 0., 0., 1., 2.])
```

다른 활성화 함수로는 `nn.Sigmoid`, `nn.Tanh`, `nn.LeakyReLU` 등이 있습니다.

> [[Sigmoid]]란?

### 2.5.6 `nn.CrossEntropyLoss` (분류 문제)

`nn.CrossEntropyLoss`는 다중 클래스 분류 문제에서 사용하는 손실 함수입니다. 내부적으로 [[LogSoftmax]]와 [[NegativeLogLikelihood]]를 결합한 것으로, 모델의 출력은 [[Softmax]]를 거치지 않은 로짓(logit) 값이어야 합니다. 수치 안정성을 위해 softmax를 직접 계산하지 않고 로짓을 입력받습니다.

```python
loss_fn = nn.CrossEntropyLoss()

# 모델 출력 (로짓): 배치 크기 3, 클래스 수 5
logits = torch.randn(3, 5)
# 정답 레이블
target = torch.tensor([1, 2, 4])

loss = loss_fn(logits, target)
```

### 2.5.7 `model.train()` vs `model.eval()`

신경망은 학습 모드와 평가 모드에서 다르게 동작합니다. `model.train()`은 학습 모드를 활성화하여 [[Dropout]]이나 [[BatchNormalization]] 같은 레이어가 정상적으로 작동하도록 합니다. `model.eval()`은 평가 모드를 활성화하여 Dropout을 비활성화하고 BatchNormalization을 고정시킵니다.

```python
def train(dataloader, model, loss_fn, optimizer):
    model.train()  # 학습 모드
    for X, y in dataloader:
        pred = model(X)
        loss = loss_fn(pred, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

def test(dataloader, model, loss_fn):
    model.eval()  # 평가 모드
    with torch.no_grad():  # 그래디언트 계산 비활성화
        for X, y in dataloader:
            pred = model(X)
            loss = loss_fn(pred, y)
```

평가 모드에서는 `torch.no_grad()`와 함께 사용하여 메모리를 절약하고 계산 속도를 높입니다.

### 2.5.8 `.to(device)` (GPU 이동)

텐서와 모델을 GPU로 이동시키려면 `.to(device)` 메서드를 사용합니다. GPU가 사용 가능한지 확인한 후 적절한 장치를 선택하고, 모델과 데이터를 모두 같은 장치로 이동시켜야 합니다. 장치가 다르면 연산 시 에러가 발생합니다.

```python
# 장치 설정
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

# 모델을 GPU로 이동
model = NeuralNetwork().to(device)

# 학습 루프에서 데이터도 같은 장치로 이동
for X, y in dataloader:
    X, y = X.to(device), y.to(device)
    
    pred = model(X)
    loss = loss_fn(pred, y)
```

---

## 2.6 Stable Softmax

### 2.6.1 Softmax 수치 불안정성 문제

[[Softmax]] 함수는 다중 클래스 분류에서 로짓을 확률로 변환하는 데 사용됩니다. 수식은 $\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}$입니다. 그러나 $e^{z_i}$를 직접 계산할 때 $z_i$ 값이 크면 오버플로우가, 작으면 언더플로우가 발생할 수 있습니다. 예를 들어 $e^{1000}$은 컴퓨터로 표현할 수 없는 매우 큰 수가 됩니다.

### 2.6.2 Log-sum-exp Trick

수치 안정성을 확보하기 위해 log-sum-exp trick을 사용합니다. 모든 로짓 값에서 최댓값을 빼주면 지수 연산의 결과가 적절한 범위 내에 유지됩니다. 수식은 $\text{softmax}(z_i) = \frac{e^{z_i - \max(z)}}{\sum_{j} e^{z_j - \max(z)}}$입니다. 이렇게 하면 지수 부분의 최댓값이 $e^0 = 1$이 되어 오버플로우를 방지할 수 있습니다.

```python
import torch

z = torch.tensor([1000., 1001., 1002.])

# 불안정한 방법 (오버플로우 발생)
# exp_z = torch.exp(z)  # inf 발생!

# 안정적인 방법
z_shifted = z - z.max()  # [-2., -1., 0.]
exp_z = torch.exp(z_shifted)
softmax = exp_z / exp_z.sum()
print(softmax)  # tensor([0.0900, 0.2447, 0.6652])
```

### 2.6.3 PyTorch 내장 함수 사용 권장

PyTorch는 수치적으로 안정적인 softmax 구현을 제공하므로 직접 구현하지 말고 내장 함수를 사용해야 합니다. `torch.nn.functional.softmax()`는 자동으로 log-sum-exp trick을 적용합니다. 또한 `nn.CrossEntropyLoss`는 내부적으로 안정적인 softmax 계산을 수행하므로, 모델 출력에 softmax를 직접 적용하지 않고 로짓 그대로 전달해야 합니다.

```python
import torch.nn.functional as F

# 올바른 방법: 내장 함수 사용
logits = torch.tensor([1000., 1001., 1002.])
softmax = F.softmax(logits, dim=0)
print(softmax)

# 분류 문제에서는 CrossEntropyLoss 사용
loss_fn = nn.CrossEntropyLoss()
# 모델은 로짓을 출력하고, softmax는 적용하지 않음
logits = model(x)  # softmax 없이 로짓만
loss = loss_fn(logits, target)  # 내부에서 안정적으로 처리
```

---
## 2.7 [[Gradient]] Accumulation (메모리 제약)

### 2.7.1 메모리 부족 시 해결 방법

대규모 신경망을 학습할 때 GPU 메모리가 부족한 경우가 자주 발생합니다. 배치 크기가 크면 더 안정적인 그래디언트를 얻을 수 있지만, 메모리가 부족하면 큰 배치를 한 번에 처리할 수 없습니다. Gradient Accumulation은 이 문제를 해결하는 기법으로, 큰 배치를 여러 개의 작은 배치로 나누어 처리합니다. 각 작은 배치에서 계산된 그래디언트를 누적한 후, 마지막에 한 번만 파라미터를 업데이트합니다.

### 2.7.2 작은 배치로 나누어 그래디언트 누적

일반적인 학습에서는 각 배치마다 그래디언트를 계산하고 바로 파라미터를 업데이트합니다. Gradient Accumulation에서는 여러 번의 forward/backward를 수행하면서 그래디언트를 누적하고, 정해진 횟수만큼 누적한 후에 한 번만 업데이트합니다.

```python
# 일반적인 학습
for X, y in dataloader:
    pred = model(X)
    loss = loss_fn(pred, y)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

```python
# Gradient Accumulation
accumulation_steps = 4

for i, (X, y) in enumerate(dataloader):
    pred = model(X)
    loss = loss_fn(pred, y)
    
    # 손실을 accumulation_steps로 나눔
    loss = loss / accumulation_steps
    loss.backward()  # 그래디언트 누적
    
    # accumulation_steps번째마다 업데이트
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

손실값을 `accumulation_steps`로 나누는 이유는 그래디언트가 누적되면서 값이 커지는 것을 방지하기 위함입니다.

### 2.7.3 `optimizer.zero_grad()` 호출 타이밍

일반적인 학습에서는 매 배치마다 `optimizer.zero_grad()`를 호출하여 그래디언트를 초기화합니다. Gradient Accumulation(그레디언트 누적)에서는 여러 배치의 그래디언트를 누적해야 하므로, 파라미터를 업데이트한 후에만 `optimizer.zero_grad()`를 호출합니다. 즉, `accumulation_steps`번째마다 한 번씩만 초기화합니다.

```python
accumulation_steps = 4

for i, (X, y) in enumerate(dataloader):
    pred = model(X)
    loss = loss_fn(pred, y) / accumulation_steps
    loss.backward()  # 그래디언트가 누적됨
    
    # accumulation_steps번 누적할 때마다 업데이트 및 초기화
    # = 4개 배치마다 한 번씩 업데이트 및 초기화
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()  # 여기서만 초기화
```

이 타이밍을 잘못 설정하면 그래디언트가 제대로 누적되지 않거나 이전 값이 남아있어 학습이 잘못될 수 있습니다.

### 2.7.4 Effective Batch Size 개념

Gradient Accumulation을 사용하면 실제로는 작은 배치로 학습하지만, 효과적으로는 큰 배치로 학습하는 것과 동일합니다. **Effective batch size**는 실제 배치 크기와 누적 횟수를 곱한 값입니다. 예를 들어 배치 크기가 32이고 `accumulation_steps=4`라면, effective batch size는 128입니다.

```python
batch_size = 32
accumulation_steps = 4
effective_batch_size = batch_size * accumulation_steps  # 128
```

|설정|값|
|:--|:--|
|실제 배치 크기|32|
|누적 횟수|4|
|Effective batch size|128|

Effective batch size가 128인 것은 한 번에 128개의 샘플로 학습하는 것과 동일한 효과를 냅니다. 이를 통해 메모리 제약 없이 큰 배치의 이점을 얻을 수 있습니다.

---
## 2.8 Reproducibility (재현성)

### 2.8.1 재현성이 중요한 이유

머신러닝과 딥러닝에는 많은 무작위 요소가 포함되어 있습니다. 가중치 초기화, 데이터 셔플링, Dropout 등이 모두 무작위로 동작하므로, 같은 코드를 실행해도 매번 다른 결과가 나올 수 있습니다. 이러한 무작위성을 제어하지 않으면 다음과 같은 문제가 발생합니다:

- **디버깅의 어려움**: 에러가 재현되지 않아 원인 파악이 힘듦
- **모델 비교 불가**: 어떤 모델이 더 나은지 공정한 비교가 어려움
- **신뢰할 수 없는 결과**: 논문 발표나 실험 결과를 다른 사람이 재현할 수 없음

재현성(Reproducibility)을 보장하면 실험 결과를 검증 가능하게 만들고, 공정한 비교를 가능하게 하며, 디버깅과 논문 발표에 필수적입니다.

> **주의사항**: PyTorch는 릴리즈 버전, 개별 커밋, 플랫폼에 따라 완전히 동일한 재현성을 보장하지 않습니다. 또한 동일한 seed를 사용하더라도 CPU와 GPU 실행 간에 결과가 다를 수 있습니다.

### 2.8.2 PyTorch의 무작위성 원인

PyTorch에서 무작위성이 발생하는 주요 원인은 다음과 같습니다:

1. **가중치 초기화**: 신경망의 시작점이 랜덤하게 설정됨
2. **DataLoader 셔플링**: 배치의 순서가 매번 다름
3. **비결정적 GPU 연산**: Atomic 연산, CuDNN 커널 등
4. **외부 라이브러리**: NumPy, Python의 random 모듈

### 2.8.3 Random Seed 설정

재현성을 보장하려면 모든 라이브러리의 random seed를 고정해야 합니다:

```python
import torch
import numpy as np
import random

SEED = 42

# Python random seed
random.seed(SEED)

# NumPy random seed
np.random.seed(SEED)

# PyTorch random seed (CPU)
torch.manual_seed(SEED)

# PyTorch random seed (GPU)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)  # 멀티 GPU 환경
````

`torch.manual_seed()`는 PyTorch의 random seed를 설정하여 동일한 난수 시퀀스를 생성하도록 합니다. GPU를 사용할 때는 `torch.cuda.manual_seed_all()`도 함께 설정해야 합니다.

### 2.8.4 Deterministic 알고리즘 설정

일부 PyTorch 연산은 성능을 위해 비결정적(non-deterministic) 알고리즘을 사용합니다. 완벽한 재현성을 위해서는 이러한 알고리즘을 비활성화해야 합니다:

```python
# 결정적 알고리즘 강제
torch.use_deterministic_algorithms(True)

# cuDNN의 결정적 모드 활성화
torch.backends.cudnn.deterministic = True

# cuDNN의 벤치마크 모드 비활성화
torch.backends.cudnn.benchmark = False
```

`torch.use_deterministic_algorithms(True)`는 모든 연산이 결정적으로 동작하도록 강제합니다. `cudnn.deterministic = True`는 cuDNN의 결정적 모드를 활성화하고, `cudnn.benchmark = False`는 매번 동일한 알고리즘을 선택하도록 합니다.

> **성능 트레이드오프**: 결정적 알고리즘은 비결정적 알고리즘보다 느립니다. 따라서 단일 실행 성능이 감소할 수 있습니다. 그러나 재현성은 실험, 디버깅, 회귀 테스트를 용이하게 하여 개발 시간을 절약할 수 있습니다.

### 2.8.5 DataLoader 무작위성 제어

DataLoader의 셔플링도 제어하여 매번 동일한 순서를 보장할 수 있습니다:

```python
from torch.utils.data import DataLoader

# Generator 생성 및 seed 설정
g = torch.Generator()
g.manual_seed(SEED)

# DataLoader에 generator 전달
loader = DataLoader(
    dataset, 
    batch_size=32,
    shuffle=True, 
    generator=g
)
```

Generator를 사용하면 DataLoader의 셔플링이 매 실행마다 동일하게 이루어집니다.

### 2.8.6 환경 변수 설정

GPU 행렬 연산의 결정적 동작을 위해서는 환경 변수도 설정해야 합니다:

```bash
# 터미널에서 설정
export CUBLAS_WORKSPACE_CONFIG=:4096:8
# 또는
export CUBLAS_WORKSPACE_CONFIG=:16:8
```

Python 코드 내에서 설정:

```python
import os
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
```

### 2.8.7 재현성 보장을 위한 통합 함수

모든 설정을 하나의 함수로 통합하면 편리합니다:

```python
import torch
import numpy as np
import random
import os

def set_seed(seed=42):
    """재현성을 위한 모든 random seed 설정"""
    
    # Python random seed
    random.seed(seed)
    
    # NumPy random seed
    np.random.seed(seed)
    
    # PyTorch random seed
    torch.manual_seed(seed)
    
    # CUDA random seed
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    # Deterministic 설정
    torch.use_deterministic_algorithms(True)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # 환경 변수 설정
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

# 사용법
set_seed(42)
```

### 2.8.8 재현성의 한계

완벽한 재현성에는 다음과 같은 한계가 있습니다:

- **일부 GPU 연산**: 결정적 구현이 없는 연산이 존재
- **성능 저하**: 결정적 알고리즘은 일반적으로 더 느림
- **환경 차이**: 정확한 재현성은 다음 환경에서 다를 수 있음
    - PyTorch 버전
    - GPU 모델
    - 드라이버 버전
    - 운영체제

### 2.8.9 재현성 체크리스트

완벽한 재현성을 위한 체크리스트:

**필수 설정:**

- Python의 `random.seed()` 설정
- NumPy의 `np.random.seed()` 설정
- PyTorch의 `torch.manual_seed()` 설정
- CUDA의 `torch.cuda.manual_seed_all()` 설정
- `torch.use_deterministic_algorithms(True)` 설정
- `torch.backends.cudnn.deterministic = True` 설정
- `torch.backends.cudnn.benchmark = False` 설정
- DataLoader의 `generator` 설정
- 환경 변수 `CUBLAS_WORKSPACE_CONFIG` 설정

**문서화 권장:**

- PyTorch 버전
- Python 버전
- CUDA 버전
- GPU 모델
- 운영체제

이 모든 설정을 적용하면 동일한 환경에서 코드를 실행할 때 항상 같은 결과를 얻을 수 있습니다.