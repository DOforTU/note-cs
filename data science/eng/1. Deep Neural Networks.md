# 1. Deep Neural Networks

## Contents

- **1.1 Feedforward Neural Networks**
- **1.2 Learning XOR**
- **1.3 Activation Functions**
- **1.4 Optimization & Training**
- **1.5 Calculus Fundamentals**
- **1.6 Output Units**

---

## 1.1 Feedforward Neural Networks

Feedforward Neural Networks, sometimes called Multi-Layer Perceptrons (MLPs), are the most basic form of neural networks.

### 1.1.1 Components and Information Flow

A feedforward neural network has a structure where multiple layers of functions are connected like a chain to form one large function.

**Main Components**:

1. **Input Layer:** The layer where data first enters.
2. **Hidden Layers:** There can be zero or more hidden layers.
3. **Output Layer:** The layer that produces the final result $(\hat{y})$.

Multiple layers $f^{(1)}, f^{(2)}, \dots, f^{(l)}$ are connected sequentially to create the final model $f(\mathbf{x})$. This can be expressed as: $f(\mathbf{x}) = f^{(l)}(\cdots f^{(2)}(f^{(1)}(\mathbf{x})))$.

**Information Flow:** As the name suggests, information always flows in one direction from input to output ('feedforward'). Since there are no feedback or recurrent connections in the network, information travels in only one direction.

### 1.1.2 Relationship with Other Learning Algorithms

When feedforward neural networks have no hidden layers, they become equivalent to other well-known linear learning algorithms.

- **Linear Regression:** A feedforward neural network with no hidden layers and a single linear output unit is the same as linear regression.
- **Logistic Regression:** A feedforward neural network with no hidden layers and a single sigmoid output unit is the same as logistic regression.

### 1.1.3 Key Differences in Training

To train a neural network, you need to choose an appropriate cost function and determine how to represent the output and hidden units.

The biggest difference between other linear models and feedforward neural networks in the training process is the **shape of the cost function**.

1. **Nonconvex Cost Function Due to Nonlinearity:** The nonlinearity introduced in neural networks makes the cost function nonconvex.
2. **Difficulty in Optimization:** Because the cost function is nonconvex, there can be multiple local optima, and there is no guarantee that we can find the global optimum we ultimately want.

---

## 1.2 Learning XOR

The XOR (exclusive OR) problem is the most basic yet important example when explaining the need for neural networks, especially feedforward neural networks. XOR is a logic function where the input data should produce results: $(0, 0) \to 0$, $(0, 1) \to 1$, $(1, 0) \to 1$, $(1, 1) \to 0$.

### 1.2.1 Limitations of Linear Models and the Need for New Feature Space

A single linear boundary alone cannot solve the XOR problem. In other words, for a linear model to solve the XOR problem, it absolutely needs a **'different feature space'** where the linear model can work, not the original feature space.

Neural networks play the role of automatically creating this 'new feature space'.

### 1.2.2 Introduction of Hidden Layers

To solve the XOR problem, we use a very simple form of neural network. This neural network includes one hidden layer, and this hidden layer contains two hidden features $\mathbf{h} = (h_1, h_2)$.

This multi-layer structure is expressed as multiple layers of functions connected like a chain: $$\hat{y} = f^{(2)}(f^{(1)}(\mathbf{x}))$$

### 1.2.3 Problems with Linear Activation Functions

If the first layer $f^{(1)}$ of the neural network uses a linear activation function, that is, in the form $\mathbf{h} = f^{(1)}(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{c}$, a serious problem occurs.

The output $\hat{y}$ takes $\mathbf{h}$ as input and performs a linear operation again: $$\hat{y} = f^{(2)}(\mathbf{h}) = \mathbf{w}^T\mathbf{h} + b$$

Combining these two equations simplifies to: $$\hat{y} = \mathbf{w}^T(\mathbf{W}\mathbf{x} + \mathbf{c}) + b = (\mathbf{w}^T\mathbf{W})\mathbf{x} + (\mathbf{w}^T\mathbf{c} + b)$$ $$= \mathbf{w}'^T \mathbf{x} + b'$$

In the end, no matter how deep you stack the layers, if all functions are linear, the entire model is **reduced to a simple linear model** of the form $\mathbf{w}'^T \mathbf{x} + b'$. Therefore, to solve nonlinear problems, we must use a nonlinear function $g$ in the hidden layer.

### 1.2.4 Hand-Coded Example Using Nonlinear Function (ReLU)

To actually solve the XOR problem, we need to apply a nonlinear function $g$ when calculating the hidden features $\mathbf{h}$: $$\mathbf{h} = g(\mathbf{W}\mathbf{x} + \mathbf{c})$$ The final output is: $$\hat{y} = \mathbf{w}^T f^{(1)}(\mathbf{x}) + b = \mathbf{w}^T g(\mathbf{W}\mathbf{x} + \mathbf{c}) + b$$

Here, we can use ReLU (Rectified Linear Unit) as an example of the nonlinear function $g$. ReLU is defined as $g(z) = \max(0, z)$.

**Hand-Coded Parameter Example**: By manually setting specific weights and biases (hand-coded solution), we can perfectly derive XOR results.

|Parameter|Value|
|:--|:--|
|**W**|$\begin{pmatrix} 1 & 1 \ 1 & 1 \end{pmatrix}$|
|**c**|$\begin{pmatrix} 0 \ -1 \end{pmatrix}$|
|**w**|$\begin{pmatrix} 1 \ -2 \end{pmatrix}$|
|**b**|$0$|
|**Activation Function**|$g(z) = \max(0, z)$ (ReLU)|

**Data Input (XOR)**: $$\mathbf{x}_1 = \begin{pmatrix} 0 \ 0 \end{pmatrix}, \mathbf{x}_2 = \begin{pmatrix} 0 \ 1 \end{pmatrix}, \mathbf{x}_3 = \begin{pmatrix} 1 \ 0 \end{pmatrix}, \mathbf{x}_4 = \begin{pmatrix} 1 \ 1 \end{pmatrix}$$

**Calculation Example (Intermediate Step $\mathbf{W}\mathbf{x} + \mathbf{c}$)**:

- $\mathbf{W}\mathbf{x}_1 + \mathbf{c} = \begin{pmatrix} 0 \ -1 \end{pmatrix}$
- $\mathbf{W}\mathbf{x}_4 + \mathbf{c} = \begin{pmatrix} 2 \ 1 \end{pmatrix}$

**After ReLU Application (Hidden Features $\mathbf{h}$)**:

- $g(\mathbf{W}\mathbf{x}_1 + \mathbf{c}) = g\begin{pmatrix} 0 \ -1 \end{pmatrix} = \begin{pmatrix} 0 \ 0 \end{pmatrix}$
- $g(\mathbf{W}\mathbf{x}_2 + \mathbf{c}) = \begin{pmatrix} 1 \ 0 \end{pmatrix}$
- $g(\mathbf{W}\mathbf{x}_3 + \mathbf{c}) = \begin{pmatrix} 1 \ 0 \end{pmatrix}$
- $g(\mathbf{W}\mathbf{x}_4 + \mathbf{c}) = g\begin{pmatrix} 2 \ 1 \end{pmatrix} = \begin{pmatrix} 2 \ 1 \end{pmatrix}$

**Final Results ($\mathbf{w}^T \mathbf{h} + b$)**: Through this process, calculating the final output gives us the correct XOR answers:

- Output for $\mathbf{x}_1$: $0$
- Output for $\mathbf{x}_2$: $1$
- Output for $\mathbf{x}_3$: $1$
- Output for $\mathbf{x}_4$: $0$

By introducing hidden layers with nonlinear activation functions, neural networks can create a new feature space that solves the XOR problem, which was not linearly separable.

---

## 1.3 Activation Functions

Activation functions are key elements that give nonlinearity to the hidden layers of neural networks, allowing them to learn complex functions and go beyond the limits of linear models. The choice of activation function can significantly affect learning speed and performance.

### 1.3.1 Sigmoid

The sigmoid function was widely used in early neural networks.

|Feature|Description|
|:--|:--|
|**Formula**|$g(z) = \frac{1}{1 + e^{-z}}$|
|**Output Range**|Compresses numbers to the range [0, 1].|

**Problems**: The sigmoid function causes several problems during training and is rarely used nowadays.

1. **Gradient Killing:** Kills gradients when saturated.
2. **Not Zero-centered:** Output values are not centered around zero.
3. **Computational Cost:** Requires $\exp()$ operation, which is somewhat computationally expensive.

### 1.3.2 Tanh (Hyperbolic Tangent)

The tanh function is an improved version of the sigmoid function that addresses the centering problem.

|Feature|Description|
|:--|:--|
|**Formula**|$g(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$|
|**Output Range**|Compresses numbers to the range [-1, 1].|

**Problems**: Like sigmoid, tanh also has the problem of killing gradients when the values become extremely large or small and saturate.

### 1.3.3 ReLU (Rectified Linear Unit)

ReLU is the most widely used activation function in modern deep learning.

|Feature|Description|
|:--|:--|
|**Formula**|$g(z) = \max(0, z)$|

**Advantages**:

1. **Non-saturation:** Does not saturate in the positive (+) region.
2. **Efficiency:** Very computationally efficient.
3. **Convergence Speed:** Converges much faster than sigmoid or tanh in actual training.

**Disadvantages**:

1. **Not Zero-centered:** Output values are not centered around zero.
2. **Dying ReLU Problem:** In the $z < 0$ region, the gradient becomes zero, which can cause weight updates to stop.

### 1.3.4 LeakyReLU and Parametric ReLU (PReLU)

LeakyReLU and PReLU were introduced to address ReLU's drawback (the gradient becoming zero when $z<0$).

|Type|Formula|Features|
|:--|:--|:--|
|**LeakyReLU**|$g(z) = \max(0.01z, z)$|Allows a very small slope in the negative region.|
|**PReLU**|$g(z) = \max(\alpha z, z)$ (where $\alpha$ is a learnable parameter)|$\alpha$ is set as a parameter to be learned.|

**Common Advantages**:

1. Does not saturate.
2. Computationally efficient.
3. Converges much faster than sigmoid/tanh in real environments.
4. Gradients will not die.

### 1.3.5 ELU (Exponential Linear Unit)

ELU is a function that attempts to keep ReLU's advantages while making the output value distribution closer to zero.

**Key Features**:

1. Has **all the advantages of ReLU**.
2. Output mean is closer to zero mean.
3. The negative saturation regime adds robustness to noise compared to LeakyReLU.
4. However, requires $\exp()$ operation.

---

## 1.4 Optimization & Training

To use neural networks effectively, an optimization process that minimizes the complex nonlinear cost function is essential. This process is a key step where the model learns from given data and adjusts parameters $\mathbf{\theta}$ to improve actual prediction ability.

### 1.4.1 Overview of the Training Process

Neural network training starts by initializing parameters $\mathbf{\theta}$ with small random numbers. Then, the following two steps are repeated until a stopping condition is reached (e.g., when the error becomes very small).

1. **Forward Propagation:** Propagate input data $\mathbf{x}$ from the input to the output direction of the neural network to calculate the predicted value $\hat{y}$.
2. **Backpropagation:** Calculate the difference (cost) between the calculated predicted value and the actual answer $y$, and propagate this cost backward to update the parameters.

### 1.4.2 Cost Function and Risk Minimization

The goal of training is to minimize the total cost $J(\mathbf{\theta})$ defined through the loss function $L$.

- **Cost Function:** Defined as the average of individual losses $L(y_i, \hat{y}_i)$ for all samples in the training dataset $D$. $$J(\mathbf{\theta}) = \frac{1}{n} \sum_{\mathbf{x}_i, y_i \in D} L(y_i, \hat{y}_i)$$
    
- **Empirical Risk Minimization (ERM):** Aims to minimize error on the observed training dataset $D$. This is the process of minimizing the cost function $J(\mathbf{\theta})$. However, ERM can be vulnerable to overfitting problems where the model only memorizes the training data when the model's representational capacity is large.
    
- **Structural Risk Minimization:** Uses methods (e.g., L1/L2 regularization) that control the model's representational capacity (capacity, complexity) while minimizing empirical risk to prevent overfitting.
    

### 1.4.3 Gradient Descent

The basic method for updating parameters is gradient descent. Parameter $\mathbf{\theta}$ moves by $\epsilon$ along the gradient (slope) of the cost function $J(\mathbf{\theta})$ to find the minimum point. $$\mathbf{\theta} \coloneqq \mathbf{\theta} - \epsilon \nabla_{\mathbf{\theta}} J(\mathbf{\theta})$$ Here, $\epsilon > 0$ is the learning rate.

#### 1.4.3.1 Batch Gradient Descent

Uses the entire training dataset (the entire training set, batch) when calculating the gradient.

- **Advantages:** Accurate and simple.
- **Disadvantages:** When the training dataset is large, the computational cost of each iteration is very high.

#### 1.4.3.2 Stochastic Gradient Descent (SGD)

Instead of using the entire dataset, calculates the gradient using only a subset of data called a minibatch.

- **Minibatch:** A subset of $m$ instances selected from the training dataset (typically $m=16, 32, 64, \dots$).
- **Advantages:** The computation time per iteration is related only to the minibatch size $m$ regardless of the total dataset size, so it's much faster on large datasets.
- **Disadvantages:** Since the cost function varies depending on the minibatch, gradient estimates are very noisy, and there's a need to gradually decrease the learning rate over time.

### 1.4.4 Key Terms Related to SGD

|Term|Definition|
|:--|:--|
|**Epoch**|Processing the entire training dataset once.|
|**Minibatch**|The number of training instances used for a single parameter update.|
|**Iteration**|One minibatch processing for parameter update.|

### 1.4.5 Backpropagation

Backpropagation is the process of applying the **chain rule** to neural network training. Using the cost calculated through forward propagation, it efficiently calculates the gradient (partial derivative value) of each parameter by propagating backward from the output layer to the input layer. This is a core mathematical method for updating weights in complex multi-layer neural networks.

### 1.4.6 Learning Rate Improvement Algorithms

A major problem with gradient descent-based algorithms is that the learning rate $\epsilon$ significantly affects performance. To solve this, methods that decrease the learning rate over time or 'adaptive learning rate' methods that adjust the learning rate for each parameter have been developed.

|Algorithm|Features|
|:--|:--|
|**Momentum**|Accumulates an exponentially weighted moving average of past gradients and continues to move in that direction.|
|**AdaGrad**|Adjusts the learning rate of individual parameters according to the accumulated value of past squared gradients. Gives larger learning rates to parameters with small historical gradient values. However, the learning rate can become too small as iterations increase.|
|**RMSProp**|A modified version of AdaGrad that accumulates past squared gradients as an exponentially weighted moving average, discarding old records.|
|**Adam (ADAptive Moments)**|An algorithm that combines RMSProp and momentum.|

---

## 1.5 Calculus Fundamentals

In deep learning, the process of finding the parameters (weights and biases) that make neural networks work best, that is, optimization, is basically the process of descending along the gradient of the cost function. The core mathematical tool needed to calculate this gradient is calculus, especially the concepts of derivative and gradient.

### 1.5.1 Slope and Tangent

**Slope:** The slope of a straight line is calculated as the ratio of vertical change to horizontal change between two points ("rise over run") $\left(\frac{\Delta y}{\Delta x}\right)$. This is the value that represents the degree of change that we commonly know.

**Tangent Line:** Intuitively, a tangent line is a straight line that touches a given point on a curve **'just grazing by'**. The slope of this tangent line is connected to the concept of derivative.

### 1.5.2 Derivative

The derivative of a function $f$ is the core of calculus.

- **Definition:** The derivative is the slope of the tangent line to the function graph at a given input value.
- **Rate of Change:** The derivative is often described as the instantaneous rate of change, which means the ratio of the instantaneous change in the dependent variable to the instantaneous change in the independent variable.
- **Mathematical Definition:** The derivative at point $a$ is defined using a limit. $$f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}$$
- **Notation:** Can be written as $f'(a)$, $\frac{d}{dx} f(a)$, or $\frac{df}{dx}(a)$.

### 1.5.3 Rules of Differentiation

To differentiate complex functions, several key rules are used.

1. **The Power Function Rule:** For a function of the form $y = f(x) = ax^n$, $\frac{dy}{dx} = nax^{n-1}$.
2. **Linearity:** When $h(x) = af(x) + bg(x)$, then $h'(x) = af'(x) + bg'(x)$.
3. **Chain Rule:** Used when functions are composed. When $z = g(f(x))$ or $y = f(x)$ and $z = g(y)$, it's calculated as $\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}$. This chain rule is used as a core in neural network backpropagation.

### 1.5.4 Partial Derivative

This concept is used when a function has multiple variables.

- **Definition:** In a multivariate function, differentiating with respect to only a specific variable while holding other variables constant.
- **Notation:** When partially differentiating function $f(x, y, \dots)$ with respect to $x$, it's written as $\frac{\partial}{\partial x} f(\mathbf{a})$ or $\frac{\partial f}{\partial x}(\mathbf{a})$. For example, if $z = f(x, y, \dots)$, it's written as $\frac{\partial z}{\partial x}$.

### 1.5.5 Gradient

Gradient is the most important concept that determines the direction of neural network optimization.

- **Definition:** Defined for a scalar-valued function $f: \mathbb{R}^n \to \mathbb{R}$, it's a vector whose elements are all partial derivative values.
- **Vector Structure:** At $\mathbf{p} = (x_1, \dots, x_n)$, the gradient $\nabla f(\mathbf{p})$ is defined as: $$\nabla f (\mathbf{p}) = \begin{pmatrix} \frac{\partial f}{\partial x_1}(\mathbf{p}) \ \vdots \ \frac{\partial f}{\partial x_n}(\mathbf{p}) \end{pmatrix}$$
- **Meaning:** The gradient $\nabla f (\mathbf{p})$ at a point $\mathbf{p}$ tells us **the direction and rate at which the function $f$ increases most rapidly** at that point. In gradient descent, we move in the opposite direction to minimize the cost.
- **Rules:** Linearity and chain rule of differentiation also apply to gradients.

---

## 1.6 Output Units

Output units are located at the very last stage of the neural network structure and play the role of transforming the features calculated from the last hidden layer $\mathbf{h}$ to match the format of the target task we actually want to solve, producing the final result $\hat{y}$. In other words, they provide the final transformation in the form $\hat{y} = f^{(l)}(\mathbf{h})$.

The choice of output unit depends on the type of problem you want to solve (regression, binary classification, multi-class classification).

### 1.6.1 Linear Units

Linear units are mainly used for **regression** problems.

|Feature|Description|
|:--|:--|
|**Purpose**|Regression problems where the target $y$ has real values ($y \in \mathbb{R}$).|
|**Formula**|Output as a linear transformation of the last hidden layer $\mathbf{h}$: $\hat{y} = \mathbf{w}^T\mathbf{h} + b$.|
|**Note**|Sometimes used to generate the mean parameter of a normal distribution.|

### 1.6.2 Sigmoid Units

Sigmoid units are used for **binary classification** problems.

|Feature|Description|
|:--|:--|
|**Purpose**|Binary classification problems where the target $y$ has values in ${0, 1}$.|
|**Formula**|Outputs the probability that $y=1$: $\hat{y} = P(y=1\|\mathbf{x}) = \sigma(z) = \frac{1}{1 + e^{-z}}$, where $z = \mathbf{w}^T\mathbf{h} + b$.|
|**Note**|As input $z$ approaches infinity, $\hat{y}$ saturates to 1, and as $z$ approaches negative infinity, $\hat{y}$ saturates to 0.|

### 1.6.3 Softmax Units

Softmax units are used for **multi-class classification** problems.

|Feature|Description|
|:--|:--|
|**Purpose**|Multi-class classification problems where the target $y$ belongs to one of the classes ${1, 2, \dots, c}$.|
|**Output Format**|Output as a probability vector in the form $\hat{\mathbf{y}} = (\hat{y}_1, \dots, \hat{y}_c)$, where each $\hat{y}_k$ represents the probability that $y=k$.|
|**Formula**|First calculate linear transformation $\mathbf{z} = (z_1, \dots, z_c) = \mathbf{W}^T\mathbf{h} + \mathbf{b}$, then apply this formula for each $k$:$$\hat{y}_k = \text{softmax}(\mathbf{z})_k = \frac{\exp(z_k)}{\sum_{j=1}^c \exp(z_j)}$$|
|**Note**|The sum of all class probabilities must be 1 ($\sum_{k=1}^c \hat{y}_k = 1$).|
