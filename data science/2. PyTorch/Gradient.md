# Gradient

## 정의

**그래디언트(Gradient)** 는 다변수 함수의 모든 편미분을 벡터로 모은 것입니다. 함수가 가장 빠르게 증가하는 방향과 그 변화율을 나타냅니다.

1변수 함수의 미분: $$f'(x) = \frac{df}{dx}$$

다변수 함수의 그래디언트: $$\nabla f(x_1, x_2, ..., x_n) = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}\right]$$

머신러닝의 [[Loss Function]]은 수많은 파라미터를 가진 다변수 함수입니다. 예를 들어 신경망에 천만 개의 파라미터가 있다면, 그래디언트는 천만 차원의 벡터가 됩니다. 각 파라미터에 대한 편미분을 하나씩 모아서 이 벡터를 구성합니다.

---

## 기하학적 의미

Gradient를 이해하는 가장 좋은 방법은 산을 등반하는 상황을 생각하는 것입니다. 어떤 지점에 서 있을 때, 그래디언트는 두 가지 정보를 제공합니다:

- **방향**: 함수값이 가장 빠르게 증가하는 방향
- **크기**: 그 방향으로의 변화율 (경사의 가파른 정도)

간단한 예시를 봅시다. $f(x, y) = x^2 + y^2$라는 함수가 있고, 점 $(1, 2)$에서 그래디언트를 계산하면:

$$\nabla f(1, 2) = [2 \cdot 1, 2 \cdot 2] = [2, 4]$$

벡터 $[2, 4]$는 $(1, 2)$ 지점에서 함수값을 가장 빠르게 증가시키려면 $x$ 방향으로 2, $y$ 방향으로 4의 비율로 움직여야 한다는 의미입니다. 벡터의 크기 $\sqrt{2^2 + 4^2} \approx 4.47$은 그 방향으로 얼마나 가파른지를 나타냅니다.

등고선 지도를 본 적이 있다면, 그래디언트는 항상 등고선에 수직으로 향한다는 사실을 기억하세요. 그리고 머신러닝에서 중요한 것은 그래디언트의 **반대 방향**(-∇f)입니다. 왜냐하면 우리는 산 정상으로 올라가는 것이 아니라 가장 낮은 골짜기, 즉 손실의 최솟값을 찾아야 하기 때문입니다.

---

## 머신러닝에서의 역할

머신러닝의 궁극적인 목표는 [[Loss Function]]을 최소화하는 것입니다. 하지만 수백만 개의 파라미터를 가진 신경망에서 어떻게 최솟값을 찾을 수 있을까요? 모든 가능한 파라미터 조합을 시도해볼 수는 없습니다.

손실 함수 $L$에 대한 파라미터 $w$의 그래디언트 $\frac{\partial L}{\partial w}$를 계산하면, 다음을 알 수 있습니다:

- $\frac{\partial L}{\partial w} > 0$: $w$를 증가시키면 손실이 증가 → $w$를 감소시켜야 함
- $\frac{\partial L}{\partial w} < 0$: $w$를 증가시키면 손실이 감소 → $w$를 증가시켜야 함
- $\frac{\partial L}{\partial w} = 0$: 최솟값에 도달했거나 안장점

그래디언트는 파라미터를 어느 방향으로 조정해야 손실이 줄어드는지 알려주는 나침반 역할을 합니다. 양수라면 파라미터를 줄이고, 음수라면 늘리면 됩니다. 이것이 바로 경사 하강법의 핵심 아이디어입니다.

---

## 경사 하강법 (Gradient Descent)

경사 하강법은 그래디언트의 반대 방향으로 반복적으로 이동하여 손실의 최솟값을 찾는 알고리즘입니다.

**알고리즘 단계:**

1. 파라미터를 랜덤하게 초기화
2. 현재 위치에서 손실의 그래디언트 계산
3. 그래디언트의 반대 방향으로 파라미터 업데이트
4. 수렴할 때까지 반복

이 알고리즘은 매우 단순하지만 강력합니다. 산을 내려갈 때 가장 가파른 내리막 방향으로 한 걸음씩 내려가는 것과 같습니다. 매번 가장 가파른 방향을 찾아서 조금씩 이동하면, 결국 골짜기에 도달하게 됩니다.

**수식:** $$w_{new} = w_{old} - \eta \cdot \frac{\partial L}{\partial w}$$

여기서:

- $w$: 최적화하려는 파라미터
- $\eta$ (에타): 학습률 (Learning Rate)
- $\frac{\partial L}{\partial w}$: 손실에 대한 파라미터의 그래디언트

학습률 $\eta$는 한 번에 얼마나 큰 걸음을 내딛을지 결정합니다. 너무 크면 최솟값을 지나쳐버리고, 너무 작으면 도달하는 데 시간이 오래 걸립니다.

**PyTorch 구현:**

```python
import torch

# 파라미터 정의 (requires_grad=True로 그래디언트 추적 활성화)
w = torch.tensor([2.0], requires_grad=True)
b = torch.tensor([1.0], requires_grad=True)

# Forward pass: 모델의 예측값 계산
x = torch.tensor([3.0])
y = torch.tensor([7.0])  # 실제값
y_pred = w * x + b       # 예측값: 2*3 + 1 = 7

# 손실 계산 (여기서는 제곱 오차)
loss = (y_pred - y) ** 2

# Backward pass: 그래디언트 자동 계산
loss.backward()

print(f"∂L/∂w = {w.grad}")  # tensor([6.])
print(f"∂L/∂b = {b.grad}")  # tensor([2.])

# 경사 하강법으로 파라미터 업데이트
learning_rate = 0.01
with torch.no_grad():
    w -= learning_rate * w.grad  # 2.0 - 0.01 * 6 = 1.94
    b -= learning_rate * b.grad  # 1.0 - 0.01 * 2 = 0.98
```

이 코드에서 `loss.backward()`가 호출되면, PyTorch는 자동으로 모든 파라미터에 대한 그래디언트를 계산하여 `.grad` 속성에 저장합니다. 그 다음 `w -= learning_rate * w.grad` 형태로 파라미터를 업데이트합니다.

---

## 학습률 (Learning Rate)

학습률 $\eta$는 그래디언트를 얼마나 반영할지 결정하는 하이퍼파라미터입니다. 이것을 적절히 설정하는 것은 학습 성공의 핵심입니다.

**학습률이 너무 큰 경우:**

- 증상: 파라미터가 최솟값 주변에서 크게 진동하며 수렴하지 못함
- 심한 경우: 손실이 발산하여 `NaN`이나 `inf` 발생
- 비유: 산을 내려가는데 너무 큰 보폭으로 걸어서 계속 반대편 언덕으로 올라가버림

**학습률이 너무 작은 경우:**

- 증상: 학습 속도가 매우 느림
- 위험: 지역 최솟값(local minimum)에 갇혀 더 나은 해를 찾지 못함
- 비유: 너무 작은 걸음으로 천천히 내려가다가 작은 골짜기에 갇힘

**적절한 학습률:**

- 안정적이면서 빠르게 수렴
- 일반적으로 0.001 ~ 0.1 범위에서 시작
- 실험을 통해 손실의 변화를 관찰하며 조정

많은 경우 학습 초기에는 큰 학습률로 빠르게 이동하다가, 최솟값에 가까워지면 학습률을 줄여서 정밀하게 조정하는 것이 효과적입니다. 이를 학습률 스케줄링이라고 합니다.

---

## 그래디언트 계산 방법

그래디언트를 계산하는 방법에는 여러 가지가 있습니다.

### 수치적 미분 (Numerical Differentiation)

미분의 정의를 직접 사용하는 방법입니다: $$f'(x) \approx \frac{f(x + h) - f(x)}{h}$$

매우 작은 $h$ (예: 0.0001)를 사용하여 함수값의 변화를 측정합니다. 구현은 간단하지만 몇 가지 문제가 있습니다. 첫째, 파라미터가 많으면 각각에 대해 함수를 두 번씩 계산해야 하므로 매우 느립니다. 둘째, 컴퓨터의 부동소수점 오차 때문에 정확한 값을 얻기 어렵습니다.

### 심볼릭 미분 (Symbolic Differentiation)

수학 공식을 적용하여 도함수를 계산하는 방법입니다. 예를 들어 $\frac{d}{dx}(x^2) = 2x$처럼 미적분 규칙을 사용합니다. 정확한 값을 얻을 수 있지만, 복잡한 함수에서는 도함수 식이 폭발적으로 길어지는 문제가 있습니다. 특히 신경망처럼 여러 층이 중첩된 경우 식이 너무 복잡해져서 실용적이지 않습니다.

### 자동 미분 (Automatic Differentiation)

PyTorch의 [[Autograd]]가 사용하는 방법입니다. 계산 그래프를 구성하고 [[Chain rule]]을 자동으로 적용하여 효율적으로 그래디언트를 계산합니다. 이것이 [[Backpropagation]]의 핵심 기술입니다.

```python
# PyTorch가 모든 것을 자동으로 처리
x = torch.tensor([2.0], requires_grad=True)
y = x ** 3 + 2 * x ** 2

y.backward()  # 자동으로 dy/dx 계산
print(x.grad)  # tensor([20.]) = 3*2² + 2*2*2
```

이 방법은 수치적 미분만큼 구현이 간단하면서도 심볼릭 미분만큼 정확합니다. 게다가 계산 속도도 빠릅니다. 현대 딥러닝 프레임워크들이 모두 이 방법을 사용하는 이유입니다.

---

## 다중 파라미터

실제 신경망은 수백만 개의 파라미터를 가집니다. 예를 들어 GPT-3는 1750억 개의 파라미터를 가지고 있습니다. PyTorch는 이 모든 파라미터의 그래디언트를 한 번의 `.backward()` 호출로 계산할 수 있습니다.

```python
# 여러 레이어를 가진 신경망
w1 = torch.randn(10, 5, requires_grad=True)
w2 = torch.randn(5, 1, requires_grad=True)
b1 = torch.randn(5, requires_grad=True)
b2 = torch.randn(1, requires_grad=True)

# Forward pass: 데이터가 신경망을 통과
h = torch.relu(x @ w1 + b1)  # 첫 번째 층
y_pred = h @ w2 + b2          # 두 번째 층
loss = ((y_pred - y) ** 2).mean()

# Backward pass: 모든 파라미터의 그래디언트를 한 번에 계산
loss.backward()

# 각 파라미터는 자신의 shape과 동일한 그래디언트를 가짐
print(w1.grad.shape)  # torch.Size([10, 5])
print(w2.grad.shape)  # torch.Size([5, 1])
```

각 파라미터의 그래디언트는 파라미터와 같은 shape을 가집니다. 이는 각 가중치마다 하나씩 대응하는 그래디언트가 있다는 의미입니다.

---

## 그래디언트 누적

PyTorch를 사용할 때 주의해야 할 중요한 특성이 있습니다. `.backward()`를 호출하면 그래디언트가 기존 값에 **더해집니다**:

```python
x = torch.tensor([1.0], requires_grad=True)

# 첫 번째 계산
y1 = x ** 2
y1.backward()
print(x.grad)  # tensor([2.])

# 두 번째 계산 (초기화하지 않음)
y2 = x ** 3
y2.backward()
print(x.grad)  # tensor([5.]) = 2 + 3
```

이것은 버그가 아니라 의도된 기능입니다. 여러 손실 함수를 동시에 최적화하는 경우처럼, 그래디언트를 합쳐야 하는 상황이 있기 때문입니다. 하지만 일반적인 학습 루프에서는 매 iteration마다 그래디언트를 초기화해야 합니다:

```python
# 올바른 사용법
x.grad.zero_()  # 또는 x.grad = None
y3 = x ** 4
y3.backward()
print(x.grad)  # tensor([4.])
```

실전에서는 `optimizer.zero_grad()`를 사용하여 모델의 모든 파라미터 그래디언트를 한 번에 초기화합니다. 이를 잊으면 그래디언트가 계속 누적되어 학습이 제대로 이루어지지 않습니다.

---

## 그래디언트 소실과 폭발

깊은 신경망을 학습시킬 때 자주 마주치는 문제가 있습니다.

### 그래디언트 소실 (Vanishing Gradient)

[[Backpropagation]]을 통해 그래디언트를 전파할 때, 여러 층을 거치면서 [[Chain rule]]에 의해 작은 값들이 계속 곱해지면 그래디언트가 0에 가까워집니다. 예를 들어 각 층에서 0.5가 곱해진다면, 10개 층을 거치면 $0.5^{10} \approx 0.001$이 되어 거의 0입니다. 결과적으로 초기 층의 파라미터는 거의 업데이트되지 않아 학습이 제대로 이루어지지 않습니다.

**해결 방법:**

- [[ReLU]] 활성화 함수 사용
- [[BatchNormalization]]
- Residual Connection (ResNet)

### 그래디언트 폭발 (Exploding Gradient)

반대로 큰 값들이 계속 곱해지면 그래디언트가 폭발적으로 커집니다. 파라미터가 불안정하게 변하고 손실이 `NaN`이 됩니다.

**해결 방법:**

Gradient Clipping을 통해 그래디언트의 크기를 제한합니다:

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

이 외에도 작은 학습률 사용, 적절한 가중치 초기화 등의 방법이 있습니다.

---

## 실전 활용

### 그래디언트 모니터링

학습이 제대로 진행되는지 확인하려면 그래디언트를 모니터링하는 것이 좋습니다:

```python
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        print(f"{name}: grad norm = {grad_norm:.4f}")
```

그래디언트가 0에 가깝다면 소실 문제를, 매우 크다면 폭발 문제를 의심해볼 수 있습니다.

### 학습률 스케줄링

학습이 진행됨에 따라 학습률을 조정하면 더 나은 결과를 얻을 수 있습니다:

```python
scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer, 
    step_size=30,  # 30 에포크마다
    gamma=0.1      # 학습률을 0.1배로 감소
)
```

초기에는 큰 학습률로 빠르게 이동하고, 최솟값에 가까워지면 학습률을 줄여 정밀하게 조정합니다.

---

## 핵심 요약

- 그래디언트는 함수가 가장 빠르게 증가하는 방향을 나타냄
- 경사 하강법은 그래디언트의 반대 방향으로 이동하여 손실을 최소화
- 학습률은 업데이트 크기를 조절하는 중요한 하이퍼파라미터
- PyTorch [[Autograd]]가 자동으로 그래디언트 계산
- 그래디언트는 누적되므로 매 iteration마다 `zero_grad()` 필요

---

## 관련 개념

- [[Loss Function]] - 그래디언트 계산의 대상
- [[Backpropagation]] - 그래디언트를 효율적으로 계산하는 알고리즘
- [[Chain rule]] - 역전파의 수학적 기초
- [[Autograd]] - PyTorch의 자동 미분 시스템
- [[Optimizer(Data Science)]] - 그래디언트를 사용한 파라미터 업데이트 알고리즘