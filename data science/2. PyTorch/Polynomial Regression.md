# Polynomial Regression

## 정의

**다항 회귀(Polynomial Regression)** 는 독립변수와 종속변수 사이의 관계를 다항식으로 모델링하는 회귀 분석 기법입니다. 선형 회귀가 직선으로 데이터를 근사한다면, 다항 회귀는 곡선으로 근사합니다.

n차 다항 회귀 모델: $$y = a_0 + a_1x + a_2x^2 + a_3x^3 + \cdots + a_nx^n$$

여기서 $a_0, a_1, ..., a_n$은 학습해야 할 파라미터이고, $x$는 입력 변수입니다.

---

## 선형 회귀와의 관계

다항 회귀는 선형 회귀의 확장입니다. "선형"이라는 말은 입력 $x$에 대해 선형이 아니라, 파라미터 $a_i$에 대해 선형이라는 의미입니다.

**선형 회귀:** $$y = a_0 + a_1x$$

직선으로 데이터를 근사합니다. 간단하지만 복잡한 패턴을 표현할 수 없습니다.

**2차 다항 회귀:** $$y = a_0 + a_1x + a_2x^2$$

포물선으로 데이터를 근사합니다. U자 모양이나 역 U자 모양의 패턴을 표현할 수 있습니다.

**3차 다항 회귀:** $$y = a_0 + a_1x + a_2x^2 + a_3x^3$$

더 복잡한 S자 곡선을 표현할 수 있습니다.

차수가 높아질수록 더 복잡한 패턴을 표현할 수 있지만, 과적합(overfitting)의 위험도 커집니다.

---

## 파라미터 학습

다항 회귀의 목표는 데이터를 가장 잘 설명하는 파라미터 $a_0, a_1, ..., a_n$을 찾는 것입니다. 이를 위해 [[Loss Function]]을 정의하고 최소화합니다.

### 손실 함수

회귀 문제에서는 일반적으로 평균 제곱 오차(MSE)를 사용합니다:

$$L = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2$$

여기서:

- $N$: 데이터 포인트의 개수
- $y_i$: 실제값
- $\hat{y}_i$: 모델의 예측값

예측값과 실제값의 차이를 제곱하여 평균을 냅니다. 제곱을 하는 이유는 양수와 음수 오차가 상쇄되는 것을 방지하고, 큰 오차에 더 큰 페널티를 주기 위함입니다.

### 최적화

파라미터를 찾기 위해 [[Gradient]] 기반 최적화를 사용합니다. 각 파라미터에 대한 손실의 그래디언트를 계산하여 경사 하강법으로 업데이트합니다:

$$a_i^{new} = a_i^{old} - \eta \cdot \frac{\partial L}{\partial a_i}$$

이 과정은 [[Forward Pass]] → [[Loss Function]] 계산 → [[Backpropagation]] → 파라미터 업데이트의 순서로 반복됩니다.

---

## 차수 선택의 중요성

다항식의 차수를 선택하는 것은 매우 중요합니다. 차수가 너무 낮으면 데이터의 패턴을 제대로 표현하지 못하고(underfitting), 차수가 너무 높으면 훈련 데이터에 과도하게 맞춰져 새로운 데이터에 일반화하지 못합니다(overfitting).

**차수가 너무 낮은 경우 (Underfitting):**

- 모델이 너무 단순하여 데이터의 진짜 패턴을 놓침
- 훈련 데이터와 테스트 데이터 모두에서 성능이 낮음
- 편향(bias)이 높음

**차수가 너무 높은 경우 (Overfitting):**

- 모델이 너무 복잡하여 노이즈까지 학습
- 훈련 데이터에는 잘 맞지만 테스트 데이터에는 성능이 낮음
- 분산(variance)이 높음

**적절한 차수:**

- 데이터의 복잡도에 맞는 표현력
- 훈련 데이터와 테스트 데이터 모두에서 좋은 성능
- 검증 데이터(validation set)를 사용하여 최적의 차수 선택

---

## 특징 변환 (Feature Transformation)

다항 회귀는 비선형 문제를 선형 모델로 변환하는 특징 변환의 한 예입니다. 입력 $x$를 $[1, x, x^2, x^3, ...]$로 변환하면, 원래 비선형 문제가 새로운 특징 공간에서는 선형 문제가 됩니다.

**원래 공간:** $$y = a_0 + a_1x + a_2x^2 + a_3x^3$$

**변환된 특징 공간:**

새로운 특징을 정의합니다: $\phi_0 = 1, \phi_1 = x, \phi_2 = x^2, \phi_3 = x^3$

그러면 모델은 다음과 같이 선형 형태가 됩니다: $$y = a_0\phi_0 + a_1\phi_1 + a_2\phi_2 + a_3\phi_3$$

이는 파라미터 $a_i$에 대해 선형이므로, 선형 회귀와 같은 방법으로 최적화할 수 있습니다.

---

## 정규화 (Regularization)

차수가 높은 다항 회귀는 과적합되기 쉽습니다. 이를 방지하기 위해 정규화 기법을 사용합니다.

### L2 정규화 (Ridge Regression)

손실 함수에 파라미터의 크기에 대한 페널티를 추가합니다:

$$L = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2 + \lambda\sum_{j=1}^{n}a_j^2$$

여기서 $\lambda$는 정규화 강도를 조절하는 하이퍼파라미터입니다. $\lambda$가 크면 파라미터가 작게 유지되어 모델이 단순해지고, $\lambda$가 작으면 정규화 효과가 줄어듭니다.

### L1 정규화 (Lasso Regression)

L2 대신 절댓값을 사용합니다:

$$L = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2 + \lambda\sum_{j=1}^{n}|a_j|$$

L1 정규화는 일부 파라미터를 정확히 0으로 만들어 자동으로 특징 선택(feature selection)을 수행합니다.

---

## 다변수 다항 회귀

입력 변수가 여러 개인 경우로 확장할 수 있습니다. 예를 들어 두 개의 입력 $x_1, x_2$가 있다면:

$$y = a_0 + a_1x_1 + a_2x_2 + a_3x_1^2 + a_4x_2^2 + a_5x_1x_2$$

이 경우 항의 개수가 급격히 증가합니다. 2차 다항식에서 변수가 $d$개라면 항의 개수는 대략 $O(d^2)$이고, 3차라면 $O(d^3)$입니다. 차수와 변수 개수가 증가할수록 계산량과 과적합 위험이 커집니다.

---

## 장단점

**장점:**

- 비선형 관계를 표현할 수 있음
- 구현이 간단하고 해석이 직관적
- 선형 회귀의 도구와 이론을 그대로 활용 가능

**단점:**

- 차수가 높아지면 과적합 위험
- 외삽(extrapolation)에 취약: 훈련 데이터 범위를 벗어나면 예측이 불안정
- 고차원에서는 특징 개수가 폭발적으로 증가

---

## 신경망과의 연결

다항 회귀는 신경망의 가장 단순한 형태로 볼 수 있습니다. 활성화 함수가 없는 단층 신경망에 다항식 특징을 입력으로 주는 것과 같습니다. 신경망은 이를 일반화하여:

- 여러 층을 쌓아 더 복잡한 특징 자동 학습
- 비선형 활성화 함수로 더 풍부한 표현력 획득
- 다항식처럼 명시적으로 특징을 정의하지 않아도 됨

다항 회귀는 신경망의 기본 개념인 특징 변환, 손실 최소화, 그래디언트 기반 학습을 이해하는 좋은 출발점입니다.

---

## 실전 예제

3차 다항 회귀로 $y = \sin(x)$를 근사하는 예제입니다:

$$\hat{y} = a + bx + cx^2 + dx^3$$

```python
import torch

# 데이터 생성
x = torch.linspace(-3.14, 3.14, 100)
y = torch.sin(x)

# 파라미터 초기화
a = torch.randn((), requires_grad=True)
b = torch.randn((), requires_grad=True)
c = torch.randn((), requires_grad=True)
d = torch.randn((), requires_grad=True)

learning_rate = 1e-3

for t in range(4000):
    # Forward pass
    y_pred = a + b * x + c * x ** 2 + d * x ** 3
    
    # Loss 계산
    loss = ((y_pred - y) ** 2).mean()
    
    # Backward pass
    loss.backward()
    
    # 파라미터 업데이트
    with torch.no_grad():
        a -= learning_rate * a.grad
        b -= learning_rate * b.grad
        c -= learning_rate * c.grad
        d -= learning_rate * d.grad
    
    # 그래디언트 초기화
    a.grad = None
    b.grad = None
    c.grad = None
    d.grad = None

print(f'Result: y = {a.item():.3f} + {b.item():.3f}x + {c.item():.3f}x^2 + {d.item():.3f}x^3')
```

이 예제는 PyTorch의 전체 학습 흐름을 보여줍니다: [[Forward Pass]]에서 예측값을 계산하고, [[Loss Function]]으로 오차를 측정하며, [[Backpropagation]]으로 [[Gradient]]를 구한 뒤, 파라미터를 업데이트합니다. 이 패턴은 더 복잡한 신경망에서도 동일하게 적용됩니다.

---

## 핵심 요약

- 다항 회귀는 비선형 관계를 다항식으로 모델링
- 차수가 높을수록 복잡한 패턴 표현 가능하지만 과적합 위험
- 특징 변환을 통해 비선형 문제를 선형 문제로 변환
- 정규화로 과적합 방지
- 신경망 학습의 기본 개념을 이해하는 좋은 출발점

---

## 관련 개념

- [[Loss Function]] - MSE를 사용한 오차 측정
- [[Gradient]] - 경사 하강법으로 파라미터 최적화
- [[Forward Pass]] - 예측값 계산
- [[Backpropagation]] - 그래디언트 계산
- [[Optimizer(Data Science)]] - 파라미터 업데이트 알고리즘