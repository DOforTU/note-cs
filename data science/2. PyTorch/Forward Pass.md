# Forward Pass

## 정의

**순전파(Forward Pass)** 는 입력 데이터가 신경망의 첫 번째 층부터 마지막 층까지 순차적으로 통과하여 예측값을 계산하는 과정입니다. 신경망이 입력을 받아 출력을 생성하는 방향이 "앞으로" 진행되기 때문에 순전파라고 부릅니다.

$$\text{입력}(x) \rightarrow \text{층1} \rightarrow \text{층2} \rightarrow \cdots \rightarrow \text{층n} \rightarrow \text{출력}(\hat{y})$$

---

## 기본 원리

Forward pass를 이해하는 가장 좋은 방법은 공장의 조립 라인을 떠올리는 것입니다. 원자재가 첫 번째 공정에 투입되면, 각 공정을 순서대로 거쳐 최종 제품이 완성됩니다. 신경망도 마찬가지입니다. 입력 데이터가 첫 번째 층에서 처리되고, 그 결과가 두 번째 층으로 전달되며, 이런 식으로 계속 이어져 최종 예측값이 나옵니다.

각 층에서는 다음과 같은 계산이 수행됩니다:

$$\text{출력} = f(W \times \text{입력} + b)$$

여기서:

- $W$: 가중치 행렬 (학습할 파라미터)
- $b$: 편향 (학습할 파라미터)
- $f$: 활성화 함수 (비선형 변환)

가중치 행렬 $W$는 입력의 각 요소가 출력에 얼마나 영향을 미치는지를 결정합니다. 편향 $b$는 기본적인 오프셋을 추가합니다. 활성화 함수 $f$는 비선형성을 부여하여 신경망이 복잡한 패턴을 표현할 수 있게 합니다.

---

## 단계별 과정

Forward pass는 다음 단계로 진행됩니다.

### 1. 입력 데이터 준비

모든 것은 입력 데이터에서 시작합니다. 이미지라면 픽셀값, 텍스트라면 단어 임베딩, 표 데이터라면 각 특징의 값이 입력이 됩니다.

예를 들어 손글씨 숫자 이미지(28×28 픽셀)는 784개의 숫자로 이루어진 벡터로 변환됩니다: $$x = [x_1, x_2, ..., x_{784}]$$

### 2. 첫 번째 층 통과

입력이 첫 번째 층으로 들어갑니다. 각 뉴런은 가중치 합을 계산하고 활성화 함수를 적용합니다:

$$z^{(1)} = W^{(1)}x + b^{(1)}$$ $$h^{(1)} = f(z^{(1)})$$

여기서 $h^{(1)}$은 첫 번째 층의 출력이자 두 번째 층의 입력이 됩니다. 예를 들어 첫 번째 층에 128개의 뉴런이 있다면, $h^{(1)}$은 128개의 숫자를 담은 벡터입니다.

### 3. 중간 층들 통과

같은 과정이 각 층마다 반복됩니다. 이전 층의 출력이 현재 층의 입력이 됩니다:

$$z^{(2)} = W^{(2)}h^{(1)} + b^{(2)}$$ $$h^{(2)} = f(z^{(2)})$$

$$z^{(3)} = W^{(3)}h^{(2)} + b^{(3)}$$ $$h^{(3)} = f(z^{(3)})$$

층이 깊어질수록 더 추상적이고 복잡한 특징을 추출합니다. 이미지 인식에서 초기 층은 선이나 모서리 같은 단순한 패턴을 감지하고, 중간 층은 눈이나 코 같은 부분을 인식하며, 마지막 층은 얼굴 전체를 인식합니다.

### 4. 출력 층

마지막 층은 최종 예측값을 생성합니다. 출력 형태는 문제 유형에 따라 다릅니다.

**회귀 문제:** $$\hat{y} = W^{(L)}h^{(L-1)} + b^{(L)}$$

활성화 함수 없이 직접 값을 출력합니다. 예를 들어 집 가격 예측이라면 "3억 2천만원" 같은 연속적인 값이 나옵니다.

**분류 문제:** $$\hat{y} = \text{softmax}(W^{(L)}h^{(L-1)} + b^{(L)})$$

Softmax 함수를 적용하여 각 클래스에 대한 확률을 출력합니다. 예를 들어 숫자 인식이라면 "0일 확률 5%, 1일 확률 2%, ..., 7일 확률 95%"처럼 10개 클래스 각각에 대한 확률이 나옵니다.

---

## 구체적 예시

손글씨 숫자 "7"을 인식하는 신경망의 forward pass를 따라가 봅시다.

**입력층:**

- 28×28 픽셀 이미지를 784개 숫자의 벡터로 변환
- $x = [0.0, 0.0, 0.2, 0.8, ..., 0.0]$ (각 픽셀의 밝기)

**첫 번째 은닉층 (784 → 128):** $$z^{(1)} = W^{(1)}x + b^{(1)}$$ $$h^{(1)} = \text{ReLU}(z^{(1)})$$

784개 입력을 받아 128개 뉴런이 각자의 특징을 계산합니다. ReLU 활성화 함수는 음수를 0으로 만들고 양수는 그대로 통과시킵니다. 결과는 128개 숫자의 벡터입니다.

**두 번째 은닉층 (128 → 64):** $$z^{(2)} = W^{(2)}h^{(1)} + b^{(2)}$$ $$h^{(2)} = \text{ReLU}(z^{(2)})$$

128개 입력을 받아 64개 뉴런으로 더 압축된 표현을 만듭니다. 이 층은 첫 번째 층이 찾은 특징들을 조합하여 더 복잡한 패턴을 감지합니다.

**출력층 (64 → 10):** $$z^{(3)} = W^{(3)}h^{(2)} + b^{(3)}$$ $$\hat{y} = \text{softmax}(z^{(3)})$$

64개 입력을 받아 10개 클래스(숫자 0~9)에 대한 확률을 계산합니다.

**최종 출력:** $$\hat{y} = [0.01, 0.02, 0.01, 0.03, 0.01, 0.02, 0.01, 0.95, 0.02, 0.01]$$

7번 인덱스가 0.95로 가장 높으므로, 이 이미지는 "7"일 확률이 95%입니다.

---

## 활성화 함수의 역할

활성화 함수가 없다면 아무리 많은 층을 쌓아도 결국 하나의 선형 변환에 불과합니다. 수학적으로:

$$h_2 = W_2(W_1x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2)$$

이는 $W'x + b'$ 형태의 단순한 선형 변환과 같습니다. 선형 변환으로는 직선이나 평면만 표현할 수 있어서 복잡한 패턴을 학습할 수 없습니다.

활성화 함수는 비선형성을 추가하여 이 문제를 해결합니다. 대표적인 활성화 함수들:

**ReLU (Rectified Linear Unit):** $$f(x) = \max(0, x)$$

음수는 0으로, 양수는 그대로 통과시킵니다. 가장 널리 사용되며 계산이 빠르고 gradient 소실 문제가 적습니다.

**Sigmoid:** $$f(x) = \frac{1}{1 + e^{-x}}$$

모든 값을 0과 1 사이로 압축합니다. 출력을 확률로 해석할 수 있지만, 극단값에서 gradient가 거의 0이 되는 문제가 있습니다.

**Tanh:** $$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

모든 값을 -1과 1 사이로 압축합니다. Sigmoid보다 gradient 소실이 덜하지만 여전히 존재합니다.

---

## Forward Pass의 두 가지 역할

Forward pass는 신경망에서 두 가지 중요한 역할을 합니다.

### 1. 추론 (Inference)

학습이 완료된 모델을 실제로 사용할 때는 forward pass만 실행합니다. 새로운 이미지를 입력하면 신경망을 통과시켜 "이것은 고양이입니다"라는 예측을 얻습니다. 이 과정을 추론이라고 부릅니다.

추론 시에는:

- 파라미터가 고정되어 있음 (업데이트하지 않음)
- [[Backpropagation]]이 필요 없음
- 빠른 계산이 가능

실시간 서비스에서는 수천, 수만 개의 요청을 동시에 처리해야 하므로 forward pass의 속도가 매우 중요합니다.

### 2. 학습 (Training)

학습 단계에서 forward pass는 [[Loss Function]]을 계산하기 위한 첫 번째 단계입니다.

**학습 과정:**

1. Forward pass로 예측값 $\hat{y}$ 계산
2. [[Loss Function]]으로 오차 계산: $L = f(\hat{y}, y)$
3. [[Backpropagation]]으로 [[Gradient]] 계산
4. [[Gradient]]를 사용하여 파라미터 업데이트

Forward pass에서 계산된 중간 결과들($h^{(1)}, h^{(2)}$, ...)은 backward pass에서 gradient를 계산할 때 필요하므로 메모리에 저장됩니다. 이를 계산 그래프라고 부릅니다.

---

## 배치 처리 (Batch Processing)

실전에서는 하나의 데이터가 아니라 여러 개를 동시에 처리합니다. 이를 배치(batch) 처리라고 합니다.

**단일 샘플:** $$h = f(Wx + b)$$

여기서 $x$는 벡터(1D), $h$도 벡터입니다.

**배치 처리:** $$H = f(WX + b)$$

여기서 $X$는 행렬(2D), $H$도 행렬입니다. 각 열(또는 행)이 하나의 샘플입니다.

예를 들어 배치 크기가 32라면:

- $X$: 32개 샘플을 담은 행렬
- $H$: 32개 샘플 각각에 대한 출력을 담은 행렬

배치 처리의 장점:

**계산 효율성:** GPU는 병렬 처리에 최적화되어 있어, 32개를 하나씩 처리하는 것보다 동시에 처리하는 것이 훨씬 빠릅니다.

**안정적인 학습:** 여러 샘플의 [[Gradient]]를 평균내어 사용하므로, 노이즈가 줄어들고 학습이 안정적입니다.

**메모리 효율:** GPU 메모리를 효율적으로 사용할 수 있습니다.

---

## PyTorch 예시

PyTorch에서 forward pass를 구현하는 방법입니다:

```python
import torch
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Linear(784, 128)  # 784 → 128
        self.layer2 = nn.Linear(128, 64)   # 128 → 64
        self.layer3 = nn.Linear(64, 10)    # 64 → 10
        self.relu = nn.ReLU()
        
    def forward(self, x):
        # 첫 번째 층
        h1 = self.layer1(x)
        h1 = self.relu(h1)
        
        # 두 번째 층
        h2 = self.layer2(h1)
        h2 = self.relu(h2)
        
        # 출력 층
        output = self.layer3(h2)
        
        return output

# 모델 생성
model = SimpleNet()

# 배치 크기 32의 랜덤 입력
x = torch.randn(32, 784)

# Forward pass 실행
output = model(x)

print(output.shape)  # torch.Size([32, 10])
```

`model(x)`를 호출하면 자동으로 `forward` 메서드가 실행됩니다. 각 층을 순차적으로 통과하며 최종 출력을 계산합니다.

---

## 계산 복잡도

Forward pass의 계산량은 네트워크 구조에 따라 결정됩니다.

**완전 연결층 (Fully Connected Layer):** 입력 차원이 $n$, 출력 차원이 $m$일 때:

- 곱셈 연산: $O(n \times m)$
- 덧셈 연산: $O(n \times m)$

**예시:**

- 층1: 784 → 128 = 약 100,000번 연산
- 층2: 128 → 64 = 약 8,000번 연산
- 층3: 64 → 10 = 약 640번 연산

총 약 110,000번의 연산이 필요합니다. 현대 GPU는 초당 수조 번의 연산을 수행할 수 있어, 이 정도 규모의 신경망은 밀리초 단위로 실행됩니다.

깊은 신경망일수록, 층의 크기가 클수록 계산량이 증가합니다. 하지만 병렬 처리 덕분에 실시간 서비스에서도 충분히 사용 가능합니다.

---

## 주의사항

Forward pass를 구현할 때 주의해야 할 점들이 있습니다.

### 차원 불일치

각 층의 출력 차원이 다음 층의 입력 차원과 일치해야 합니다. 불일치하면 에러가 발생합니다:

```python
layer1 = nn.Linear(784, 128)
layer2 = nn.Linear(64, 10)  # 잘못됨! 128이어야 함

x = torch.randn(32, 784)
h = layer1(x)  # shape: (32, 128)
output = layer2(h)  # Error! 128 ≠ 64
```

### 활성화 함수 위치

활성화 함수는 일반적으로 선형 변환 직후에 적용됩니다. 하지만 출력층에서는 문제 유형에 따라 다릅니다:

**회귀 문제:** 활성화 함수 없이 직접 값 출력

**이진 분류:** Sigmoid 적용

**다중 클래스 분류:** Softmax 적용 (보통 [[Loss Function]]에 포함)

### 중간 결과 저장

학습 시에는 [[Backpropagation]]을 위해 중간 결과들을 저장해야 합니다. PyTorch의 [[Autograd]]가 자동으로 처리하지만, 메모리 사용량이 증가합니다.

추론 시에는 `torch.no_grad()`를 사용하여 중간 결과 저장을 비활성화하면 메모리를 절약할 수 있습니다:

```python
with torch.no_grad():
    output = model(x)  # 중간 결과를 저장하지 않음
```

---

## 핵심 요약

- Forward pass는 입력이 신경망을 통과하여 예측값을 생성하는 과정
- 각 층에서 가중치 합을 계산하고 활성화 함수를 적용
- 활성화 함수가 비선형성을 부여하여 복잡한 패턴 학습 가능
- 추론 시에는 forward pass만 실행
- 학습 시에는 forward pass 후 [[Loss Function]]과 [[Backpropagation]] 실행
- 배치 처리로 여러 샘플을 동시에 효율적으로 처리

---

## 관련 개념

**선수 지식:**

- [[Machine Learning Basics]] - 머신러닝 기본 원리
- [[Loss Function]] - 오차 측정 방법

**관련 개념:**

- [[Backpropagation]] - Gradient를 계산하는 역방향 과정
- [[Deep Learning Core Concepts]] - 딥러닝 전체 구조
- [[Autograd]] - PyTorch의 자동 미분 시스템

**다음 단계:**

- [[Chain rule]] - 역전파의 수학적 기초
- [[Optimizer(Data Science)]] - SGD, Adam 등 파라미터 업데이트 알고리즘