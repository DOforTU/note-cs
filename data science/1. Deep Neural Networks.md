# 1. Deep Neural Networks

## Contents

- **1.1 Feedforward NN (피드포워드 신경망)**
- **1.2 Learning XOR (XOR 학습)**
- **1.3 Activation Functions (활성화 함수)**
- **1.4 Optimization & Training (최적화 및 학습)**
- **1.5 Calculus Fundamentals (미적분학 기초)**

---
## 1.1 Feedforward NN (피드포워드 신경망)

피드포워드 신경망(Feedforward Neural Network)은 때로는 다층 퍼셉트론(Multi-Layer Perceptron, MLP)이라고도 불리며, 신경망의 가장 기본적인 형태입니다.

### 1.1.1 구성 요소와 정보 흐름

피드포워드 신경망은 여러 층의 함수들이 마치 사슬처럼 연결되어 하나의 큰 함수를 이루는 구조를 가집니다.

**주요 구성 요소**:

1. **입력층 (Input Layer):** 데이터가 처음으로 들어오는 층입니다.
2. **은닉층 (Hidden Layers):** 0개 또는 그 이상의 은닉층이 존재할 수 있습니다.
3. **출력층 (Output Layer):** 최종적인 결과 $(\hat{y})$를 도출하는 층입니다.

여러 개의 층 $f^{(1)}, f^{(2)}, \dots, f^{(l)}$은 순차적으로 연결되어 최종 모델 $f(\mathbf{x})$를 만듭니다. 이를 수식으로 나타내면 $f(\mathbf{x}) = f^{(l)}(\cdots f^{(2)}(f^{(1)}(\mathbf{x})))$ 와 같습니다.

**정보의 흐름** 이 신경망의 특징은 그 이름처럼 정보가 항상 입력에서 출력 방향으로만 흐른다는 점입니다 ('feedforward'). 네트워크 내에 피드백이나 순환적인 연결이 없기 때문에, 정보는 일방통행으로만 전달됩니다.

### 1.1.2 다른 학습 알고리즘과의 관계

피드포워드 신경망은 은닉층이 없을 경우, 이미 널리 알려진 다른 선형 학습 알고리즘들과 동일해집니다.

- **선형 회귀 (Linear Regression):** 은닉층이 전혀 없고 단일 선형 출력 유닛(single linear output unit)을 사용하는 피드포워드 신경망은 선형 회귀와 같습니다.
- **로지스틱 회귀 (Logistic Regression):** 은닉층이 없고 단일 시그모이드 출력 유닛(single sigmoid output unit)을 사용하는 피드포워드 신경망은 로지스틱 회귀와 같습니다.

### 1.1.3 학습 과정에서의 중요한 차이점

신경망을 훈련하려면 적절한 비용 함수(cost function)를 선택하고 출력 유닛 및 은닉 유닛의 표현 방식을 결정해야 합니다.

다른 선형 모델과 피드포워드 신경망이 학습 과정에서 가지는 가장 큰 차이점은 바로 **비용 함수의 형태**입니다.

1. **비선형성으로 인한 비볼록 비용 함수:** 신경망에 도입된 비선형성(non-linearity)으로 인해 비용 함수가 비볼록(nonconvex)하게 됩니다.
2. **최적화의 어려움:** 비용 함수가 비볼록하기 때문에 여러 지역 최적점(local optima)이 존재할 수 있으며, 우리가 궁극적으로 원하는 전역 최적점(global optimum)을 찾을 수 있다는 보장이 어렵다는 특징이 있습니다.

---
## 1.2 Learning XOR (XOR 학습)

XOR(배타적 논리합) 문제는 신경망, 특히 피드포워드 신경망의 필요성을 설명할 때 가장 기본적이면서 중요한 예시입니다. XOR는 입력 데이터가 $(0, 0) \to 0$, $(0, 1) \to 1$, $(1, 0) \to 1$, $(1, 1) \to 0$ 의 결과를 가져야 하는 논리 함수입니다.

### 1.2.1 선형 모델의 한계와 새로운 특징 공간의 필요성

단 하나의 선형 분리선(linear boundary)만으로는 XOR 문제를 해결할 수 없습니다. 즉, 선형 모델(Linear Model)이 XOR 문제를 풀기 위해서는 기존의 특징 공간이 아닌, 선형 모델이 작동할 수 있는 **'다른 특징 공간(different feature space)'** 이 반드시 필요합니다.

신경망은 바로 이 '새로운 특징 공간'을 자동으로 생성해주는 역할을 수행합니다.

### 1.2.2 은닉층의 도입

XOR 문제를 해결하기 위해 아주 간단한 형태의 신경망을 사용합니다. 이 신경망은 하나의 은닉층(Hidden Layer)을 포함하며, 이 은닉층에는 두 개의 은닉 특징 $\mathbf{h} = (h_1, h_2)$가 존재합니다.

이러한 다층 구조는 여러 층의 함수가 사슬처럼 연결된 형태로 표현됩니다. $$\hat{y} = f^{(2)}(f^{(1)}(\mathbf{x}))$$

### 1.2.3 선형 활성화 함수의 문제

만약 신경망의 첫 번째 층 $f^{(1)}$이 선형 활성화 함수를 사용한다면, 즉 $\mathbf{h} = f^{(1)}(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{c}$ 형태라면, 심각한 문제가 발생합니다.

출력 $\hat{y}$는 $\mathbf{h}$를 입력으로 받아 다시 선형 연산을 수행합니다: $$\hat{y} = f^{(2)}(\mathbf{h}) = \mathbf{w}^T\mathbf{h} + b$$

이 두 식을 결합하면 다음과 같이 단순화됩니다: $$\hat{y} = \mathbf{w}^T(\mathbf{W}\mathbf{x} + \mathbf{c}) + b = (\mathbf{w}^T\mathbf{W})\mathbf{x} + (\mathbf{w}^T\mathbf{c} + b)$$ $$= \mathbf{w}'^T \mathbf{x} + b'$$

결국, 층을 아무리 깊게 쌓더라도 모든 함수가 선형이라면 전체 모델은 $\mathbf{w}'^T \mathbf{x} + b'$ 형태의 **단순 선형 모델(linear model)로 환원**되어 버립니다. 따라서 비선형적인 문제를 해결하기 위해서는 은닉층에 반드시 비선형 함수 $g$를 사용해야 합니다.

### 1.2.4 비선형 함수 (ReLU)를 사용한 핸드 코딩 예제

실제로 XOR 문제를 해결하기 위해서는 은닉 특징 $\mathbf{h}$를 계산할 때 비선형 함수 $g$를 적용해야 합니다. $$\mathbf{h} = g(\mathbf{W}\mathbf{x} + \mathbf{c})$$ 최종 출력은 다음과 같습니다: $$\hat{y} = \mathbf{w}^T f^{(1)}(\mathbf{x}) + b = \mathbf{w}^T g(\mathbf{W}\mathbf{x} + \mathbf{c}) + b$$

여기서 비선형 함수 $g$의 예시로 ReLU(Rectified Linear Unit)를 사용해볼 수 있습니다. ReLU는 $g(z) = \max(0, z)$로 정의됩니다.

**핸드 코딩된 파라미터 예시**: 특정 가중치와 편향을 수동으로 설정(hand-coded solution)하여 XOR 결과를 완벽하게 도출할 수 있습니다.

| 파라미터       | 값                                             |
| :--------- | :-------------------------------------------- |
| **W**      | $\begin{pmatrix} 1 & 1 \ 1 & 1 \end{pmatrix}$ |
| **c**      | $\begin{pmatrix} 0 \ -1 \end{pmatrix}$        |
| **w**      | $\begin{pmatrix} 1 \ -2 \end{pmatrix}$        |
| **b**      | $0$                                           |
| **활성화 함수** | $g(z) = \max(0, z)$ (ReLU)                    |

**데이터 입력 (XOR)**: $$\mathbf{x}_1 = \begin{pmatrix} 0 \ 0 \end{pmatrix}, \mathbf{x}_2 = \begin{pmatrix} 0 \ 1 \end{pmatrix}, \mathbf{x}_3 = \begin{pmatrix} 1 \ 0 \end{pmatrix}, \mathbf{x}_4 = \begin{pmatrix} 1 \ 1 \end{pmatrix}$$

**계산 예시 (중간 단계 $\mathbf{W}\mathbf{x} + \mathbf{c}$)**:

- $\mathbf{W}\mathbf{x}_1 + \mathbf{c} = \begin{pmatrix} 0 \ -1 \end{pmatrix}$
- $\mathbf{W}\mathbf{x}_4 + \mathbf{c} = \begin{pmatrix} 2 \ 1 \end{pmatrix}$

**ReLU 적용 후 (은닉 특징 $\mathbf{h}$)**:

- $g(\mathbf{W}\mathbf{x}_1 + \mathbf{c}) = g\begin{pmatrix} 0 \ -1 \end{pmatrix} = \begin{pmatrix} 0 \ 0 \end{pmatrix}$
- $g(\mathbf{W}\mathbf{x}_2 + \mathbf{c}) = \begin{pmatrix} 1 \ 0 \end{pmatrix}$
- $g(\mathbf{W}\mathbf{x}_3 + \mathbf{c}) = \begin{pmatrix} 1 \ 0 \end{pmatrix}$
- $g(\mathbf{W}\mathbf{x}_4 + \mathbf{c}) = g\begin{pmatrix} 2 \ 1 \end{pmatrix} = \begin{pmatrix} 2 \ 1 \end{pmatrix}$

**최종 결과 ($\mathbf{w}^T \mathbf{h} + b$)**: 이 과정을 통해 최종 출력을 계산하면 다음과 같이 XOR의 정답이 나옵니다:

- $\mathbf{x}_1$의 출력: $0$
- $\mathbf{x}_2$의 출력: $1$
- $\mathbf{x}_3$의 출력: $1$
- $\mathbf{x}_4$의 출력: $0$

이처럼 비선형 활성화 함수를 가진 은닉층을 도입함으로써, 신경망은 선형적으로 분리 불가능했던 XOR 문제를 해결할 수 있는 새로운 특징 공간을 만들게 됩니다.

---
## 1.3 Activation Functions (활성화 함수)

활성화 함수(Activation Functions)는 신경망의 은닉층에 비선형성을 부여하여, 신경망이 복잡한 함수를 학습하고 선형 모델의 한계를 넘어설 수 있게 해주는 핵심 요소입니다. 어떤 활성화 함수를 사용하느냐에 따라 학습 속도와 성능에 큰 차이가 발생합니다.

### 1.3.1 시그모이드 (Sigmoid)

시그모이드 함수는 초창기 신경망에서 널리 사용되던 함수입니다.

|특징|내용|
|:--|:--|
|**수식**|$g(z) = \frac{1}{1 + e^z}$|
|**출력 범위**|숫자를 범위로 압축합니다.|

**문제점**: 시그모이드 함수는 훈련 과정에서 여러 가지 문제점을 유발하여 최근에는 잘 사용되지 않습니다.

1. **그레디언트 소실 (Gradient Killing):** 포화(saturated)될 경우 그레디언트를 죽입니다 (kills gradients).
2. **출력 값 비중심화 (Not Zero-centered):** 출력 값이 0을 중심으로 분포하지 않습니다.
3. **계산 비용:** $\exp()$ 연산이 필요하여 계산 비용이 다소 높습니다.

### 1.3.2 Tanh (Hyperbolic Tangent)

Tanh 함수는 시그모이드 함수의 중심화 문제를 개선한 버전입니다.

|특징|내용|
|:--|:--|
|**수식**|$g(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$|
|**출력 범위**|숫자를 [-1, 1] 범위로 압축합니다.|

**문제점**: Tanh 역시 시그모이드와 마찬가지로 값이 극단적으로 커지거나 작아져 포화되면 그레디언트를 죽이는 문제가 있습니다.

### 1.3.3 ReLU (Rectified Linear Unit)

ReLU는 현대 딥러닝에서 가장 널리 사용되는 활성화 함수입니다.

|특징|내용|
|:--|:--|
|**수식**|$g(z) = \max(0, z)$|

**장점**:

1. **비포화(Non-saturation):** 양수(+) 영역에서는 포화되지 않습니다.
2. **효율성:** 계산 효율이 매우 높습니다 (Very computationally efficient).
3. **수렴 속도:** 실제 훈련 시 시그모이드나 Tanh보다 훨씬 빠르게 수렴합니다.

**단점**:

1. **출력 값 비중심화:** 출력 값이 0을 중심으로 분포하지 않습니다.
2. **Dying ReLU 문제:** $z < 0$ 영역에서는 그레디언트가 0이 되어 가중치 업데이트가 멈추는 문제가 발생할 수 있습니다.

### 1.3.4 LeakyReLU 및 Parametric ReLU (PReLU)

LeakyReLU와 PReLU는 ReLU의 단점($z<0$ 일 때 그레디언트가 0이 되는 문제)을 보완하기 위해 도입되었습니다.

|구분|수식|특징|
|:--|:--|:--|
|**LeakyReLU**|$g(z) = \max(0.01z, z)$|음수 영역에서 아주 작은 기울기를 허용합니다.|
|**PReLU**|$g(z) = \max(\alpha z, z)$ (여기서 $\alpha$는 학습되는 파라미터)|$\alpha$ 값을 파라미터로 두어 학습합니다.|

**공통 장점**:

1. 포화되지 않습니다.
2. 계산 효율이 좋습니다.
3. 실제 환경에서 시그모이드/Tanh보다 훨씬 빠르게 수렴합니다.
4. 그레디언트가 죽지 않습니다 (Gradients will not die).

### 1.3.5 ELU (Exponential Linear Unit)

ELU는 ReLU의 장점을 유지하면서 출력 값 분포를 0에 가깝게 만들려고 시도한 함수입니다.

**주요 특징**:

1. **ReLU의 모든 장점**을 가집니다.
2. 출력 평균이 0에 더 가깝습니다 (Closer to zero mean outputs).
3. 음수 영역에서의 포화(Negative saturation regime) 특징이 LeakyReLU에 비해 노이즈에 대한 견고성(robustness to noise)을 추가합니다.
4. 단, $\exp()$ 연산이 필요합니다.


---
## 1.4 Optimization & Training (최적화 및 학습)

신경망을 효과적으로 사용하려면 복잡한 비선형 비용 함수(Cost Function)를 최소화하는 최적화 과정이 필수적입니다. 이 과정은 모델이 주어진 데이터를 통해 학습하고, 실제 예측 능력을 향상시키도록 파라미터 $\mathbf{\theta}$를 조정하는 핵심 단계입니다.

### 1.4.1 학습 과정의 개요

신경망 학습은 파라미터 $\mathbf{\theta}$를 작은 무작위 수(small random numbers)로 초기화하는 것부터 시작됩니다. 이후 정지 조건(예: 에러가 매우 작아질 때)에 도달할 때까지 다음 두 단계를 반복합니다.

1. **순전파 (Forward Propagation):** 입력 데이터 $\mathbf{x}$를 신경망의 입력에서 출력 방향으로 전파시켜 예측 값 $\hat{y}$를 계산합니다.
2. **역전파 (Backpropagation):** 계산된 예측 값과 실제 정답 $y$ 간의 차이(비용)를 계산하고, 이 비용을 역방향으로 전파시켜 파라미터를 업데이트합니다.

### 1.4.2 비용 함수 및 위험 최소화

학습의 목표는 비용 함수(Loss Function) $L$을 통해 정의된 총 비용 $J(\mathbf{\theta})$를 최소화하는 것입니다.

- **비용 함수 (Cost Function):** 훈련 데이터셋 $D$에 있는 모든 샘플에 대한 개별 손실(Loss) $L(y_i, \hat{y}_i)$의 평균으로 정의됩니다. $$J(\mathbf{\theta}) = \frac{1}{n} \sum_{\mathbf{x}_i, y_i \in D} L(y_i, \hat{y}_i)$$
    
- **경험적 위험 최소화 (Empirical Risk Minimization, ERM):** 우리가 관찰한 훈련 데이터셋 $D$에서 오류를 최소화하는 것을 목표로 합니다. 이것이 우리가 비용 함수 $J(\mathbf{\theta})$를 최소화하는 과정입니다. 하지만 ERM은 모델의 표현 능력이 클 경우 훈련 데이터만 외우는 과적합(overfitting) 문제에 취약할 수 있습니다.
    
- **구조적 위험 최소화 (Structural Risk Minimization):** 경험적 위험을 최소화함과 동시에 모델의 표현 능력(capacity, 복잡도)을 제어하는 방식(예: L1/L2 정규화)을 사용하여 과적합을 방지합니다.
    

### 1.4.3 경사 하강법 (Gradient Descent)

파라미터를 업데이트하는 기본 방법은 경사 하강법입니다. 파라미터 $\mathbf{\theta}$는 비용 함수 $J(\mathbf{\theta})$의 기울기(경사)를 따라 $\epsilon$만큼 이동하며 최소점을 찾아갑니다. $$\mathbf{\theta} \coloneqq \mathbf{\theta} - \epsilon \nabla_{\mathbf{\theta}} J(\mathbf{\theta})$$ 여기서 $\epsilon > 0$은 학습률(learning rate)입니다.

#### 1.4.3.1 배치 경사 하강법 (Batch Gradient Descent)

경사를 계산할 때 전체 훈련 데이터셋(The entire training set, Batch)을 사용합니다.

- **장점:** 정확하고 단순합니다.
- **단점:** 훈련 데이터셋이 클 경우 각 반복(Iteration)의 계산 비용이 매우 높습니다.

#### 1.4.3.2 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)

전체 데이터셋 대신 미니 배치(Minibatch)라 불리는 데이터의 부분 집합만을 사용하여 경사를 계산합니다.

- **미니 배치:** 훈련 데이터셋에서 선택된 $m$개의 인스턴스 부분 집합입니다 (일반적으로 $m=16, 32, 64, \dots$).
- **장점:** 반복당 계산 시간이 전체 데이터셋 크기에 관계없이 미니 배치 크기 $m$에만 관련되므로, 대규모 데이터셋에서 훨씬 빠릅니다.
- **단점:** 비용 함수가 미니 배치에 따라 달라지기 때문에 경사 추정치가 매우 노이즈가 심하며, 학습률을 시간이 지남에 따라 점진적으로 감소시킬 필요가 있습니다.

### 1.4.4 SGD 관련 주요 용어

|용어|정의|
|:--|:--|
|**에포크 (Epoch)**|전체 훈련 데이터셋을 한 번 처리하는 것.|
|**미니 배치 (Minibatch)**|단일 파라미터 업데이트를 위해 사용되는 훈련 인스턴스의 수.|
|**반복 (Iteration)**|파라미터 업데이트를 위한 한 번의 미니 배치 처리 과정.|

### 1.4.5 역전파 (Backpropagation)

역전파는 신경망 학습에 **연쇄 법칙(Chain Rule)** 을 적용하는 과정입니다. 순전파를 통해 계산된 비용을 이용하여 출력층에서 입력층으로 거꾸로 전달하며 각 파라미터의 기울기(편미분 값)를 효율적으로 계산합니다. 이는 복잡하게 연결된 다층 신경망의 가중치를 업데이트하는 핵심적인 수학적 방법입니다.

### 1.4.6 학습률 개선 알고리즘

경사 하강 기반 알고리즘의 주요 문제는 학습률 $\epsilon$이 성능에 크게 영향을 미친다는 것입니다. 이를 해결하기 위해 학습률을 시간이 지남에 따라 감소시키거나, 파라미터별로 학습률을 조정하는 '적응적 학습률(Adaptive Learning Rate)' 방법이 개발되었습니다.

|알고리즘|특징|
|:--|:--|
|**모멘텀 (Momentum)**|과거 기울기의 지수 가중 이동 평균을 축적하여 해당 방향으로 계속 이동하게 합니다.|
|**AdaGrad**|과거 제곱 기울기의 누적 값에 따라 개별 파라미터의 학습률을 조정합니다. 역사적 기울기 값이 작은 파라미터에 더 큰 학습률을 부여합니다. 하지만 반복이 많아지면 학습률이 너무 작아질 수 있습니다.|
|**RMSProp**|AdaGrad의 수정 버전으로, 과거 제곱 기울기를 지수 가중 이동 평균으로 축적하여 오래된 기록을 버립니다.|
|**Adam (ADAptive Moments)**|RMSProp과 모멘텀을 결합한 알고리즘입니다.|

---
## 1.5 Calculus Fundamentals (미적분학 기초)

딥러닝에서 신경망이 가장 잘 작동하는 파라미터(가중치와 편향)를 찾는 과정, 즉 최적화(Optimization)는 기본적으로 비용 함수의 기울기를 따라 내려가는 과정입니다. 이러한 기울기를 계산하는 데 필요한 핵심적인 수학 도구가 바로 미적분학, 특히 미분(Derivative)과 기울기(Gradient) 개념입니다.

### 1.5.1 기울기(Slope)와 접선(Tangent)

**기울기 (Slope)** 직선의 기울기는 두 점 사이의 수직 변화량과 수평 변화량의 비율("rise over run")로 계산됩니다 $\left(\frac{\Delta y}{\Delta x}\right)$. 이는 우리가 일반적으로 알고 있는 변화의 정도를 나타내는 값입니다.

**접선 (Tangent Line)** 접선이란, 직관적으로 어떤 곡선 위의 주어진 한 점에서 **'단지 스치듯이'** 닿는 직선을 의미합니다. 이 접선의 기울기는 미분 개념으로 연결됩니다.

### 1.5.2 도함수/미분 (Derivative)

함수 $f$의 도함수(Derivative)는 미적분학의 핵심입니다.

- **정의:** 도함수는 주어진 입력 값에서 함수 그래프의 접선 기울기입니다.
- **변화율:** 도함수는 종종 순간적인 변화율(instantaneous rate of change)로 설명되는데, 이는 종속 변수의 순간 변화량과 독립 변수의 순간 변화량의 비율을 의미합니다.
- **수식적 정의:** 점 $a$에서의 도함수는 극한을 사용하여 정의됩니다. $$f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}$$
- **표기법:** $f'(a)$, $\frac{d}{dx} f(a)$, 또는 $\frac{df}{dx}(a)$ 등으로 표기할 수 있습니다.

### 1.5.3 미분 규칙 (Rules of Differentiation)

복잡한 함수를 미분하기 위해서는 몇 가지 주요 규칙을 사용합니다.

1. **멱함수의 법칙 (The Power Function Rule):** $y = f(x) = ax^n$ 형태의 함수에 대해 $\frac{dy}{dx} = nax^{n-1}$ 입니다.
2. **선형성 (Linearity):** $h(x) = af(x) + bg(x)$ 인 경우, $h'(x) = af'(x) + bg'(x)$ 가 성립합니다.
3. **연쇄 법칙 (Chain Rule):** 함수가 합성되어 있을 때 사용됩니다. $z = g(f(x))$ 이거나 $y = f(x)$ 이고 $z = g(y)$ 일 때, $\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}$ 로 계산됩니다. 신경망의 역전파(Backpropagation)에서 이 연쇄 법칙이 핵심적으로 사용됩니다.

### 1.5.4 편미분 (Partial Derivative)

함수가 여러 변수를 가질 때 사용되는 개념입니다.

- **정의:** 다변수 함수에서 특정 변수에 대해서만 미분하고, 나머지 변수들은 상수로 고정(held constant)하여 미분하는 것입니다.
- **표기법:** 함수 $f(x, y, \dots)$를 $x$에 대해 편미분하는 경우, $\frac{\partial}{\partial x} f(\mathbf{a})$ 또는 $\frac{\partial f}{\partial x}(\mathbf{a})$ 와 같이 표기합니다. 예를 들어, $z = f(x, y, \dots)$ 라면 $\frac{\partial z}{\partial x}$로 표기됩니다.

### 1.5.5 그래디언트 (Gradient)

그래디언트는 신경망 최적화의 방향을 결정하는 가장 중요한 개념입니다.

- **정의:** 스칼라 값 함수 $f: \mathbb{R}^n \to \mathbb{R}$에 대해 정의되며, 모든 편미분 값을 요소로 가지는 벡터입니다.
- **벡터 구성:** $\mathbf{p} = (x_1, \dots, x_n)$에서 그래디언트 $\nabla f(\mathbf{p})$는 다음과 같이 정의됩니다. $$\nabla f (\mathbf{p}) = \begin{pmatrix} \frac{\partial f}{\partial x_1}(\mathbf{p}) \ \vdots \ \frac{\partial f}{\partial x_n}(\mathbf{p}) \end{pmatrix}$$
- **의미:** 어떤 점 $\mathbf{p}$에서의 그래디언트 $\nabla f (\mathbf{p})$는 해당 점에서 함수 $f$가 **가장 빠르게 증가하는 방향과 속도**를 알려줍니다. 경사 하강법에서는 이 방향의 반대 방향으로 이동하여 비용을 최소화합니다.
- **규칙:** 미분의 선형성 및 연쇄 법칙은 그래디언트에도 적용됩니다.

---
## 1.6 Output Units (출력 단위)

출력 단위(Output Units)는 신경망 구조의 가장 마지막 단계에 위치하며, 마지막 은닉 계층 $\mathbf{h}$에서 계산된 특징들을 우리가 실제로 해결하고자 하는 대상 작업(target task)의 형식에 맞게 변환하여 최종 결과 $\hat{y}$를 도출하는 역할을 합니다. 즉, $\hat{y} = f^{(l)}(\mathbf{h})$ 형태의 최종 변환을 제공합니다.

어떤 출력 단위를 사용할지는 풀고자 하는 문제의 유형(회귀, 이진 분류, 다중 분류)에 따라 달라집니다.

### 1.6.1 선형 단위 (Linear Units)

선형 단위는 주로 **회귀(Regression)** 문제에 사용됩니다.

|특징|내용|
|:--|:--|
|**사용 목적**|타겟 $y$가 실수 값($y \in \mathbb{R}$)을 가지는 회귀 문제.|
|**수식**|마지막 은닉 계층 $\mathbf{h}$의 선형 변환 $\hat{y} = \mathbf{w}^T\mathbf{h} + b$ 으로 출력됩니다.|
|**특이 사항**|종종 정규 분포(Normal distribution)의 평균 파라미터를 생성하는 데 사용되기도 합니다.|

### 1.6.2 시그모이드 단위 (Sigmoid Units)

시그모이드 단위는 **이진 분류(Binary Classification)** 문제에 사용됩니다.

|특징|내용|
|:--|:--|
|**사용 목적**|타겟 $y$가 ${0, 1}$ 값을 가지는 이진 분류 문제.|
|**수식**|$y=1$일 확률을 출력하며, $\hat{y} = P(y=1|
|**특이 사항**|입력 $z$가 무한대로 커지면 $\hat{y}$는 1로 포화되고, $z$가 음의 무한대로 작아지면 $\hat{y}$는 0으로 포화됩니다.|

### 1.6.3 소프트맥스 단위 (Softmax Units)

소프트맥스 단위는 **다중 클래스 분류(Multi-class Classification)** 문제에 사용됩니다.

|특징|내용|
|:--|:--|
|**사용 목적**|타겟 $y$가 ${1, 2, \dots, c}$ 중 하나의 클래스에 속하는 다중 클래스 분류 문제.|
|**출력 형태**|$\hat{\mathbf{y}} = (\hat{y}_1, \dots, \hat{y}_c)$ 형태의 확률 벡터로 출력되며, 각 $\hat{y}_k$는 $y=k$일 확률을 나타냅니다.|
|**수식**|먼저 선형 변환 $\mathbf{z} = (z_1, \dots, z_c) = \mathbf{W}^T\mathbf{h} + \mathbf{b}$ 를 계산한 후, 각 $k$에 대해 다음 수식을 적용합니다:$$\hat{y}_k = \text{softmax}(\mathbf{z})_k = \frac{\exp(z_k)}{\sum_{j=1}^c \exp(z_j)}$$|
|**특이 사항**|모든 클래스 확률의 합은 반드시 1이 됩니다 ($\sum_{k=1}^c \hat{y}_k = 1$).|

