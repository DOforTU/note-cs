## Content

- **3.1 Optimizer 개요**
- **3.2 MyOptimizer 베이스 클래스 구현**
- **3.3 MySGD 구현**
- **3.4 SGD with Momentum 구현**
- **3.5 Adam Optimizer**
- **3.6 Fashion MNIST 예제 적용**

---

## 3.1 Optimizer 개요

### 3.1.1 Optimizer의 역할과 필요성

신경망 학습에서 Optimizer는 손실 함수를 최소화하기 위해 모델의 파라미터를 업데이트하는 알고리즘입니다. 역전파를 통해 계산된 그래디언트를 바탕으로 가중치를 조정하며, 학습의 효율성과 최종 성능에 큰 영향을 미칩니다.

Optimizer가 없다면 개발자가 모든 파라미터에 대해 수동으로 업데이트 코드를 작성해야 합니다. 이는 코드의 복잡도를 높이고 유지보수를 어렵게 만듭니다. Optimizer 객체를 사용하면 파라미터 업데이트 로직을 추상화하여 코드를 간결하게 만들고, 다양한 최적화 알고리즘을 쉽게 교체할 수 있습니다.

### 3.1.2 Optimizer 없는 코드와 사용 코드 비교

Polynomial Regression 예제에서 Optimizer를 사용하지 않은 코드는 다음과 같습니다:

```python
learning_rate = 1e-3
for t in range(4000):
    y_pred = compute_y_pred(x)
    loss = compute_loss(y, y_pred)
    
    if (t + 1) % 200 == 0:
        print(t + 1, loss.item())
    
    loss.backward()
    
    with torch.no_grad():
        a -= learning_rate * a.grad
        b -= learning_rate * b.grad
        c -= learning_rate * c.grad
        d -= learning_rate * d.grad
    
    a.grad = None
    b.grad = None
    c.grad = None
    d.grad = None
```

이 코드에서는 파라미터 $a, b, c, d$를 각각 수동으로 업데이트하고 그래디언트를 초기화해야 합니다. `torch.no_grad()` 컨텍스트 매니저를 사용하여 파라미터 업데이트 시 그래디언트 추적을 비활성화합니다.

반면 Optimizer 객체를 사용한 코드는 훨씬 간결합니다:

```python
optimizer = torch.optim.SGD([a, b, c, d], lr=1e-3)

for t in range(4000):
    y_pred = compute_y_pred(x)
    loss = compute_loss(y, y_pred)
    
    if (t + 1) % 200 == 0:
        print(t + 1, loss.item())
    
    loss.backward()
    optimizer.step() # 모든 파라미터를 자동으로 업데이트
    optimizer.zero_grad()
```

`optimizer.step()`은 모든 파라미터를 자동으로 업데이트하고, `optimizer.zero_grad()`는 모든 그래디언트를 한 번에 초기화합니다. 이러한 추상화를 통해 코드의 가독성이 크게 향상됩니다.

### 3.1.3 주요 Optimizer 종류

딥러닝에서 사용되는 대표적인 Optimizer는 다음과 같습니다.

**SGD (Stochastic Gradient Descent)** 는 가장 기본적인 최적화 알고리즘으로, 그래디언트의 반대 방향으로 파라미터를 업데이트합니다. 수식은 $\theta \leftarrow \theta - \epsilon g$이며, 여기서 $\epsilon$은 학습률(learning rate), $g$는 그래디언트입니다.

**SGD with Momentum**은 이전 업데이트 방향을 기억하여 관성을 추가합니다. 그래디언트의 이동 평균을 계산하여 진동을 줄이고 수렴 속도를 높입니다. Velocity $v$를 도입하여 $v \leftarrow \alpha v - \epsilon g$, $\theta \leftarrow \theta + v$로 업데이트합니다.

**Adam (Adaptive Moment Estimation)** 은 가장 널리 사용되는 Optimizer 중 하나로, 각 파라미터마다 적응적인 학습률을 적용합니다. First moment(그래디언트의 이동 평균)와 second moment(그래디언트 제곱의 이동 평균)를 모두 활용하여 안정적이고 빠른 학습을 가능하게 합니다.

---

## 3.2 MyOptimizer 베이스 클래스 구현

### 3.2.1 추상 베이스 클래스 설계

Optimizer의 공통 기능을 추상화하기 위해 `MyOptimizer` 베이스 클래스를 설계합니다. Python의 ABC(Abstract Base Class) 모듈을 사용하여 추상 클래스를 정의하며, 모든 Optimizer가 구현해야 하는 인터페이스를 명시합니다.

```python
from abc import ABC, abstractmethod
from typing import Iterable

class MyOptimizer(ABC):
    def __init__(self, params: Iterable[torch.nn.Parameter], lr: float):
        self.params = list(params)
        self.lr = lr
    
    def zero_grad(self) -> None:
        for p in self.params:
            p.grad = None
    
    @torch.no_grad()
    def step(self) -> None:
        for p in self.params:
            if p.grad is None:
                continue
            self._update_param(p, p.grad)
    
    # 추상 메서드는 베이스 클래스에서 구현되지 않음
    @abstractmethod
    def _update_param(self, p: torch.nn.Parameter, grad: torch.Tensor) -> None:
        ...
```

### 3.2.2 `__init__` 메서드

생성자는 최적화할 파라미터 리스트와 학습률을 받아 초기화합니다. `params`는 `Iterable[torch.nn.Parameter]` 타입으로, 모델의 모든 학습 가능한 파라미터를 전달받습니다. `list(params)`를 통해 이터레이터를 리스트로 변환하여 저장하며, `lr`은 학습률(learning rate)을 나타냅니다.

### 3.2.3 `zero_grad()` 메서드

`zero_grad()` 메서드는 모든 파라미터의 그래디언트를 초기화합니다. 각 파라미터 `p`의 `.grad` 속성을 `None`으로 설정하여 이전 반복에서 누적된 그래디언트를 제거합니다. 이는 매 학습 스텝마다 새로운 그래디언트를 계산하기 전에 반드시 수행되어야 합니다.

### 3.2.4 `step()` 메서드와 `@torch.no_grad()` 데코레이터

`step()` 메서드는 계산된 그래디언트를 사용하여 파라미터를 업데이트합니다. `@torch.no_grad()` 데코레이터는 이 메서드 내에서 수행되는 모든 연산에 대해 그래디언트 추적을 비활성화합니다. 파라미터 업데이트는 학습 과정의 일부가 아니므로 계산 그래프에 포함될 필요가 없으며, 이를 통해 메모리를 절약하고 성능을 향상시킵니다.

메서드 내부에서는 모든 파라미터를 순회하며, 그래디언트가 `None`이 아닌 경우에만 `_update_param()`을 호출합니다. 이는 일부 파라미터가 특정 배치에서 사용되지 않아 그래디언트가 계산되지 않는 경우를 처리합니다.

### 3.2.5 `_update_param()` 추상 메서드

`_update_param()` 메서드는 `@abstractmethod` 데코레이터로 표시된 추상 메서드입니다. 이 메서드는 베이스 클래스에서 구현되지 않으며, 하위 클래스에서 반드시 구현해야 합니다. 각 Optimizer의 고유한 업데이트 규칙을 여기에 정의합니다.

메서드 이름이 언더스코어(`_`)로 시작하는 것은 Python의 관례로, 이 메서드가 내부적으로만 사용되는 private 메서드임을 나타냅니다. 외부에서 직접 호출하지 않고 `step()` 메서드 내부에서만 사용됩니다.

---

## 3.3 MySGD 구현

### 3.3.1 [[SGD]]의 원리

SGD(Stochastic Gradient Descent)는 가장 기본적인 최적화 알고리즘입니다. 손실 함수의 그래디언트를 계산하고, 그 반대 방향으로 파라미터를 이동시킵니다. 업데이트 수식은 $\theta \leftarrow \theta - \epsilon \nabla_\theta L(\theta)$이며, 여기서 $\theta$는 파라미터, $\epsilon$은 학습률, $\nabla_\theta L(\theta)$는 손실 함수 $L$의 그래디언트입니다.

> **Algorithm 8.1** Stochastic gradient descent (SGD) update
> **Require**: Learning rate schedule $\epsilon_1, \epsilon_2, \ldots$ 
> **Require**: Initial parameter $\boldsymbol{\theta}$
> 	$k \leftarrow 1$
> 	**while** stopping criterion not met **do**
> 		- ==important==: Sample a minibatch of $m$ examples from the training set ${\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)}}$ with corresponding targets $\boldsymbol{y}^{(i)}$. 
> 		- Compute gradient estimate: $\hat{\boldsymbol{g}} \leftarrow \frac{1}{m}\nabla_{\boldsymbol{\theta}} \sum_i L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)})$
> 		- Apply update: $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \epsilon_k \hat{\boldsymbol{g}}$
> 		- $k \leftarrow k + 1$
> 	**end while**

### 3.3.2 MySGD 클래스 구현

`MySGD`는 `MyOptimizer`를 상속받아 구현됩니다:

```python
class MySGD(MyOptimizer):
    def _update_param(self, p: torch.nn.Parameter, grad: torch.Tensor) -> None:
        p -= self.lr * grad
```

`_update_param()` 메서드에서 파라미터 `p`를 학습률과 그래디언트의 곱만큼 감소시킵니다. `p -= self.lr * grad`는 in-place 연산으로, 기존 파라미터를 직접 수정합니다. 이는 `p = p - self.lr * grad`와 달리 새로운 텐서를 생성하지 않으므로 메모리 효율적입니다. 

> `torch.no_grad()` 블록 내에서는 **그래디언트 추적이 비활성화**되므로, in-place 연산을 해도 계산 그래프에 영향을 주지 않습니다.

### 3.3.3 MySGD 사용 예시

Fashion MNIST 데이터셋에서 `MySGD`를 사용하는 예시는 다음과 같습니다:

```python
model = NeuralNetwork()
optimizer = MySGD(model.parameters(), lr=1e-3)

for epoch in range(10):
    for X, y in dataloader:
        pred = model(X)
        loss = loss_fn(pred, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

`model.parameters()`는 모델의 모든 학습 가능한 파라미터를 반환하며, 이를 optimizer에 전달합니다. 학습 루프에서 `zero_grad()`, `backward()`, `step()`의 3단계를 순차적으로 수행합니다.

---

## 3.4 SGD with Momentum 구현

### 3.4.1 Momentum의 개념

Momentum은 경사 하강법에 관성을 추가하는 기법입니다. 단순 SGD는 그래디언트의 방향이 급격하게 변할 때 진동하며 수렴 속도가 느려질 수 있습니다. Momentum은 이전 **업데이트 방향을 기억하여 일관된 방향으로 더 빠르게 이동**하도록 돕습니다.

물리학에서 공이 경사면을 굴러 내려가는 상황을 상상해보면, 공은 속도를 얻어 관성으로 계속 앞으로 나아갑니다. 마찬가지로 Momentum은 velocity(속도) 변수 $v$를 도입하여 그래디언트의 지수 이동 평균을 계산합니다.

### 3.4.2 Momentum 알고리즘

[[SGDMomentum]]의 업데이트 규칙은 다음과 같습니다:

$$v \leftarrow \alpha v - \epsilon g$$ $$\theta \leftarrow \theta + v$$

여기서 $\alpha$는 momentum parameter(모멘텀 파라미터)로, 보통 0.9 정도의 값을 사용합니다. $v$는 velocity로, 이전 velocity에 $\alpha$를 곱하고 현재 그래디언트에 학습률을 곱한 값을 더합니다. 최종적으로 파라미터 $\theta$는 velocity만큼 이동합니다.

> **Algorithm 8.2** Stochastic gradient descent (SGD) with momentum
> **Require**: Learning rate $\epsilon$, momentum parameter $\alpha$. **Require**: Initial parameter $\boldsymbol{\theta}$, initial velocity $\boldsymbol{v}$.
> **while** stopping criterion not met **do**
> 	- Sample a minibatch of $m$ examples from the training set ${\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)}}$ with corresponding targets $\boldsymbol{y}^{(i)}$.
> 	- Compute gradient estimate: $\boldsymbol{g} \leftarrow \frac{1}{m}\nabla_{\boldsymbol{\theta}} \sum_i L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \boldsymbol{y}^{(i)})$
> 	- ==important==: Compute velocity update: $\boldsymbol{v} \leftarrow \alpha \boldsymbol{v} - \epsilon \boldsymbol{g}$ 
> 	- Apply update: $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \boldsymbol{v}$
> **end while**

### 3.4.3 상태(State) 관리

Momentum을 구현하려면 각 파라미터마다 velocity를 저장해야 합니다. 이를 위해 `state`라는 딕셔너리를 사용하여 파라미터별 상태를 관리합니다:

```python
class MySGDMomentum(MyOptimizer):
    def __init__(self, params: Iterable[torch.nn.Parameter], lr: float, momentum: float = 0.9):
        super().__init__(params, lr)
        self.momentum = momentum
        self.state = {}
        
        for p in self.params:
            self.state[p] = {'velocity': torch.zeros_like(p)}
    
    def _update_param(self, p: torch.nn.Parameter, grad: torch.Tensor) -> None:
        v = self.state[p]['velocity']
        v.mul_(self.momentum).add_(grad)  # v ← αv + g
        p.add_(v, alpha=-self.lr)  # θ ← θ - ε*v
```

`__init__` 메서드에서 각 파라미터에 대해 velocity를 0으로 초기화합니다. `torch.zeros_like(p)`는 파라미터와 같은 크기의 0으로 채워진 텐서를 생성합니다.

`_update_param()` 메서드에서는 먼저 해당 파라미터의 velocity를 가져옵니다. `v.mul_(self.momentum)`은 velocity에 momentum parameter를 곱하고, `.add_(grad, alpha=-self.lr)`은 그래디언트에 학습률을 곱한 값을 더합니다. 마지막으로 `p.add_(v)`로 파라미터를 velocity만큼 업데이트합니다.

### 3.4.4 In-place 연산의 활용

코드에서 `.mul_()`, `.add_()`와 같이 언더스코어가 붙은 메서드들은 in-place 연산입니다. 이들은 새로운 텐서를 생성하지 않고 기존 텐서를 직접 수정하여 메모리를 절약합니다. `@torch.no_grad()` 데코레이터가 적용된 컨텍스트 내에서 안전하게 사용할 수 있습니다.

---

## 3.5 [[Adam]] Optimizer

### 3.5.1 Adam의 원리

Adam(Adaptive Moment Estimation)은 각 파라미터에 대해 적응적인 학습률을 적용하는 Optimizer입니다. SGD with Momentum이 first moment(그래디언트의 이동 평균)만 사용하는 반면, Adam은 second moment(그래디언트 제곱의 이동 평균)도 함께 사용합니다.

Adam은 그래디언트의 크기가 작은 파라미터는 큰 학습률로, 큰 파라미터는 작은 학습률로 업데이트하여 학습을 안정화합니다. 이러한 적응적 학습률 덕분에 다양한 문제에서 좋은 성능을 보이며, 하이퍼파라미터 튜닝에 덜 민감합니다.

### 3.5.2 Adam 알고리즘

Adam의 업데이트 규칙은 다음과 같습니다:

$$m_t \leftarrow \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$$ $$v_t \leftarrow \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$$ $$\hat{m}_t \leftarrow \frac{m_t}{1 - \beta_1^t}$$ $$\hat{v}_t \leftarrow \frac{v_t}{1 - \beta_2^t}$$ $$\theta_t \leftarrow \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

여기서 $m_t$는 first moment(그래디언트의 지수 이동 평균), $v_t$는 second moment(그래디언트 제곱의 지수 이동 평균)입니다. $\beta_1$과 $\beta_2$는 exponential decay rates로, 일반적으로 $\beta_1 = 0.9$, $\beta_2 = 0.999$를 사용합니다.

> **Algorithm 1**: Adam, our proposed algorithm for stochastic optimization
> **Require**: $\alpha$: Stepsize 
> **Require**: $\beta_1, \beta_2 \in [0, 1)$: Exponential decay rates for the moment estimates 
> **Require**: $f(\theta)$: Stochastic objective function with parameters $\theta$ 
> **Require**: $\theta_0$: Initial parameter vector
> 	- $m_0 \leftarrow 0$ (Initialize 1st moment vector) 
> 	- $v_0 \leftarrow 0$ (Initialize 2nd moment vector) 
> 	- $t \leftarrow 0$ (Initialize timestep)
> **while** $\theta_t$ not converged **do** 
> 	- $\quad t \leftarrow t + 1$ 
> 	- $\quad g_t \leftarrow \nabla_\theta f_t(\theta_{t-1})$ (Get gradients w.r.t. stochastic objective at timestep ) 
> 	- $\quad m_t \leftarrow \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$ (Update biased first moment estimate) 
> 	- $\quad v_t \leftarrow \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$ (Update biased second raw moment estimate) 
> 	- $\quad \hat{m}_t \leftarrow m_t/(1 - \beta_1^t)$ (Compute bias-corrected first moment estimate) 
> 	- $\quad \hat{v}_t \leftarrow v_t/(1 - \beta_2^t)$ (Compute bias-corrected second raw moment estimate) 
> 	- $\quad \theta_t \leftarrow \theta_{t-1} - \alpha \cdot \hat{m}_t/(\sqrt{\hat{v}_t} + \epsilon)$ (Update parameters) 
> **end while**
> **return** $\theta_t$ (Resulting parameters)

Good default settings for the tested machine learning problems are $\alpha = 0.001$, $\beta_1 = 0.9$, $\beta_2 = 0.999$ and $\epsilon = 10^{-8}$. All operations on vectors are element-wise. With $\beta_1^t$ and $\beta_2^t$ we denote $\beta_1$ and $\beta_2$ to the power $t$.

### 3.5.3 Bias Correction

$\hat{m}_t$와 $\hat{v}_t$는 bias-corrected moment estimates입니다. 초기에 $m_0 = 0$, $v_0 = 0$으로 초기화하면 초반에 이 값들이 0에 가깝게 편향(bias)됩니다. Bias correction은 timestep $t$를 고려하여 이러한 편향을 보정합니다.

$1 - \beta_1^t$와 $1 - \beta_2^t$로 나누어주면, 초기 timestep에서는 큰 값으로 나누어져 편향이 제거됩니다. 예를 들어 $t=1$일 때 $1 - \beta_1^1 = 1 - 0.9 = 0.1$로 나누므로, 실제 moment 값이 10배 증폭되어 편향이 사라집니다.

### 3.5.4 하이퍼파라미터

Adam의 주요 하이퍼파라미터는 다음과 같습니다:

- $\alpha$ (stepsize 또는 learning rate): 일반적으로 0.001을 사용합니다.
- $\beta_1$: First moment의 exponential decay rate로, 기본값은 0.9입니다.
- $\beta_2$: Second moment의 exponential decay rate로, 기본값은 0.999입니다.
- $\epsilon$: 수치 안정성을 위한 작은 상수로, 보통 $10^{-8}$을 사용합니다.

$\epsilon$은 분모가 0이 되는 것을 방지하여 나눗셈 에러를 막습니다. $\sqrt{\hat{v}_t}$가 매우 작을 때 발생할 수 있는 불안정성을 제거합니다.

### 3.5.5 Adam 구현

Adam을 구현하면 다음과 같습니다:

```python
class MyAdam(MyOptimizer):
    def __init__(self, params: Iterable[torch.nn.Parameter], lr: float = 1e-3,
                 beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):
        super().__init__(params, lr)
        self.beta1 = beta1  # First moment의 exponential decay rate
        self.beta2 = beta2  # Second moment의 exponential decay rate
        self.eps = eps      # 수치 안정성을 위한 작은 상수
        self.state = {}     # 각 파라미터의 상태 저장 딕셔너리
        
        # 각 파라미터마다 first moment, second moment, timestep 초기화
        for p in self.params:
            self.state[p] = {
                'm': torch.zeros_like(p),  # first moment (그래디언트의 이동 평균)
                'v': torch.zeros_like(p),  # second moment (그래디언트 제곱의 이동 평균)
                't': 0                      # timestep (bias correction용)
            }
    
    def _update_param(self, p: torch.nn.Parameter, grad: torch.Tensor) -> None:
        # 현재 파라미터의 상태 가져오기
        state = self.state[p]
        m, v, t = state['m'], state['v'], state['t']
        
        # Timestep 증가 (bias correction 계산에 사용)
        t += 1
        state['t'] = t
        
        # First moment 업데이트: m ← β₁·m + (1-β₁)·g
        # m.mul_(self.beta1): m ← β₁·m (기존 moment에 decay rate 적용)
        # .add_(grad, alpha=1-self.beta1): m ← m + (1-β₁)·g (현재 gradient 추가)
        m.mul_(self.beta1).add_(grad, alpha=1 - self.beta1)
        
        # Second moment 업데이트: v ← β₂·v + (1-β₂)·g²
        # v.mul_(self.beta2): v ← β₂·v (기존 moment에 decay rate 적용)
        # .addcmul_(grad, grad, value=1-self.beta2): v ← v + (1-β₂)·(g⊙g)
        # addcmul_은 element-wise 곱셈 후 덧셈: tensor.addcmul_(t1, t2, value=α) → tensor + α·(t1⊙t2)
        v.mul_(self.beta2).addcmul_(grad, grad, value=1 - self.beta2)
        
        # Bias correction 적용
        # 초기 timestep에서 0으로 편향된 moment를 보정
        m_hat = m / (1 - self.beta1 ** t)  # m̂ ← m / (1 - β₁ᵗ)
        v_hat = v / (1 - self.beta2 ** t)  # v̂ ← v / (1 - β₂ᵗ)
        
        # 파라미터 업데이트: θ ← θ - α·m̂/(√v̂ + ε)
        # p.addcdiv_(m_hat, v_hat.sqrt().add_(self.eps), value=-self.lr)
        # addcdiv_는 element-wise 나눗셈 후 덧셈: tensor.addcdiv_(t1, t2, value=α) → tensor + α·(t1/t2)
        # v_hat.sqrt().add_(self.eps): √v̂ + ε (분모가 0이 되는 것 방지)
        # value=-self.lr: 학습률에 음수를 곱해 gradient descent 방향으로 이동
        p.addcdiv_(m_hat, v_hat.sqrt().add_(self.eps), value=-self.lr)
```

#### 3.5.5.1 PyTorch의 In-place 연산 메서드

Adam 구현에서 사용된 PyTorch의 in-place 연산 메서드들을 자세히 살펴보겠습니다.

**`mul_(scalar)`**: 텐서의 모든 요소에 스칼라를 곱합니다. `m.mul_(self.beta1)`은 $m \leftarrow \beta_1 \cdot m$을 수행합니다.

**`add_(tensor, alpha=value)`**: 텐서에 다른 텐서를 더합니다. `alpha` 파라미터는 더해지는 텐서에 곱해지는 스칼라입니다. `m.add_(grad, alpha=1-self.beta1)`은 $m \leftarrow m + (1-\beta_1) \cdot g$를 수행합니다.

**`addcmul_(tensor1, tensor2, value=alpha)`**: Element-wise 곱셈 후 덧셈을 수행합니다. 수식으로는 `tensor ← tensor + α·(tensor1 ⊙ tensor2)`입니다. `v.addcmul_(grad, grad, value=1-self.beta2)`는 $v \leftarrow v + (1-\beta_2) \cdot (g \odot g)$를 수행하며, 이는 $v \leftarrow v + (1-\beta_2) \cdot g^2$와 동일합니다.

**`addcdiv_(tensor1, tensor2, value=alpha)`**: Element-wise 나눗셈 후 덧셈을 수행합니다. 수식으로는 `tensor ← tensor + α·(tensor1 / tensor2)`입니다. `p.addcdiv_(m_hat, v_hat.sqrt().add_(self.eps), value=-self.lr)`는 $\theta \leftarrow \theta + (-\alpha) \cdot \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon}$를 수행하며, 이는 $\theta \leftarrow \theta - \alpha \cdot \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon}$와 동일합니다.

#### 3.5.5.2 Bias Correction의 동작 원리

Bias correction이 필요한 이유를 구체적인 예시로 살펴보겠습니다. 첫 번째 timestep($t=1$)을 가정하면, $m_0 = 0$에서 시작하여 $m_1 = \beta_1 \cdot 0 + (1-\beta_1) \cdot g_1 = (1-\beta_1) \cdot g_1$이 됩니다. $\beta_1 = 0.9$일 때 $m_1 = 0.1 \cdot g_1$로, 실제 그래디언트보다 10배 작은 값이 됩니다.

Bias correction을 적용하면 $\hat{m}_1 = \frac{m_1}{1-\beta_1^1} = \frac{0.1 \cdot g_1}{0.1} = g_1$이 되어, 원래의 그래디언트 크기를 회복합니다. Timestep이 증가할수록 $\beta_1^t$는 0에 가까워지므로 $1-\beta_1^t$는 1에 가까워지고, bias correction의 효과는 점차 사라집니다.

예를 들어 $t=10$일 때 $1-0.9^{10} \approx 0.651$이며, $t=100$일 때는 $1-0.9^{100} \approx 0.99997$로 거의 1에 가까워집니다. 따라서 초기 학습 단계에서만 bias correction이 중요한 역할을 하며, 학습이 진행될수록 그 영향은 미미해집니다.

#### 3.5.5.3 메모리 효율성

모든 연산이 in-place로 수행되므로 추가 메모리 할당 없이 기존 텐서를 직접 수정합니다. `m_hat`과 `v_hat`만 새로운 텐서로 생성되지만, 이는 bias correction을 위해 필요한 임시 변수이며 파라미터 업데이트 후 즉시 해제됩니다. 각 파라미터당 2개의 moment 텐서(m, v)와 1개의 timestep 정수만 추가로 저장되므로, 메모리 오버헤드가 크지 않습니다.

---

## 3.6 Fashion MNIST 예제 적용

### 3.6.1 Fashion MNIST 데이터셋

Fashion MNIST는 의류 이미지 분류를 위한 데이터셋으로, 28×28 크기의 흑백 이미지와 10개의 클래스 레이블로 구성됩니다. 클래스는 티셔츠(T-shirt/top), 바지(Trouser), 풀오버(Pullover), 드레스(Dress), 코트(Coat), 샌들(Sandal), 셔츠(Shirt), 스니커즈(Sneaker), 가방(Bag), 앵클 부츠(Ankle boot)를 포함합니다.

이 데이터셋은 기존 코드를 재활용하여 다양한 Optimizer의 성능을 비교하는 데 적합합니다.

### 3.6.2 기본 학습 코드 구조

Fashion MNIST를 사용한 기본 학습 코드는 다음과 같습니다:

```python
from torchvision import datasets
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader

# 데이터 로드
training_data = datasets.FashionMNIST(
    root="data", train=True, download=True, transform=ToTensor()
)
test_data = datasets.FashionMNIST(
    root="data", train=False, download=True, transform=ToTensor()
)

# DataLoader 생성
train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64)

# 모델 정의
model = NeuralNetwork()
loss_fn = nn.CrossEntropyLoss()

# Optimizer 선택
optimizer = MySGD(model.parameters(), lr=1e-3)
# optimizer = MySGDMomentum(model.parameters(), lr=1e-3, momentum=0.9)
# optimizer = MyAdam(model.parameters(), lr=1e-3)

# 학습
epochs = 10
for epoch in range(epochs):
    model.train()
    for X, y in train_dataloader:
        pred = model(X)
        loss = loss_fn(pred, y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 3.6.3 다양한 Optimizer 성능 비교

동일한 모델과 하이퍼파라미터 설정에서 서로 다른 Optimizer를 사용하여 학습 곡선을 비교할 수 있습니다. 일반적으로 다음과 같은 경향이 관찰됩니다:

**My[[SGD]]** 는 가장 느린 수렴 속도를 보이며, 학습 곡선이 진동할 수 있습니다. 단순한 구조로 인해 복잡한 최적화 문제에서는 local minimum에 빠지기 쉽습니다.

**My[[SGDMomentum]]** 은 SGD보다 빠른 수렴 속도를 보이며, 진동이 감소합니다. Momentum이 일관된 방향으로 파라미터를 이동시켜 안정적인 학습을 가능하게 합니다.

**My[[Adam]]** 은 가장 빠르고 안정적인 수렴을 보이는 경우가 많습니다. 적응적 학습률 덕분에 초기 하이퍼파라미터 설정에 덜 민감하며, 대부분의 딥러닝 문제에서 좋은 기본 선택지가 됩니다.

그 외에도 [[AdaGrad]]와 [[RMSProp]]은 각각 고유한 특성을 가집니다. AdaGrad는 그래디언트 제곱의 누적 합을 사용하여 각 파라미터에 적응적 학습률을 적용하지만, 학습이 진행될수록 학습률이 지나치게 감소하여 학습이 조기에 정체될 수 있습니다. 

RMSProp은 AdaGrad의 이러한 문제를 해결하기 위해 지수 이동 평균을 사용하여 최근 그래디언트에 더 큰 가중치를 부여합니다. 이는 Adam의 second moment와 유사한 메커니즘이며, 비순환 신경망(non-recurrent networks)에서 좋은 성능을 보입니다.

### 3.6.4 Optimizer 선택 가이드

![[optimizer.png]]

실무에서 Optimizer를 선택할 때는 다음 사항을 고려합니다:

학습 속도가 중요하고 빠른 프로토타이핑이 필요한 경우 Adam을 사용하는 것이 좋습니다. 최종 성능을 극대화하고자 할 때는 SGD with Momentum을 신중하게 튜닝하는 것이 더 나은 결과를 낼 수 있습니다. 메모리가 제한된 환경에서는 단순 SGD가 유리할 수 있습니다.

일반적으로 Adam을 기본 Optimizer로 시작하고, 필요에 따라 다른 옵션을 실험하는 것이 권장됩니다.