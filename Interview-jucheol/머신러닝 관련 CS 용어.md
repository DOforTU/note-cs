
**REST API** - **Fast API**로 만든 HTTP기반이고 서버와 클라이언트가 데이터를 주고 받는 설계 원칙이다. 

RAG - LLM과 검색 엔진을 결합하여 외부 문서, DB를 실시간으로 검색하고 그 결과를 바탕으로 답변을 생성하는 AI 아키텍쳐

LangChain - 대규모 언어 모델을 활용한 애플리케이션 개발을 쉽게 만들어주는 오픈소스 프레임워크이다. (LangChain은 "프레임워크"이지만, 개발자는 파이썬에서 "라이브러리"처럼 import해서 사용합니다. 즉, 두 개념이 동시에 적용된다고 볼 수 있습니다.)

파인튜닝 - 이미 학습된 AI 모델에 특정 데이터셋을 추가로 학습시켜, 원하는 도메인이나 테스크에 맞게 성능을 최적화하는 과정

LLM - 대규모 텍스트(언어) 데이터로 학습된 인공지능 언어 모델 <GPT, BERT, PaLM 등이 있음

AI - 인간의 지능적 행동을 컴퓨터가 수행하는 기술 <머신러닝, 딥러닝, 자연어처리, 데이터 분석 등이 있음> 챗봇, 추천 시스템, 예측 모델 등 활용가능

Docker - 애플리케이션을 컨테이너라는 격리된 환경에 패키징 -> 어디서나 일관된 환경에서 실행할 수 있게 해주는 오픈소스 플랫폼

EC2 - AWS에서 제공하는 가상 서버 서비스 (클라우드 인스턴스)

TypeOrm - 서버와 데이터 베이스 연결을 편하게 하기 위한 연결 방법 / 단점으로는 조인이 안되고 조회만 되기에 create builder를 사용하여 위 문제를 해결할 수 있습니다.

Load Balance - 요청이 많아서 과부하걸릴 때 분산시켜 서버 과부하를 피하는 도구이다. (클라우드)
-> 자주 조회되는거는 캐시에 저장하여 db에 과부하를 줄일수 있다. 기존 서버를 scale out하여 복제하여 요청을 분산시켜 해결하는것이다. 

#### 머신러닝 (딥러닝 포함) 수학 관한 질문 및 답변

- CNN은 합성곱 층, 풀링 층, 밀집 층으로 구성되어있다. **이미지 처리와 패턴 인식에 성능을 보여주는 신경망**이며 합성곱 층에서 이루어짐. **입력 이미지와 작은 크기의 필터(커널) 간의 합성곱 연산**을 통해 새로운 **특정 맵을 생성**. 합성곱 레이어는 이미지의 지역적 특성(local feature - 직선이면 직선 곡선이면 곡선) 을 분석하여 확률로 리턴하는 역할. 합성곱 층이 깊어질 수록 복잡한  이미지를 분류할 수 있다. 커널과 stride(보폭)은 합성곱 층의 출력층의 크기를 조절하는 요소. **피처 맵의 크기가 크면 복잡한 이미지를 분석할 수 있지만 연산량이 많아서 오래 걸리면 부정적인 요소**가 됨. 적절한 크기가 중요. 이미지의 픽셀수가 많을수록 디테일한 이미지가 된다. 피처맵의 크기가 크면 feature의 디테일을 표현할 수 있지만, 연산량이 많아져 적절한 크기를 찾는게 중요. stride가 1일 경우 한칸 씩 움직이며 합성곱 연산을 함. 커널의 크기가 같아도 보폭에 따라 피처 맵의 크기가 달라짐. 이 둘은 연산량과 모델의 정확도 사이에서 밸런스를 맞춰야함.
  
- RNN은 연속적인 데이터를 가지고 순서를 고려하여 분석, 예측하며, 앞 단계의 출력을 다음 단계의 입력에 반영하여 이전 정보를 기억하여 활용한다. 레이어를 깊게 쌓으면 더 복잡한 패턴이나 장기 의존성을 학습할 수 있다. 단어예측, 문장생성, 시계열 예측 등 널리 사용된다. 
  
- Pytorch - 페이스북에서 개발, 동적 연산 그래프이며, 코드가 실행될 때 그래프가 생성, 디버깅과 실험이 쉽고 직관적인 장점을 가졌다. 빠른 프로토타이핑 강점, 자연어 처리,  
  
- Tensorflow - 구글에서 개발, 그래프 기반 연산을 정의하며, 복잡한 모델과 대규모 분산 학습에 강점입니다. Keras는 텐서플로우의 공식 고수준 API로, 직관적이고 쉬운 신경망 설계가 가능하다. 

- GAN - 딥러닝에서 데이터를 생성하는 데 사용되는 대표적인 생성 모델이다. 두 개의 신경망인 생성자와 판별자가 서로 경쟁하며 함께 발전하는 모델입니다. 생성자는 진짜처럼 보이는 가짜 데이터를 생성하고, 판별자는 실제 데이터인지, 생성자가 만든 데이터인지 구별합니다. 

- crossentropyloss - 손실함수이며, 효율적인 계산을 하는 손실함수 라이브러리이다.
  
- optimizer - 신경망의 학습과정에서 발생하는 문제를 해결하기 위한 기법들 (adam이 흔한 옵티마이저 모델)
  
- 동기 / 비동기 - 동기는 작업이 순차적으로 진행되는 것을 의미한다. 한 작업이 시작되면 완료될 때가지 다른 작업을 기다려야 함. 여러 작업이 동시에 실행되야 하는 경우 간단하고 직관적인 코드를 작성하기 쉽다. 기다리는 동안 시간이 소요 되고 전체 프로세스의 성능이 저하될 수 있다. 한 작업이 지연되면 다른 작업도 모두 지연되는 문제가 발생할 수 있음. 
  
  비동기는 동시에 일어나지 않고 독립적으로 실행된다. 완료 여부를 기다리지 않고 다른 작업을 실행할 수 있는 방식을 의미한다. 작업이 완료될 때까지 기다리지 않고 다음 코드를 실행하여 전체적인 성능이 향상된다. 
  - 싱글 스레드는 비동기 방식에서 유용하고 입출력이 오는데 오래걸리지만 서버가 멈추지 않고 다른 요청을 동시에 수행할 수 있기에 웹 개발에서 유용하여 노드js를 선택했습니다.
  
- attention - 입력 시퀀스를 모두 저장했다가 어느 부분에 더 집중해야하는지 스스로 계산해서 중요한 부분의 정보를 더 많이 반영하는 메커니즘
  
- LSTM - 입력 게이트와 망각 게이트, 출력 게이트로 구성되어있다. 장기와 단기 정보를 모두 기억하고 위 모델은 정보가 필요한지 아닌지 스스로 판단 후 중요한 정보만 남기고 제거 후 오랫동안 기억할 데이터 특성을 효과적으로 학습하는 신경망. 
  
- 경사하강법 - 손실함수를 최소화하는 가중치를 찾는 방법 
 
#### 통계적 개념
-  귀무가설 - 통계적 검정에서 기본적으로 "차이가 없다", "효과가 없다"는 전제를 세우는 가설.
  
-  대립가설 - 귀무가설을 기각한다는 의미, 새로운 주장을 입증하기위해 귀무가설을 검증하고 통계적으로 반박해야 대립가설이 신뢰를 받을 수 있다. (P-값이 작게 나오면 기각)
  
- 편미분 - 3차원 식에서 사용 / 
 
-  선형회귀 (직선 / 선형 연속적인 숫자값) - 독립변수와 종속변수간의 선형관계를 분석하여 최적의 회귀선을 찾아낸다. 
	y=mx +b 
	경사하강법 - 최적화 기법 / 최적의 매개변수 값을 지정 / 비용함수 (예측한 값과 실제 값 사이의 차이 측정) 를 사용하여 평균 제곱 오차를 사용한다. 기울기가 낮아지는 방향으로 찾아간다.
	등분산성이 중요함 분산이 동일, 비슷한 경우 선형회귀 예측력이 타당. 분류는 극단값에 영향을 받아 별로임.

-  로지스틱 회귀 (분류 문제 / 범주) - 어떤 변수간의 상관관계를 통해 비어있는 부분을 예측할때사용하는 회귀 분석 방법(이항 분류 문제에서 사용), S자 형태의 확률분포로 데이터를 최적화함. 이진분포도 잘 최적화함.  
   극단적인 값에 영향 없음 -> 시그모이드 (확률분포 이름) 함수. 이진분류와 같은 경우 확률과 같이 중요하게 생각하는게 Odds 승산비이다. P를 1-P로 나눈 값 (일어날 확률 P) -> 승산비 
   Odds에 로그를 취하면 logit이 된다. 이를 그래프로 하면 선형의 일차함수가 된다. 로짓과 선형함수가 합쳐서 로지스틱 회귀가 된다. 시그모이드 함수와 선형함수와 그 둘을 매개하는 로짓의 관계를 아는 것이 핵심. 

결론 회귀는 연속적인 출력 변수를 예측하는데 사용 / 분류는 이산적인 출력 변수를 예측하는데 사용

전기 사용량 예측 공모전에서 회귀용 모델을 사용 숫자를 예측하는 경우이기 때문

