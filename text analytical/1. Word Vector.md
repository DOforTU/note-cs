## Contents

- **1.1 이산 기호로 단어 표현의 문제점**
- **1.2 단어 벡터 및 임베딩**
- **1.3 문맥에 의한 단어 표현 (분포 의미론)**
- **1.4 Word2vec 프레임워크**
- **1.5 Word2vec 목적 함수 및 최적화**
- **1.6 네거티브 샘플링을 사용한 Skip-gram**

---

# 1.1 이산 기호로 단어 표현의 문제점

## 1.1.1 전통적 NLP 접근

전통적인 자연어 처리(NLP)에서는 단어를 이산적인(discrete) 기호로 취급합니다. 각 단어는 고유한 심볼로 간주되며, 이를 컴퓨터가 처리할 수 있도록 원-핫(one-hot) 벡터로 표현합니다. 원-핫 벡터는 어휘 사전(vocabulary)의 크기만큼의 차원을 가지며, 해당 단어에 해당하는 위치만 1이고 나머지는 모두 0인 형태입니다.

예를 들어, "motel"과 "hotel"이라는 단어를 원-핫 벡터로 표현하면 다음과 같습니다:

- `motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]`
- `hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]`

이 방식은 단순하고 명확하지만, 벡터의 차원이 어휘 사전의 크기에 비례한다는 단점이 있습니다. 실제 언어 모델에서 어휘 사전은 수만에서 수십만 개의 단어를 포함하므로, 원-핫 벡터는 매우 고차원이면서도 대부분의 값이 0인 희소(sparse) 벡터가 됩니다.

## 1.1.2 유사성 부재 (직교성)

원-핫 벡터 표현의 가장 큰 문제점은 단어 간의 의미적 유사성을 전혀 포착하지 못한다는 것입니다. 웹 검색을 예로 들어보겠습니다. 사용자가 "Seattle motel"을 검색할 때, "Seattle hotel"이 포함된 문서도 관련성 있는 결과로 나타나야 합니다. "motel"과 "hotel"은 의미상 매우 유사한 단어이기 때문입니다.

그러나 원-핫 벡터로 표현된 두 단어의 내적(dot product)을 계산하면 0이 나옵니다. 두 벡터가 1인 위치가 서로 다르기 때문에, 수학적으로 이들은 완전히 직교(orthogonal)합니다. 즉, 원-핫 벡터 공간에서 모든 단어는 서로 동일한 거리에 있으며, 어떤 단어 쌍도 다른 쌍보다 더 가깝거나 유사하지 않습니다.

이러한 직교성 문제는 단어의 의미를 전혀 반영하지 못합니다. "king"과 "queen"의 유사성이 "king"과 "apple"의 유사성과 동일하게 취급되는 것입니다. 결과적으로 원-핫 벡터는 언어의 풍부한 의미 구조를 표현하는 데 근본적인 한계를 가지며, 이는 자연어 이해와 처리에 있어 심각한 제약이 됩니다.

---
# 1.2 단어 벡터 및 임베딩

## 1.2.1 밀집 벡터 사용

원-핫 벡터의 문제를 해결하기 위해 밀집(dense) 벡터를 사용합니다. 밀집 벡터는 모든 요소가 실수 값을 가지는 저차원 벡터로, 일반적으로 50차원에서 300차원 정도의 크기를 가집니다. 이는 수만 차원의 희소한 원-핫 벡터와 대조적입니다.

단어 벡터는 유사한 문맥에서 나타나는 단어들이 비슷한 벡터 값을 가지도록 학습됩니다. 예를 들어, "banking"과 "monetary"는 비슷한 문맥에서 사용되므로, 이들의 벡터 표현도 서로 가까운 위치에 배치됩니다. 벡터 간의 유사성은 내적(dot product)으로 측정하며, 내적 값이 클수록 두 단어가 의미적으로 유사함을 나타냅니다.

구체적인 예시를 보면 다음과 같습니다:

- banking = [0.286, 0.792, -0.177, -0.107, 0.109, -0.542, 0.349, 0.271]
- monetary = [0.413, 0.582, -0.007, 0.247, 0.216, -0.718, 0.147, 0.051]

이러한 밀집 벡터 표현은 단어의 의미적 관계를 수치적으로 포착할 수 있으며, 벡터 공간에서 "banking" 근처에는 "monetary", "financial", "economic" 같은 관련 단어들이 위치하게 됩니다.

## 1.2.2 다른 명칭: 단어 임베딩, 신경망 단어 표현

이러한 밀집 벡터 표현은 여러 이름으로 불립니다. 가장 일반적인 용어는 **단어 임베딩(word embeddings)** 입니다. "임베딩"이라는 표현은 고차원의 이산적인 단어 공간을 저차원의 연속적인 벡터 공간에 "끼워 넣는다"는 의미를 담고 있습니다.

또한 **신경망 단어 표현(neural word representations)** 이라고도 불립니다. 이는 이러한 벡터들이 신경망을 통해 학습되기 때문입니다. 단순히 **단어 벡터(word vectors)** 라고 부르기도 합니다.

이 세 가지 용어는 본질적으로 같은 개념을 가리키며, 문헌에서 혼용되어 사용됩니다. 중요한 것은 명칭이 아니라, 이러한 표현 방식이 단어의 의미를 수치화하고 단어 간의 관계를 벡터 공간에서 기하학적으로 표현할 수 있다는 점입니다.

---
# 1.3 문맥에 의한 단어 표현 (분포 의미론)

## 1.3.1 기본 개념

단어 벡터를 학습하는 핵심 아이디어는 **분포 의미론(Distributional Semantics)** 에 기반합니다. 이는 "단어의 의미는 그 단어와 함께 자주 등장하는 주변 단어들에 의해 결정된다"는 개념입니다. 언어학자 J.R. Firth가 1957년에 제시한 유명한 문구가 이를 잘 표현합니다: "You shall know a word by the company it keeps" (단어를 알고 싶다면 그 단어가 어울리는 동료들을 보라).

이 아이디어는 현대 통계적 자연어 처리에서 가장 성공적인 개념 중 하나입니다. 예를 들어, "banking"이라는 단어가 텍스트에 나타날 때 주변에 어떤 단어들이 함께 등장하는지 살펴보겠습니다:

- "...government debt problems turning into **banking** crises as happened in 2009..."
- "...saying that Europe needs unified **banking** regulation to replace the hodgepodge..."
- "...India has just given its **banking** system a shot in the arm..."

이러한 여러 문맥들을 수집하면, "banking" 주변에는 "crisis", "regulation", "system", "financial" 같은 단어들이 자주 등장한다는 것을 알 수 있습니다. 이 주변 단어들(문맥)이 모여서 "banking"의 의미를 구성하게 됩니다.

## 1.3.2 문맥 정의

문맥(context)은 구체적으로 어떻게 정의될까요? 단어 w가 텍스트에 나타날 때, 그 **문맥은 고정된 크기의 윈도우(window) 내에서 w 주변에 등장하는 단어들의 집합**입니다.

예를 들어, 윈도우 크기가 2인 경우를 보겠습니다:

- "...problems turning **into** banking crises..."

여기서 중심 단어(center word)가 "into"일 때, 좌우로 2개씩 총 4개의 단어가 문맥을 구성합니다:

- 왼쪽 문맥: "problems", "turning"
- 오른쪽 문맥: "banking", "crises"

이러한 문맥 단어들은 **외부 단어(outside words)** 또는 **문맥 단어(context words)** 라고 불립니다. 텍스트 전체를 순회하면서 각 위치에서 중심 단어와 그 문맥 단어들의 관계를 학습함으로써, 비슷한 문맥에서 나타나는 단어들은 비슷한 벡터 표현을 갖게 됩니다. 이것이 바로 Word2vec과 같은 단어 임베딩 모델의 핵심 원리입니다.

---
# 1.4 Word2vec 프레임워크

## 1.4.1 개요

Word2vec은 Mikolov 등이 2013년에 제안한 단어 벡터 학습 프레임워크입니다. 이 방법은 대규모 텍스트 코퍼스(corpus)에서 단어의 분포 패턴을 학습하여 의미를 포착하는 밀집 벡터를 생성합니다.

Word2vec의 기본 구조는 다음과 같습니다. 먼저 고정된 어휘 사전의 모든 단어를 벡터로 표현합니다. 텍스트의 각 위치 t를 순회하면서, 해당 위치의 중심 단어 c와 그 주변의 문맥 단어 o를 확인합니다. 그리고 중심 단어 벡터와 문맥 단어 벡터의 유사도를 이용해 실제로 o가 c 주변에 등장할 확률을 계산합니다. 학습의 목표는 이 확률을 최대화하도록 모든 단어 벡터를 조정하는 것입니다.

## 1.4.2 학습 과정

Word2vec의 학습 과정을 구체적인 예시로 살펴보겠습니다. 다음과 같은 문장이 있고 윈도우 크기가 2라고 가정합니다:

"...problems turning **into** banking crises as..."

중심 단어가 "into"(위치 t)일 때:

- P(problems | into): "into"가 주어졌을 때 "problems"가 나타날 확률
- P(turning | into): "into"가 주어졌을 때 "turning"이 나타날 확률
- P(banking | into): "into"가 주어졌을 때 "banking"이 나타날 확률
- P(crises | into): "into"가 주어졌을 때 "crises"가 나타날 확률

이러한 확률들은 단어 벡터 간의 유사도를 통해 계산됩니다. 텍스트의 모든 위치를 순회하면서 실제로 함께 등장하는 단어 쌍들의 확률을 높이는 방향으로 벡터들을 반복적으로 업데이트합니다. 이 과정을 통해 비슷한 문맥에서 나타나는 단어들은 자연스럽게 비슷한 벡터 값을 갖게 됩니다.

## 1.4.3 모델 변형

Word2vec은 두 가지 주요 변형 모델을 제공합니다. 두 모델 모두 같은 목표를 가지지만, 예측 방향이 반대입니다.

### 1.4.3.1 Skip-grams

Skip-gram 모델은 **중심 단어가 주어졌을 때 주변 문맥 단어들을 예측**하는 방식입니다. 앞의 예시에서 설명한 것이 바로 Skip-gram 방식입니다. "into"라는 중심 단어로부터 "problems", "turning", "banking", "crises"를 예측하는 것입니다.

Skip-gram은 각 중심 단어에 대해 여러 개의 문맥 단어를 독립적으로 예측하므로, 상대적으로 더 많은 학습 샘플을 생성합니다. 이는 희소한 단어들에 대해서도 좋은 표현을 학습하는 데 유리합니다.

### 1.4.3.2 Continuous Bag of Words (CBOW)

CBOW 모델은 Skip-gram과 정반대로 작동합니다. **주변 문맥 단어들이 주어졌을 때 중심 단어를 예측**합니다. 즉, "problems", "turning", "banking", "crises"가 주어졌을 때 "into"를 맞추는 방식입니다.

CBOW는 여러 문맥 단어들의 벡터를 평균내어(bag of words) 중심 단어를 예측하기 때문에 이러한 이름이 붙었습니다. CBOW는 일반적으로 Skip-gram보다 학습 속도가 빠르며, 빈번하게 등장하는 단어들에 대해 좋은 성능을 보입니다.

두 모델 중 어느 것을 선택할지는 데이터의 특성과 목적에 따라 달라지지만, 실무에서는 Skip-gram이 더 널리 사용됩니다.

---
# 1.5 Word2vec 목적 함수 및 최적화

## 1.5.1 목적 함수

Word2vec의 목적 함수는 텍스트 전체에서 실제로 함께 등장하는 단어 쌍들의 확률을 최대화하는 것입니다. Skip-gram 모델의 경우, 각 위치 $t = 1, ..., T$에서 중심 단어 $w_t$가 주어졌을 때 윈도우 크기 $m$ 내의 문맥 단어 $w_{t+j}$가 나타날 확률을 최대화합니다.

수학적으로는 다음과 같이 표현됩니다:

**$$J(θ) = -1/T ∑_{t=1}^{T} ∑_{-m≤j≤m, j≠0} log P(w_{t+j} | w_t; θ)$$**

여기서 음의 로그 확률(negative log probability)을 사용하므로, 이를 **최소화**하는 것이 목표입니다. 확률을 최대화하는 것과 음의 로그 확률을 최소화하는 것은 동일한 의미입니다. 로그를 사용하는 이유는 계산의 편의성과 수치적 안정성 때문입니다.

## 1.5.2 매개변수 ($Θ$)

매개변수 $θ$는 모델이 학습해야 할 모든 변수를 포함합니다. Word2vec에서는 각 단어마다 두 개의 벡터를 유지합니다:

- **중심 단어 벡터 (v)**: 단어가 중심에 위치할 때 사용되는 벡터
- **문맥 단어 벡터 (u)**: 단어가 문맥에 위치할 때 사용되는 벡터

어휘 사전의 크기가 V이고 벡터의 차원이 d일 때, 전체 매개변수의 개수는 **2dV**입니다. 예를 들어, 어휘 사전에 10,000개의 단어가 있고 벡터 차원이 300이라면, 총 6,000,000개의 매개변수를 학습해야 합니다.

두 개의 벡터를 사용하는 이유는 최적화를 쉽게 하기 위함입니다. 학습이 끝난 후에는 일반적으로 **두 벡터를 평균내어 최종 단어 벡터로 사용**합니다.

## 1.5.3 최적화 알고리즘

목적 함수를 최소화하기 위해 **경사 하강법([[Gradient]] Descent)** 을 사용합니다. 경사 하강법의 기본 아이디어는 현재 매개변수 위치에서 목적 함수의 기울기(gradient)를 계산한 후, 기울기의 반대 방향으로 작은 스텝을 이동하는 것입니다. 이를 반복하면 결국 손실 함수의 최솟값에 도달하게 됩니다.

업데이트 공식은 다음과 같습니다:

**$$θ^{new} = θ^{old} - α∇_θ J(θ)$$**

여기서 α는 학습률(learning rate) 또는 스텝 크기(step size)로, 한 번에 얼마나 이동할지를 결정합니다.

하지만 일반적인 경사 하강법은 Word2vec에 적용하기 어렵습니다. 목적 함수 J(θ)가 전체 코퍼스의 모든 윈도우에 대한 합이기 때문에, 한 번의 업데이트를 위해 수십억 개의 단어를 모두 처리해야 합니다. 이는 실용적이지 않습니다.

따라서 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)** 을 사용합니다. SGD는 전체 데이터를 사용하지 않고, 한 번에 하나의 윈도우(또는 작은 배치)만 샘플링하여 경사를 계산하고 즉시 매개변수를 업데이트합니다. 이렇게 하면 각 업데이트가 매우 빠르게 이루어지고, 여러 번 반복하면서 전체적으로 좋은 해에 수렴하게 됩니다.

실제로는 **미니 배치 경사 하강법(Mini-Batch Gradient Descent)** 을 많이 사용하는데, 이는 한 번에 여러 개의 윈도우를 샘플링하여 경사를 계산하는 방식입니다.

## 1.5.4 손실 함수 [[Loss Function]]

### 1.5.4.1 개념

손실 함수(loss function)는 모델의 예측이 실제 데이터와 얼마나 다른지를 측정하는 함수입니다. Word2vec에서는 중심 단어가 주어졌을 때 문맥 단어가 나타날 확률을 계산해야 하는데, 이를 어떻게 정의하느냐에 따라 여러 가지 손실 함수를 사용할 수 있습니다.

### 1.5.4.2 Naive [[Softmax]] 

가장 기본적인 방법은 **소프트맥스 함수(softmax function)** 를 사용하는 것입니다. 중심 단어 c가 주어졌을 때 문맥 단어 o가 나타날 확률은 다음과 같이 정의됩니다:

**$$P(o|c) = exp(u_o^T v_c) / ∑_{w∈V} exp(u_w^T v_c)$$**

이 공식은 세 가지 핵심 요소로 구성됩니다:

1. **내적($u_o^T v_c$)**: 문맥 단어 벡터와 중심 단어 벡터의 유사도를 측정합니다. 내적이 클수록 두 단어가 유사함을 의미합니다.
    
2. **지수 함수(exponential)**: 모든 값을 양수로 만들고, 큰 값의 차이를 증폭시킵니다.
    
3. **정규화(normalization)**: 전체 어휘에 대해 합($∑_{w∈V} exp(u_w^T v_c)$)을 계산하여 나누어줌으로써, 결과가 확률 분포가 되도록 합니다(모든 확률의 합 = 1).
    

[[Softmax]]는 임의의 실수 값을 확률 분포로 변환하는 표준적인 방법으로, 딥러닝에서 매우 자주 사용됩니다. "max"라는 이름이 붙은 이유는 가장 큰 값의 확률을 증폭시키기 때문이고, "soft"라는 이름이 붙은 이유는 작은 값들에도 여전히 약간의 확률을 부여하기 때문입니다.

하지만 이 방법의 문제는 분모의 합 계산입니다. 어휘 사전의 모든 단어에 대해 지수 함수를 계산해야 하므로, 어휘 크기가 크면 계산 비용이 매우 높아집니다.

### 1.5.4.3 계층적 Softmax

계층적 소프트맥스(Hierarchical Softmax)는 naive softmax의 계산 비용을 줄이기 위한 방법입니다. 어휘를 이진 트리 구조로 조직하여, 각 단어를 루트에서 잎까지의 경로로 표현합니다. 이렇게 하면 확률 계산을 $O(V)$에서 $O(log V)$로 줄일 수 있습니다.

### 1.5.4.4 Negative Sampling

**네거티브 샘플링(Negative Sampling)** 은 Word2vec에서 실제로 가장 많이 사용되는 방법입니다. 이 방법의 핵심 아이디어는 전체 어휘에 대한 소프트맥스를 계산하는 대신, **이진 분류 문제**로 변환하는 것입니다.

구체적으로, 실제 문맥 단어 쌍(중심 단어와 그 주변에 실제로 나타난 단어)과 노이즈 쌍(중심 단어와 무작위로 선택된 단어)을 구별하도록 학습합니다. 각 실제 쌍에 대해 k개의 부정 샘플(negative samples)을 무작위로 선택합니다.

목적 함수는 다음과 같이 바뀝니다:

**$$J_{neg-sample}(u_o, v_c, U) = -log σ(u_o^T v_c) - ∑_{k∈K} log σ(-u_k^T v_c)$$**

여기서 σ는 시그모이드 함수(sigmoid function)이고, K는 k개의 부정 샘플 인덱스 집합입니다. 첫 번째 항은 실제 쌍의 확률을 최대화하고, 두 번째 항은 부정 샘플들의 확률을 최소화합니다.

부정 샘플은 단순 균등 분포가 아닌, 단어 빈도의 3/4승에 비례하는 분포 **$P(w) = U(w)^{3/4} / Z$** 에서 샘플링합니다. 이렇게 하면 희소한 단어들도 적절히 샘플링되어 더 나은 학습이 가능합니다.

네거티브 샘플링의 장점은 각 업데이트마다 전체 어휘가 아닌 k+1개(실제 단어 1개 + 부정 샘플 k개)의 단어 벡터만 업데이트하면 되므로, 계산이 매우 효율적이라는 것입니다. 일반적으로 k=5~20 정도면 충분히 좋은 결과를 얻을 수 있습니다.

---
# 1.6 네거티브 샘플링을 사용한 [[Skip-gram]]

## 1.6.1 필요성

Naive softmax의 가장 큰 문제는 정규화 항의 계산 비용입니다. 확률을 계산할 때마다 전체 어휘 V에 대한 합을 구해야 하므로, 어휘 크기가 수만에서 수십만에 이르면 계산이 매우 느려집니다.

**$$P(o|c) = exp(u_o^T v_c) / ∑_{w∈V} exp(u_w^T v_c)$$**

분모의 "큰 합(A big sum over words)"이 바로 병목 지점입니다. 각 윈도우마다 이 계산을 반복해야 하므로, 실제 대규모 코퍼스에서는 실용적이지 않습니다. 따라서 표준 Word2vec 구현에서는 네거티브 샘플링을 사용한 Skip-gram 모델을 채택합니다.

## 1.6.2 주요 아이디어

### 1.6.2.1 Binary Logistic Regression Training

네거티브 샘플링의 핵심은 다중 클래스 분류 문제를 **이진 분류 문제**로 변환하는 것입니다. 전체 어휘 중에서 정답 단어를 찾는 대신, 주어진 단어 쌍이 "실제로 함께 등장하는가?" 또는 "무작위로 선택된 것인가?"를 구별하는 이진 로지스틱 회귀(binary logistic regression)를 학습합니다.

이를 위해 **시그모이드 함수(sigmoid function)** 를 사용합니다:

**$$σ(x) = 1 / (1 + e^{-x})$$**

시그모이드 함수는 임의의 실수 값을 0과 1 사이의 값으로 매핑하여, 이진 확률로 해석할 수 있게 합니다. x가 클수록 1에 가까워지고, x가 작을수록 0에 가까워집니다.

### 1.6.2.2 True 쌍과 Noise 쌍 구별

학습 데이터는 두 가지 유형으로 구성됩니다:

**True 쌍 (긍정 샘플)**: 실제 텍스트에서 함께 등장한 중심 단어와 문맥 단어의 쌍입니다. 예를 들어, "...turning into **banking** crises..."에서 중심 단어가 "into"일 때, (into, banking)은 true 쌍입니다. 모델은 이러한 쌍에 대해 높은 확률을 부여하도록 학습합니다.

**Noise 쌍 (부정 샘플)**: 중심 단어는 동일하지만, 문맥 단어를 무작위로 선택하여 만든 쌍입니다. 예를 들어, (into, elephant), (into, democracy) 같은 쌍은 실제로 함께 등장할 가능성이 낮은 noise 쌍입니다. 모델은 이러한 쌍에 대해 낮은 확률을 부여하도록 학습합니다.

## 1.6.3 세부 사항

네거티브 샘플링의 목적 함수는 다음과 같이 정의됩니다:

$J_{neg-sample}(u_o, v_c, U) = -log σ(u_o^T v_c) - ∑_{k∈{K sampled indices}} log σ(-u_k^T v_c)$

이 식의 의미를 부분별로 살펴보겠습니다:

**첫 번째 항 $-log σ(u_o^T v_c)$**: 실제 문맥 단어 o에 대한 항입니다. $u_o^T v_c$가 클수록(두 벡터가 유사할수록) $σ(u_o^T v_c)$는 1에 가까워지고, -log 값은 0에 가까워집니다. 즉, 실제로 함께 등장하는 단어 쌍의 벡터들이 유사해지도록 만듭니다.

**두 번째 항 $-∑log σ(-u_k^T v_c$)**: k개의 부정 샘플에 대한 항입니다. 음수 부호에 주목해야 합니다. $u_k^T v_c$가 작을수록(무관한 단어일수록) $-u_k^T v_c$는 커지고, $σ(-u_k^T v_c)$는 1에 가까워져 $-log$ 값이 0에 가깝습니다. 즉, 무작위로 선택된 단어 쌍의 벡터들은 서로 다르도록 만듭니다.

각 true 쌍에 대해 k개의 부정 샘플을 선택하는데, 일반적으로 k=5에서 20 사이의 값을 사용합니다. 이렇게 하면 각 업데이트마다 k+1개의 단어 벡터만 계산하면 되므로, 전체 어휘 V에 대해 계산하는 것보다 훨씬 효율적입니다.

또한 SGD를 사용할 때, 각 윈도우에서는 최대 2m+1개의 중심 단어와 2km개의 부정 단어만 관여하므로, 경사 벡터 $∇_θ J_t(θ)$가 매우 희소(sparse)합니다. 대부분의 단어 벡터는 0인 경사를 가지므로, 실제로 나타난 단어들의 벡터만 업데이트하면 됩니다. 이를 효율적으로 구현하려면 희소 행렬 업데이트 연산이나 해시 테이블을 사용해야 합니다.

## 1.6.4 샘플링 분포

부정 샘플을 선택할 때 어떤 분포를 사용할지도 중요합니다. 단순히 균등 분포를 사용하면, 빈번한 단어(the, a, is 등)가 너무 자주 선택되고 희소한 단어는 거의 선택되지 않습니다.

Word2vec에서는 **유니그램 분포(unigram distribution)를 3/4승한 분포**를 사용합니다:

**$$P(w) = U(w)^{3/4} / Z$$**

여기서 $U(w)$는 단어 $w$의 출현 빈도이고, $Z$는 정규화 상수입니다. 3/4승을 취하면 빈도가 낮은 단어의 샘플링 확률이 상대적으로 증가합니다.

예를 들어, "the"가 "democracy"보다 100배 더 자주 등장한다면:

- 원래 비율: 100 : 1
- 3/4승 후: $100^{0.75} : 1^{0.75} ≈ 31.6 : 1$

이렇게 하면 희소한 단어들도 부정 샘플로 적절히 선택되어, 이들에 대한 단어 벡터도 잘 학습될 수 있습니다. 3/4이라는 구체적인 값은 실험적으로 결정된 것으로, 다양한 코퍼스에서 좋은 성능을 보이는 것으로 알려져 있습니다.