# Negative Sampling

## Contents

- **1. Negative Sampling의 개념**
- **2. Naive Softmax의 문제점**
- **3. Negative Sampling의 작동 원리**
- **4. 수학적 정의**
- **5. 샘플링 전략**

---

## 1. Negative Sampling의 개념

Negative Sampling은 Word2vec 모델을 효율적으로 학습시키기 위해 고안된 기법입니다. 기본적으로 다중 클래스 분류 문제를 이진 분류 문제로 변환하는 것이 핵심 아이디어입니다.

전통적인 방법에서는 중심 단어가 주어졌을 때 전체 어휘 중에서 정답 문맥 단어를 찾아야 합니다. 어휘 크기가 수만 개라면, 수만 개의 선택지 중에서 하나를 고르는 문제가 됩니다. 이는 계산 비용이 매우 높습니다.

Negative Sampling은 이 문제를 단순화합니다. "이 단어 쌍이 실제로 함께 등장하는가?"라는 예/아니오 질문으로 바꾸는 것입니다. 각 실제 단어 쌍(positive pair)에 대해 몇 개의 가짜 단어 쌍(negative pair)을 만들고, 모델이 이 둘을 구별하도록 학습합니다.

예를 들어, "banking"이라는 중심 단어가 있고 실제로 주변에 "crisis"가 나타났다면, (banking, crisis)는 positive pair입니다. 동시에 무작위로 선택한 단어들, 예를 들어 "elephant", "democracy", "pizza"와 (banking, elephant), (banking, democracy), (banking, pizza) 같은 negative pair를 만듭니다. 모델은 첫 번째 쌍은 진짜라고, 나머지는 가짜라고 판단하도록 학습됩니다.

이 방식의 장점은 각 업데이트마다 전체 어휘를 고려할 필요 없이, 단지 1개의 positive 샘플과 k개(보통 5~20개)의 negative 샘플만 처리하면 된다는 것입니다. 이는 계산 복잡도를 O(V)에서 O(k)로 대폭 감소시킵니다.

---

## 2. Naive Softmax의 문제점

Negative Sampling의 필요성을 이해하려면, 먼저 기본적인 Softmax 접근법의 문제를 살펴봐야 합니다.

Word2vec의 Skip-gram 모델에서 중심 단어 c가 주어졌을 때 문맥 단어 o가 나타날 확률은 다음과 같이 정의됩니다:

$$P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w\in V} \exp(u_w^T v_c)}$$

이 식은 세 가지 요소로 구성됩니다. 분자의 $\exp(u_o^T v_c)$는 문맥 단어 벡터와 중심 단어 벡터의 유사도를 지수 함수로 변환한 값입니다. 내적이 클수록, 즉 두 벡터가 유사할수록 이 값도 커집니다.

문제는 분모입니다. $\sum_{w\in V} \exp(u_w^T v_c)$는 전체 어휘 V의 모든 단어에 대해 지수 함수를 계산하고 합산해야 합니다. 이것이 바로 "A big sum over words"라고 불리는 병목 지점입니다.

실제 상황을 가정해봅시다. 어휘 크기가 50,000개이고, 코퍼스에 10억 개의 단어가 있다면, 각 윈도우마다 이 50,000개 단어에 대한 계산을 수행해야 합니다. 윈도우 크기가 2라면, 각 위치마다 최대 4개의 문맥 단어를 예측하므로, 총 40억 번의 윈도우 계산이 필요합니다. 각 계산마다 50,000번의 지수 함수 평가와 합산을 수행하면, 전체적으로 약 200조 번의 연산이 필요합니다.

더 큰 문제는 경사 계산입니다. 역전파를 통해 매개변수를 업데이트하려면, 이 Softmax의 경사를 계산해야 합니다. Softmax의 특성상, 한 단어의 확률을 계산할 때 전체 어휘의 모든 단어가 영향을 미칩니다. 따라서 매개변수 업데이트 시에도 전체 어휘를 고려해야 하며, 이는 대규모 코퍼스에서 실용적이지 않습니다.

---

## 3. Negative Sampling의 작동 원리

Negative Sampling은 이 문제를 이진 분류로 변환하여 해결합니다. 핵심 아이디어는 다음과 같습니다.

**True pair (긍정 샘플)**: 실제 텍스트에서 함께 등장한 중심 단어와 문맥 단어의 쌍입니다. 예를 들어, "...turning into banking crises..."에서 중심 단어가 "into"일 때, (into, banking)은 true pair입니다. 모델은 이러한 쌍에 대해 높은 확률(1에 가까운 값)을 부여하도록 학습합니다.

**Noise pair (부정 샘플)**: 중심 단어는 동일하지만, 문맥 단어를 무작위로 선택하여 만든 쌍입니다. 예를 들어, (into, elephant), (into, democracy), (into, pizza) 같은 쌍은 실제로 함께 등장할 가능성이 낮은 noise pair입니다. 모델은 이러한 쌍에 대해 낮은 확률(0에 가까운 값)을 부여하도록 학습합니다.

이진 분류 문제로 변환하면 시그모이드 함수를 사용할 수 있습니다. 시그모이드 함수는 다음과 같이 정의됩니다:

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

시그모이드는 임의의 실수를 0과 1 사이의 값으로 매핑합니다. x가 큰 양수이면 1에 가까워지고, x가 큰 음수이면 0에 가까워집니다. x가 0이면 정확히 0.5가 됩니다.

True pair에 대해서는 $\sigma(u_o^T v_c)$를 최대화합니다. 내적 $u_o^T v_c$가 클수록 시그모이드 출력이 1에 가까워지므로, 실제로 함께 등장하는 단어들의 벡터가 유사해지도록 유도합니다.

Noise pair에 대해서는 $\sigma(-u_k^T v_c)$를 최대화합니다. 음수 부호에 주목해야 합니다. 내적 $u_k^T v_c$가 작을수록(음수일수록) $-u_k^T v_c$는 커지고, 시그모이드 출력이 1에 가까워집니다. 이는 무관한 단어들의 벡터가 서로 다르도록 유도합니다.

각 true pair에 대해 k개의 negative 샘플을 생성합니다. k의 값은 데이터셋 크기에 따라 다르지만, 일반적으로 5에서 20 사이가 적절합니다. 작은 데이터셋에서는 k를 크게(15~20), 큰 데이터셋에서는 k를 작게(5~10) 설정하는 것이 일반적입니다.

---

## 4. 수학적 정의

Negative Sampling의 손실 함수는 다음과 같이 정의됩니다:

$$J_{neg-sample}(\boldsymbol{u}_o, \boldsymbol{v}_c, U) = -\log\sigma(\boldsymbol{u}_o^T\boldsymbol{v}_c) - \sum_{k\in{K \text{ sampled indices}}} \log\sigma(-\boldsymbol{u}_k^T\boldsymbol{v}_c)$$

**중요**: 이것은 손실 함수이므로 **최소화해야 하는 대상**입니다. 손실이 작아질수록 모델이 잘 학습된 것입니다.

위 식의 최솟값은 0입니다. 즉, 위 손실함수를 0에 가깝게 해야합니다. 또한 $\sigma(\boldsymbol{u}_o^T\boldsymbol{v}_c)$ <= 1 이기 때문에 로그값은 항상 음수이므로 "-"를 붙여줘야 합니다.)

즉, 첫 번째 항 $-\log\sigma(\boldsymbol{u}_o^T\boldsymbol{v}_c)$를 0에 가깝게 해야 하기에, $\log$ 안의 내적 값은 커야합니다. 이는 두 단어가 **실제로 함께 등장한다**는 것을 의미합니다. 

반대로 두 번째 항 $-\log\sigma(-\boldsymbol{u}_k^T\boldsymbol{v}_c)$에서는 시그모이드 안에 음수 부호가 있으므로, 원래 내적 $\boldsymbol{u}_k^T\boldsymbol{v}_c$가 작아야(음수이거나 0에 가까워야) 손실이 0에 가까워집니다. 이는 무작위로 선택된 단어들이 **실제로는 함께 등장하지 않는다**는 것을 의미합니다. 

결과적으로 이 손실 함수를 최소화하면, 문맥상 함께 나타나는 단어 쌍의 벡터는 가까워지고(내적이 큼), 무작위로 선택된 단어 쌍의 벡터는 멀어지게(내적이 작음) 됩니다.

### 첫 번째 항: Positive Loss

$$-\log\sigma(\boldsymbol{u}_o^T\boldsymbol{v}_c)$$

이 항은 실제 문맥 단어 o에 대한 손실입니다. $\boldsymbol{u}_o$는 문맥 단어 o의 벡터이고, $\boldsymbol{v}_c$는 중심 단어 c의 벡터입니다.

**학습 목표**: 이 항을 **작게** 만들기

- $\boldsymbol{u}_o^T\boldsymbol{v}_c$가 크다 (예: +10) ← 두 벡터가 유사
- → $\sigma(10) \approx 1$
- → $\log(1) = 0$
- → $-\log(1) = 0$ ✓ **손실이 작음 = 좋음**

따라서 true pair의 벡터들은 **내적이 커지도록** (서로 가까워지도록) 학습됩니다.

### 두 번째 항: Negative Loss

$$-\sum_{k\in{K \text{ sampled indices}}} \log\sigma(-\boldsymbol{u}_k^T\boldsymbol{v}_c)$$

이 항은 k개의 부정 샘플에 대한 손실입니다. **시그모이드 안의 음수 부호**가 핵심입니다.

**학습 목표**: 이 항을 **작게** 만들기

- $\boldsymbol{u}_k^T\boldsymbol{v}_c$가 작다 (예: -10) ← 두 벡터가 다름
- → $-\boldsymbol{u}_k^T\boldsymbol{v}_c = 10$
- → $\sigma(10) \approx 1$
- → $\log(1) = 0$
- → $-\log(1) = 0$ ✓ **손실이 작음 = 좋음**

따라서 noise pair의 벡터들은 **내적이 작아지도록** (서로 멀어지도록) 학습됩니다.

### 학습의 메커니즘

두 항 모두 앞에 마이너스(-)가 붙어있으므로, 각 항 내부의 $\log$ 값이 0에 가까워지면 전체 손실도 0에 가까워집니다.

|단어 쌍 유형|내적이 어떻게?|손실이 어떻게?|의미|
|---|---|---|---|
|True pair|크게 (양수)|작아짐|실제 함께 등장하는 단어는 가까워야 함|
|Noise pair|작게 (음수)|작아짐|무작위 단어는 멀어야 함|

### 전체 목적 함수

전체 코퍼스에 대한 목적 함수는 모든 위치의 손실을 평균낸 것입니다:

$$J(\theta) = \frac{1}{T}\sum_{t=1}^{T} \sum_{\substack{-m\leq j\leq m \ j\neq 0}} J_{neg-sample}(\boldsymbol{u}_{t+j}, \boldsymbol{v}_t, U)$$

여기서 T는 코퍼스의 전체 길이, m은 윈도우 크기입니다. 경사 하강법을 통해 이 손실 함수를 최소화하면, true pair의 벡터는 가까워지고 noise pair의 벡터는 멀어집니다.

---

## 5. 샘플링 전략

Negative 샘플을 어떻게 선택하는가도 매우 중요합니다. 가장 단순한 방법은 균등 분포(uniform distribution)를 사용하는 것이지만, 이는 좋은 선택이 아닙니다.

언어에는 빈도의 편차가 매우 큽니다. "the", "a", "is" 같은 단어는 매우 자주 등장하고, "democracy", "philosopher", "quantum" 같은 단어는 상대적으로 드뭅니다. 균등 분포로 샘플링하면, 모든 단어가 동일한 확률로 선택되므로, 희소한 단어들은 거의 negative 샘플로 선택되지 않습니다.

이 문제를 해결하기 위해 Word2vec에서는 **유니그램 분포의 3/4 제곱**을 사용합니다:

$$P(w) = \frac{U(w)^{3/4}}{Z}$$

여기서 $U(w)$는 단어 w의 출현 빈도(unigram distribution), Z는 정규화 상수입니다.

### 3/4 제곱의 효과

구체적인 예시로 살펴봅시다. "the"가 100,000번, "democracy"가 1,000번 등장했다고 가정합니다.

**원래 비율 (3/4 제곱 전)**: $$\frac{100,000}{1,000} = 100 : 1$$

**3/4 제곱 후**:

- $100,000^{0.75} \approx 31,622.78$
- $1,000^{0.75} \approx 177.83$
- 비율: $\frac{31,622.78}{177.83} \approx 177.83 : 1$

3/4 제곱을 취하면 빈도 차이가 완화됩니다. 원래 100배 차이였던 것이 약 178배 차이로 변합니다. 이는 완전히 평준화하지는 않지만, 희소한 단어들이 더 자주 선택될 수 있도록 만듭니다.

### 이점

1. **희소한 단어의 학습 기회 증가**: 드물게 등장하는 단어들도 negative 샘플로 적절히 선택되어, 이들에 대한 좋은 벡터 표현을 학습할 수 있습니다.
    
2. **빈번한 단어의 과도한 선택 방지**: "the", "a" 같은 단어들이 지나치게 자주 선택되는 것을 방지합니다. 이러한 단어들은 대부분의 문맥에서 나타나므로, 좋은 negative 샘플이 아닙니다.
    

3/4라는 구체적인 값은 Mikolov 등의 실험에서 경험적으로 결정되었습니다. 다양한 코퍼스에서 가장 좋은 성능을 보인 값이 0.75였습니다.

### 구현

```python
freqs = np.zeros(vocab_size)
for doc in corpus:
    for word_idx in doc:
        freqs[word_idx] += 1

probs = freqs ** 0.75
probs /= probs.sum()
```

이렇게 계산된 `probs`를 `np.random.choice`의 `p` 매개변수로 전달하면, 유니그램 분포의 3/4 제곱에 따라 샘플링이 이루어집니다. 이 분포를 사용함으로써 Negative Sampling은 효율적이면서도 효과적인 학습을 가능하게 합니다.