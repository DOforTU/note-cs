# 2. Data Analysis Tools, Preprocessing, and Visualization

## Contents

- **2.1 Data Processing Libraries**
- **2.2 Data Visualization**
- **2.3 Descriptive Statistics and Data Characteristics**
- **2.4 Data Preprocessing**

---

## 2.1 Data Processing Libraries

Python stands out in data analysis and visualization because of its powerful library ecosystem. These libraries let you efficiently process data, run complex statistical analyses, and create visualizations all in one place.

### 2.1.1. NumPy: The Foundation of Arrays and Vector Operations

NumPy is Python's core package for **working with vectors and matrices**. It handles the basic data structures for all kinds of scientific computing and serves as the foundation for every other data processing library.

NumPy lets you declare basic arrays, create different types of matrices, and perform matrix operations like dot products, row or column sums, and transposes. You can also access array elements in various ways - through integer indexing, slice indexing, or boolean array indexing.

NumPy's biggest strength is **speed and efficiency**. **Vectorization** means you can save time and reduce code by using **vector operations instead of loops** whenever possible. **Broadcasting** is a feature that efficiently handles **operations between arrays of different shapes** when certain conditions are met.

### 2.1.2. Pandas: The Wizard of Tabular Data

Pandas builds on NumPy arrays but specializes in **handling and analyzing tabular data**. It's widely used not just for statistical analysis and visualization, but especially as a **preprocessing tool for machine learning**.

Pandas' core components are **Series** (a single column in a table) and **DataFrame** (a collection of Series, basically a table with rows and columns). Pandas makes basic data handling easy with features like creating DataFrames from Dictionaries, Lists, or NumPy arrays, reading and saving files, selecting and filtering columns, and indexing rows using `loc`/`iloc`.

Data preprocessing can be the most time-consuming part of solving real problems, and Pandas handles this efficiently. It can delete rows (`axis=0`) or columns (`axis=1`) containing missing values, or replace numeric values with their average and categorical values with their most frequent value. Pandas also offers features like removing duplicate data, transforming data using functions or mappings, **discretization and binning** (dividing continuous data into intervals for analysis), and detecting and removing outliers.

### 2.1.3. SciPy: Advanced Algorithms and Statistical Analysis

SciPy is a **library of algorithms and mathematical tools** built to work with NumPy arrays. While NumPy provides efficient array operations, SciPy uses that data to perform complex, specialized scientific and statistical calculations.

The **statistics module (`scipy.stats`)** calculates the main statistics covered in introductory statistics and performs hypothesis testing. It computes mean, median, mode, variance, and kurtosis, calculates Pearson correlation coefficients, and provides various **hypothesis tests** like t-tests, Wilcoxon signed-rank tests, and Kolmogorov-Smirnov tests.

SciPy is useful beyond statistics too. **Optimization (`scipy.optimize`)** solves optimization problems like general minimization, constrained minimization (non-negative least-squares), and root finding. **Linear algebra (`scipy.linalg`)** is similar to NumPy's linear algebra module but can be faster because it always uses BLAS/LAPACK support, and it provides some additional functions.

---

## 2.2 Data Visualization

In statistics, data visualization is essential for clearly communicating analysis results and spotting hidden patterns and problems in the data itself. Visualization handles the final stage of statistical analysis - **'presentation'** - and it's important to use the right tools to create appropriate plots.

### 2.2.1. Data Visualization Tools

The most widely used visualization tools in Python-based data analysis are Matplotlib and Seaborn, which builds on Matplotlib.

**Matplotlib** is a comprehensive library for creating **static, animated, and interactive visualizations** in Python. It can produce publication-quality graphs and supports creating interactive figures that you can zoom, pan, and update. It takes **NumPy arrays** as input by default, but you can also plot Pandas data or Dictionaries using the `data` keyword. For complex figures, the recommended approach is **object-oriented (OO style)** - explicitly creating Figure and Axes objects and calling methods on them.

**Seaborn** builds on Matplotlib and provides a high-level interface for creating **attractive and informative statistical graphics**. It offers statistical estimation features like estimating **confidence intervals through bootstrapping** and drawing **error bars**, and provides a convenient interface for fitting regression models across conditional subsets of datasets.

### 2.2.2. Choosing Visualizations Based on Data Characteristics and Purpose

The most important principle of visualization is **choosing plots that match your data characteristics and purpose**.

|Visualization Type|Main Purpose|Data Type|Key Points/Principles|
|:--|:--|:--|:--|
|**Scatter Plot**|Observe **relationships** between two quantitative variables and identify clusters and outliers|Two quantitative variables|Remember that correlation doesn't necessarily mean causation|
|**Bar Plot**|Show **counts** of discrete or qualitative data or **compare numbers** between groups|Categorical feature + numeric value|Generally use **zero as the baseline**, and if categories have no natural order, sort by value size|
|**Histogram**|Visualize the **distribution** of a numeric variable|Numeric variable|Each bar represents a **bin** (range of values), and you need to choose an appropriate number of bins|
|**Line Chart**|Emphasize **trends** or **slopes** in Y-axis values as X-axis changes continuously (like time)|X-axis (continuous value), Y-axis (changing value)|Too many lines on one plot makes it hard to read - **5 lines or fewer** is recommended|
|**Pie Chart**|Compare the **proportion** each category takes up in the whole|Qualitative variable|Must include **annotations** to know exact percentages, and if you have **more than 5 slices, use a different chart** or group some as 'other'|
|**Heatmap**|Show values of a main variable across two axes using **color** to observe **relationship patterns**|Two axes (categorical or numeric binning) + cell value (numeric)|Include a **legend** to explain how colors map to values|
|**Box Plot**|Summarize data **distribution** and identify **outliers**|Numeric variable|Marks values outside Q1 – 1.5×IQR ~ Q3 + 1.5×IQR range as outliers|

### 2.2.3. Historical Case: The Power of Visualization

Data visualization goes beyond just making analysis results look pretty - it has **the power to drive decision-making**. During the Crimean War (1853-1856), Florence Nightingale analyzed data and used rose diagrams to visually communicate that **more soldiers died from hospital conditions (disease) than from battle wounds**. This work emphasized the importance of hygiene and helped reduce hospital death rates dramatically from 42% to 2%.

---

## 2.3 Descriptive Statistics and Data Characteristics

Understanding data characteristics is the most basic step in statistical analysis. **Descriptive statistics** is a branch of statistics that organizes and summarizes collected data to **quantitatively describe and interpret** its characteristics.

### 2.3.1. Parameters vs Statistics

The values that descriptive statistics summarize have different names depending on whether they come from a sample or the entire population.

**Parameters** are numbers that describe the **entire population** - they represent characteristics of the whole group we ultimately want to know about (like population mean or population standard deviation). **Statistics** are numbers that describe a **sample** - sample statistics should preserve enough of the population's essential statistical features (like sample mean or sample standard deviation).

In statistical inference, we **estimate population parameters from sample statistics**. For accurate estimation, samples need to be selected **randomly** to represent the population well, which enables unbiased estimation.

### 2.3.2. Measures of Central Tendency

The first way to summarize data is measuring where the data clusters around the center. This is called **central tendency** or **estimates of location**.

|Measure|Description|Features and Uses|
|:--|:--|:--|
|**Mean**|Add all values and divide by the count|Most commonly used but **sensitive** to extreme values (outliers)|
|**Median**|The middle value when data is sorted by size (50th percentile)|More **robust** to outliers than the mean - less sensitive to data values|
|**Mode**|The most frequently occurring value||
|**Trimmed Mean**|Mean calculated after excluding a certain percentage from both ends to remove the effect of extremely large or small values|A robust estimate like median, resistant to outliers|
|**Weighted Mean**|Used when variables have different importance (weights) - like averaging sensor values with different accuracy||

**Skewness** complements central tendency by showing the **asymmetry** of data distribution. Values between -0.5 and 0.5 indicate fairly symmetric distribution, while values less than -1 or greater than 1 indicate highly asymmetric distribution.

### 2.3.3. Measures of Variability

While central tendency shows the data's 'location', **variability** or **dispersion** is the second dimension that measures how spread out or tightly clustered the data values are.

**Variance** measures how far data points are from the mean, showing how spread out the data is. **Standard deviation** is the square root of variance, showing how spread out the data is around the mean in the original units. **Median absolute deviation** is calculated relative to the median and is a robust measure of variability that's resistant to outliers. **Kurtosis** shows how **peaked** the data distribution is compared to a normal distribution.

### 2.3.4. Percentile-Based Measures

Statistics based on sorted data are called **order statistics**. Percentiles address the weakness of range, which is sensitive to extreme values.

The **Pth percentile** is a value where at least P% of all values are at or below it, and at least (100-P)% are at or above it. The median is the same as the 50th percentile.

**Quartiles** refer to values at the 25%, 50%, 75%, and 100% positions in sorted data. Q1 (first quartile) is the 25th percentile, and Q3 (third quartile) is the 75th percentile. The **interquartile range (IQR)** is one way to measure data dispersion, calculated as **IQR = Q3 - Q1**.

**Box plots** visually represent these percentile-based measures. They show data distribution for one or more groups and are useful for comparing groups/categories, examining skewness and variance, and **identifying outliers**. Box plots mark values outside the **Q1 – 1.5×IQR ~ Q3 + 1.5×IQR range** as outliers.

### 2.3.5. Simpson's Paradox

**Simpson's Paradox** is a phenomenon in probability and statistics where a trend appears in several separate data groups but **disappears or reverses when those groups are combined**. It's often found in social science and medical statistics, showing that simple data analysis can lead to wrong conclusions. This emphasizes that a deep understanding of the **background knowledge** behind how data was created is essential.

---

## 2.4 Data Preprocessing

In statistics, **data preprocessing** is the most important and time-consuming step for ensuring analysis accuracy and reliability. Since you need to work with unrefined data when solving real problems, preprocessing is essential for getting correct results.

### 2.4.1. Why Preprocessing Matters

Data preprocessing is a data preparation technique that **restructures or organizes raw data** so data mining can efficiently and easily retrieve strategic information.

Real-world data for problem-solving often has missing values, duplicates, inconsistent units, or noise. Programming ability to **implement and process things quickly** is as important as logic in data analysis, and preprocessing is often **one of the most time-consuming parts** of the entire analysis process.

### 2.4.2. Handling Missing Data

Missing data refers to cases where measurement data values are empty for various reasons (refusal to respond, inability to measure, equipment failure, technical limitations, etc.). If you don't handle missing data, you'll get errors when calculating means and variances or building models later.

Missing data can be classified into three types based on how it occurs. **Missing Completely At Random (MCAR)** is when the missing pattern is independent of both the missing values themselves and other measured variable values. **Missing At Random (MAR)** is when, given the observed variables, the missing pattern is independent of the missing values themselves. **Missing Not At Random (MNAR)** is when the missing pattern is related to the missing values themselves even after adjusting for measured variables.

There are mainly deletion and imputation methods for handling missing data:

|Method|Details|Pros and Cons|
|:--|:--|:--|
|**Deletion**|Remove rows (samples) or columns (attributes) with missing values|Easiest but reduces dataset size and can bias the input feature space|
|**Single Imputation**|Replace each missing value with a single number|Numeric: replace with column mean or median<br>Categorical: replace with column mode|
|**Multiple Imputation**|Assume distributions for unobserved variables and repeatedly sample to create multiple imputed datasets|Better at explaining uncertainty in estimated parameters or predictions than single imputation|
|**Inference-based Imputation**|Use regression or classification models to estimate missing values from other predictor variables||

The Pandas library is essential for handling missing values. You can remove rows containing missing values with `axis=0` and columns with `axis=1`. The `thresh` parameter lets you keep only rows or columns with at least a certain number of non-null values. You can also replace numeric column missing values with the mean and categorical column missing values with the most frequent value.

### 2.4.3. Data Transformation

Data transformation is a technique for restructuring raw data for efficient information retrieval.

**Removing duplicates** identifies and removes duplicate data. **Transformation using functions or mapping** transforms data according to specific rules or replaces values. **Renaming axis indexes** changes the names of row/column indexes.

**Discretization and binning** divides continuous data into discrete intervals or 'bins' for analysis (like grouping people's ages into discrete age ranges). **Computing indicator/dummy variables** converts categorical variables into dummy variables consisting of 0s and 1s for use in modeling.

### 2.4.4. Data Aggregation and Normalization

**Data aggregation** is a method of storing and presenting data from multiple data sources in a summarized format. This step is important because the accuracy of data analysis insights greatly depends on the quantity and quality of data used.

**Data normalization** means **scaling data values to a much smaller range** like $[-1, 1]$ or $[0.0, 1.0]$. Main methods include Min-Max normalization and Z-score normalization.

### 2.4.5. Detecting and Handling Outliers

**Outliers** are **observations that are far away from other observations** in a given dataset. In other words, they're values that are much larger or smaller than the rest. They can occur due to data variability or experimental/human error and can seriously distort statistical analysis results.

There are several methods for detecting outliers:

1. **Visual inspection**: Looking at descriptive statistics or using graphic diagnostic tools like **box plots**.
    
2. **Z-score**: Calculate the Z-score using the formula $(X_i - \text{mean}) / \text{standard deviation}$, set a threshold (like 3), and mark data points where the absolute value exceeds the threshold as outliers.
    
3. **Interquartile Range (IQR)**: Sort data in ascending order and calculate Q1 (first quartile) and Q3 (third quartile). Calculate $\text{IQR} = Q3 - Q1$, then mark values **less than the lower bound ($Q1 - 1.5 \times \text{IQR}$) or greater than the upper bound ($Q3 + 1.5 \times \text{IQR}$)** as outliers.
    

Once outliers are detected, you can handle them in the following ways:

- **Trimming/removing**: Completely remove outliers from the dataset.
- **Flooring and capping**: Limit outliers to specific values at or below the 90th percentile or at or above the 10th percentile (for example, replace outliers that are too large with the 90th percentile value).