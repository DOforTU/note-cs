# 2. 통계 및 데이터 분석 개론

## Contents

- **2.1 데이터 처리 라이브러리**
- **2.2 데이터 시각화**
- **2.3 기술 통계 및 데이터 특성**
- **2.4 데이터 전처리 (Data Preprocessing)**

---
## 2.1 데이터 처리 라이브러리

데이터 분석 및 시각화 분야에서 Python은 독보적인 위치를 차지하고 있습니다. 이는 데이터를 효율적으로 처리하고, 복잡한 통계 분석을 수행하며, 최종적으로 시각화까지 가능하게 해주는 강력한 라이브러리 생태계 덕분입니다.

통계학 개론의 맥락에서 데이터를 다루는 데 필수적인 세 가지 핵심 라이브러리, 즉 NumPy, Pandas, 그리고 SciPy의 역할과 사용법에 대해 논의해 보겠습니다.

### 2.1.1. NumPy (Numerical Python): 배열과 벡터 연산의 기초

NumPy는 **벡터(Vector) 및 행렬(Matrix) 조작**을 위한 Python의 핵심 패키지입니다. 모든 종류의 과학 계산에서 기초 자료 구조를 담당하며, 다른 모든 데이터 처리 라이브러리의 기반이 됩니다.

#### 2.1.1.1. 주요 기능: 다차원 배열 처리

NumPy는 기본 배열을 선언하고, 다양한 형태의 행렬을 생성하며, 행렬의 기본 연산, 내적(Dot product), 행 또는 열의 합, 전치(Transpose) 등을 지원합니다. 또한, 정수 인덱싱, 슬라이스 인덱싱, 특정 조건을 만족하는 요소만 가져오는 불리언 배열 인덱싱 등 다양한 방식으로 배열 요소에 접근할 수 있습니다.

#### 2.1.1.2 효율성 극대화: 벡터화와 브로드캐스팅

NumPy의 가장 큰 강점은 **속도와 효율성**입니다.

- **벡터화(Vectorize):** 같은 작업을 수행할 때 되도록이면 **반복문(loop) 없이 벡터 단위 연산을 사용**하여 시간을 절약하고 코드양을 줄일 수 있습니다.
- **브로드캐스팅(Broadcasting):** NumPy에서 **모양(shape)이 다른 배열 간의 연산을 수행**할 때 특정 조건을 만족하는 경우 빠르고 효율적으로 작동하는 기능입니다. 이 기능은 내부적으로 C 언어로 작동하기 때문에 빠른 속도를 자랑합니다.

### 2.1.2. Pandas (Python Data Analysis Library): 테이블 데이터의 마법사

Pandas는 NumPy 배열을 기반으로 하지만, **테이블 형태의 데이터를 처리하고 분석**하는 데 특화된 라이브러리입니다. 통계 분석, 시각화뿐만 아니라, 특히 **기계 학습 등을 위한 전처리(Preprocessing) 도구**로 광범위하게 사용됩니다.

#### 2.1.2.1. 데이터의 기본 구성 요소: Series와 DataFrame

Pandas의 핵심 구성 요소는 우리가 흔히 보는 엑셀 시트와 같은 테이블 구조를 반영합니다.

- **Series:** 테이블의 **하나의 열(Column)**을 나타냅니다.
- **DataFrame:** **Series들의 집합**, 즉 행과 열로 이루어진 최종적인 테이블 형태의 데이터 구조를 나타냅니다.

Pandas는 Dictionary, List, NumPy 배열로부터 DataFrame을 생성하는 기능은 물론, 파일을 읽고 저장하는 기능, 컬럼 선택 및 필터링, `loc`/`iloc`을 사용한 행 인덱싱 등을 통해 기본적인 데이터 핸들링을 쉽게 만듭니다.

#### 2.1.2.2. 전처리 도구로서의 Pandas의 활용

데이터 전처리는 실제 문제를 해결할 때 가장 시간이 오래 걸리는 부분일 수 있는데, Pandas는 이 과정을 효율적으로 처리합니다.

- **결측값(Missing Data) 처리:** Pandas는 결측값이 포함된 행(`axis=0`) 또는 열(`axis=1`)을 삭제하거나, 숫자형 값은 평균(Average value)으로, 범주형 값은 최빈값(Most frequent value)으로 대체(Replace)하는 기능을 제공하여 결측값으로 인한 오류를 방지합니다.
- **데이터 변환 및 정리:** 중복 데이터 제거, 값을 치환하는 함수나 매핑을 이용한 데이터 변환, 연속적인 데이터를 분석을 위해 구간으로 나누는 **이산화(Discretization) 및 Binning**, 그리고 이상치(Outlier)를 감지하고 제거하는 기능 등 다양한 데이터 전처리 및 정리 기능을 제공합니다.

### 2.1.3. SciPy (Scientific Python): 고급 알고리즘과 통계 분석

SciPy는 NumPy 배열과 함께 작동하도록 구축된 **알고리즘 및 수학적 도구들의 라이브러리**입니다. NumPy가 효율적인 배열 연산을 제공한다면, SciPy는 그 데이터를 활용하여 복잡하고 전문적인 과학 및 통계 계산을 수행합니다.

#### 2.1.3.1. 통계 분석 모듈 (`scipy.stats`)

통계학 개론에서 다루는 주요 통계량을 계산하고 가설 검정을 수행하는 데 사용됩니다.

- **통계량 계산:** 평균, 중앙값(median), 최빈값(mode), 분산(variance), 첨도(kurtosis) 등을 계산합니다.
- **상관관계 및 검정:** 피어슨 상관 계수(Pearson correlation coefficient)를 계산하고, t-검정(ttest), Wilcoxon 부호 순위 검정, Kolmogorov-Smirnov 검정 등 다양한 **가설 검정(Hypothesis tests)** 기능을 제공합니다.

#### 2.1.3.2. 최적화 및 선형 대수 모듈

SciPy는 통계 외에도 다양한 분야에서 활용됩니다.

- **최적화 (`scipy.optimize`):** 일반적인 목적의 최소화(minimization)나 제한된 최소화(non-negative least-squares), 근 찾기(Root finding) 등의 최적화 문제를 해결하는 데 사용됩니다.
- **선형 대수 (`scipy.linalg`):** NumPy의 선형 대수 모듈과 유사하지만, BLAS/LAPACK 지원을 항상 사용하여 더 빠를 수 있으며 일부 추가 함수들을 제공합니다.

---
## 2.2 데이터 시각화

통계학(Statistics) 개론에서 데이터 시각화(Data Visualization)는 분석 결과를 명확하고 효율적으로 전달하며, 데이터 자체의 숨겨진 패턴과 문제점을 파악하는 데 필수적인 방법론입니다. 시각화는 통계 분석 과정의 최종 단계인 **'표현(presentation)'** 을 담당하며, 올바른 도구를 사용하여 적절한 플롯을 그리는 것이 중요합니다.

### 2.2.1. 데이터 시각화 도구: Matplotlib과 Seaborn

Python 기반의 데이터 분석 환경에서 가장 광범위하게 사용되는 시각화 도구는 Matplotlib과 이를 기반으로 하는 Seaborn입니다.

#### 2.2.1.1. Matplotlib: 시각화의 기본 엔진

Matplotlib은 Python에서 **정적(static), 애니메이션(animated), 상호작용적인 시각화**를 생성하기 위한 포괄적인 라이브러리입니다.

- **주요 특징:** 출판 품질의 그래프를 생성할 수 있으며, 확대, 축소, 업데이트가 가능한 상호작용적인 그림 제작을 지원합니다. 시각적 스타일과 레이아웃을 사용자가 정의할 수 있습니다.
- **입력 자료:** 기본적으로 **NumPy array**를 입력으로 받지만, Pandas의 데이터나 Dictionary도 `data` 키워드를 사용하여 그릴 수 있습니다.
- **권장 코딩 스타일:** 복잡한 그림을 그릴 때는 명시적으로 Figure와 Axes를 생성하고 그 위에 메서드를 호출하는 **객체 지향(OO style)** 방식이 권장됩니다.

#### 2.2.1.2. Seaborn: 통계적 시각화에 특화된 도구

Seaborn은 Matplotlib을 기반으로 개발되었으며, **매력적이고 유익한 통계 그래픽**을 그리기 위한 하이 레벨 인터페이스를 제공합니다.

- **통계적 추정 기능:** Seaborn은 통계량에 대한 **신뢰 구간(confidence interval)을 부트스트래핑(bootstrapping)으로 추정**하여 **오류 막대(error bar)**를 그려주는 등의 통계적 추정 기능을 제공합니다.
- **회귀 모델 적합:** 데이터셋의 조건부 부분 집합 전반에 걸쳐 회귀 모델을 적합시키는 데 편리한 인터페이스를 제공하는 것을 목표로 합니다.

### 2.2.2. 시각화의 목적: 데이터의 특성과 목적에 맞는 Plot 선택

시각화의 가장 중요한 원칙은 **데이터의 특성과 보여주기 위한 목적에 맞는 Plot을 그리는 것**입니다. 이는 기술 통계학(Descriptive Statistics)의 중요한 한 분야로, 자료의 특징을 요약하고 해석하는 데 사용됩니다.

|시각화 종류|주요 목적|데이터 유형|주의사항/원칙|
|:--|:--|:--|:--|
|**산점도 (Scatter Plot)**|두 양적 변수 간의 **관계** 관찰 및 군집, 이상치 파악.|이차원 양적 변수 두 개.|관계(Correlation)가 반드시 인과관계(Causation)를 의미하지 않음에 유의해야 합니다.|
|**막대그래프 (Bar Plot)**|이산형 또는 질적 자료의 **개수**를 나타내거나 그룹 간의 **수치 비교**.|범주형 특징 + 수치 값.|일반적으로 **0을 기준선(zero-valued baseline)**으로 사용하며, 범주 순서가 없다면 값의 크기 순으로 정렬하는 것이 이해하기 쉽습니다.|
|**히스토그램 (Histogram)**|수치 변수의 **분포(Distribution)**를 시각화.|수치 변수.|각 막대는 값의 범위인 **Bin**을 나타내며, 적절한 Bin의 개수를 선택해야 합니다.|
|**선도표 (Line Chart)**|X축의 연속적인 변화(예: 시간)에 따른 Y축 값의 **변화 추이**나 **기울기** 강조.|X축(연속적 값), Y축(변화 값).|하나의 Plot에 너무 많은 선(Line)이 있으면 해석이 어려우므로 **5개 이하의 Line**이 권장됩니다.|
|**원 도표 (Pie Chart)**|질적 자료에서 각 범주가 전체에서 차지하는 **비중** 비교.|질적 변수.|**Annotation**을 포함해야 비율을 정확히 알 수 있으며, 조각이 너무 많으면 읽기 어려우므로 **5개 이상인 경우 다른 차트를 사용**하거나 'other'로 묶는 것을 고려해야 합니다.|
|**히트맵 (Heatmap)**|두 축 변수에 걸친 주요 변수의 값을 **색상**으로 표현하여 **관계 패턴** 관찰.|두 축(범주 또는 Numeric Binning) + Cell Value (수치).|**범례(Legend)**를 포함하여 색상과 값 매핑에 대한 설명을 제공해야 합니다.|

### 2.2.3. 박스 플롯 (Box Plot)의 역할과 이상치 탐지

박스 플롯은 데이터의 **분포(Distribution)**를 시각적으로 요약하고 특히 전처리(Preprocessing) 단계에서 중요한 **이상치(Outlier)**를 파악하는 데 유용한 도구입니다.

#### 2.2.3.1. 박스 플롯의 구성

박스 플롯(Box and whisker plot)은 상자와 선을 사용하여 하나 이상의 그룹에 대한 수치 데이터의 분포를 나타냅니다.

- **상자 (Box):** 전체 데이터의 **50%**를 나타냅니다.
- **중앙선:** Q2, 즉 **중앙값(Median)**의 값에 표시됩니다.
- **이상치(Outlier):** 박스 플롯은 **Q1 – 1.5*IQR에서 Q3 + 1.5*IQR 범위를 넘어가는 값**을 이상치로 표기합니다. (IQR은 사분위수 범위, 즉 Q3-Q1입니다).

#### 2.2.3.2. 이상치 탐지 도구로서의 활용

이상치(Outlier)는 나머지 관찰값과 동떨어진 값으로, 데이터의 변동성이나 실험적 오류로 인해 발생하며 통계 분석에 영향을 미칩니다. 박스 플롯은 이러한 이상치를 **시각적 검사(Visual inspection)**를 통해 쉽게 탐지할 수 있도록 돕는 그래픽 진단 도구입니다. 이상치가 발견되면 분석가는 이를 제거(Trimming)하거나 특정 값으로 제한(Capping)하는 전처리 작업을 수행할 수 있습니다.

### 2.2.4. 시각화의 중요성 (역사적 교훈)

데이터 시각화는 단순히 분석 결과를 예쁘게 포장하는 것을 넘어, **의사 결정을 이끌어내는 힘**을 가집니다.

- **나이팅게일의 장미 도표:** 크림 전쟁(1853~1856) 당시 플로렌스 나이팅게일은 데이터를 분석하고 장미 도표를 사용하여, **전투에 의한 사망자보다 병원 환경(질병)에 의한 사망자가 더 많다**는 사실을 시각적으로 전달했습니다. 이 활동은 위생의 중요성을 강조했고, 그 결과 병원 내 사망률을 42%에서 2%로 크게 줄이는 데 기여했습니다. 이 사례는 시각화가 정책 변화에 미치는 중요성을 명확히 보여줍니다.

---
## 2.3 기술 통계 및 데이터 특성

통계학 개론에서 데이터의 특성을 파악하는 것은 분석의 가장 기본적인 단계입니다. 자료를 수집하고 난 후, 이 자료가 어떤 모습을 하고 있는지 요약하고 정리하는 방법론을 통틀어 **기술 통계 및 데이터 특성도**라고 부릅니다.

### 2.3.1. 기술 통계학 (Descriptive Statistics)의 역할

**기술 통계학(Descriptive Statistics)** 은 수집한 자료들을 정리하고 요약하여 자료가 어떤 특성을 갖고 있는지 **정량적으로 설명하고 해석**하는 통계학의 한 분야입니다. 이는 데이터를 이해하기 위한 첫 단계이며, 다음과 같은 방법으로 요약됩니다:

- **시각화를 통한 요약:** 산포도(Scatter Plot)나 분포도(Histogram) 등을 이용합니다.
- **통계량을 이용한 요약:** 평균 학점이나 표준편차 등의 숫자를 계산합니다.

기술 통계학이 **수집한 자료 자체**를 설명하는 반면, **추측 통계학(Inferential Statistics)**은 이러한 요약 통계량(표본)을 바탕으로 더 큰 **모집단(Population)의 속성을 추론**하는 것을 목표로 합니다.

### 2.3.2. 모수(Parameter) vs. 통계량(Statistic)

기술 통계학이 요약한 값들은 그것이 표본(Sample)에서 나왔는지, 전체 모집단(Population)에서 나왔는지에 따라 이름이 달라집니다.

- **모수 (Parameter):** **전체 모집단**을 설명하는 숫자입니다. 우리가 궁극적으로 알고자 하는 대상 전체(모집단)의 특성을 나타냅니다 (예: 모집단 평균, 모집단 표준편차).
- **통계량 (Statistic):** **표본**을 설명하는 숫자입니다. 표본 통계량은 모집단의 본질적인 통계적 특징을 만족할 만큼 보존해야 합니다 (예: 표본 평균, 표본 표준편차).

통계적 추론에서는 **표본 통계량(sample statistics)으로부터 모집단 모수(population parameters)를 추정(estimation)**하게 됩니다. 정확한 추정을 위해서는 표본이 모집단을 잘 나타내도록 **무작위 선택(random selection)**을 해야 하며, 이를 통해 편향되지 않은 추정(Unbiased estimation)이 가능해집니다.

### 2.3.3. 집중 경향 측정 (Measures of Central Tendency)

자료의 특징을 요약하는 첫 번째 방법은 데이터가 어느 한가운데(중심)에 모여 있는지를 측정하는 것입니다. 이를 **집중 경향(Central Tendency)** 또는 **위치 추정값(Estimates of Location)**이라고 합니다.

|측정 항목|설명|특징 및 용도|
|:--|:--|:--|
|**평균 (Mean)**|자료의 모든 값을 더하여 개수로 나눈 값.|가장 일반적으로 사용되나, 극단적인 값(outlier)에 **민감(sensitive)**합니다.|
|**중앙값 (Median)**|자료를 크기 순으로 정렬했을 때 정중앙에 위치하는 값 (50번째 백분위수).|평균보다 이상치(outlier)에 대해 **강건(robust)**하여 자료의 값에 덜 민감합니다 (예: 평균의 함정).|
|**최빈값 (Mode)**|자료에서 가장 자주 발생하는 값.||
|**절사 평균 (Trimmed Mean)**|극단적으로 크거나 작은 값의 영향을 제거하기 위해 양 끝의 일정 비율을 제외하고 구한 평균 (예: 올림픽 채점).|중앙값처럼 이상치에 대해 강건한(robust) 추정값입니다.|
|**가중 평균 (Weighted Mean)**|각 변수의 중요도(가중치)가 다를 경우 사용합니다 (예: 정확도가 다른 센서 값 평균).||

**왜도(Skewness):** 집중 경향을 보완하여 데이터 분포의 **비대칭성**을 나타냅니다.

- 값이 -0.5와 0.5 사이면 상당히 대칭적입니다.
- 값이 -1보다 작거나 1보다 크면 고도로 비대칭적입니다.

### 2.3.4. 변동성 측정 (Estimates of Variability)

집중 경향이 자료의 '위치'를 나타낸다면, **변동성(Variability)** 은 자료 값들이 얼마나 흩어져 있거나(spread out) 또는 밀집되어 있는지(tightly clustered)를 측정하는 두 번째 차원입니다. 이를 **산포도(Dispersion)** 라고도 합니다.

- **분산 (Variance):** 자료가 평균과 얼마나 멀리 떨어져 있는지를 측정하여 자료가 얼마나 퍼져 있는지를 나타냅니다.
- **표준편차 (Standard Deviation):** 분산에 루트를 씌운 값으로, 자료가 평균을 중심으로 얼마나 퍼져 있는지를 원 단위 그대로 나타냅니다.
- **중앙값 절대 편차 (Median Absolute Deviation):** 중앙값에 대해 계산하며, 이상치에 강건한(robust) 변동성 추정값입니다.
- **첨도 (Kurtosis):** 데이터 분포가 정규분포와 비교했을 때 얼마나 **뾰족한 정도**인지를 나타냅니다.

### 2.3.5. 백분위수 기반 측정 (Percentiles)

정렬(sorted)된 자료를 기반으로 하는 통계량을 **순서 통계량(Order statistics)**이라고 합니다. 백분위수는 데이터의 극단적인 값에 민감한 범위(Range)의 단점을 보완합니다.

- **P번째 백분위수 (Pth Percentile):** 전체 값의 P% 이상이 해당 값 이하이고, (100-P)% 이상이 해당 값 이상인 값입니다. 중앙값은 50번째 백분위수와 동일합니다.
    
- **사분위수 (Quartile):** 자료를 정렬하고 25%, 50%, 75%, 100% 위치의 값을 의미합니다.
    
    - Q1 (제1 사분위수): 25번째 백분위수.
    - Q3 (제3 사분위수): 75번째 백분위수.
- **사분위수 범위 (Interquartile Range, IQR):** 자료의 분산 정도를 측정하는 방법 중 하나로, **IQR = Q3 - Q1**으로 계산됩니다. IQR이 크면 데이터가 흩어져 있을 가능성이 높습니다.
    

#### 박스 플롯 (Box Plot)

**박스 플롯(Box Plot)**은 이러한 백분위수 기반 측정을 시각적으로 나타내는 도구입니다.

- **용도:** 하나 이상의 그룹에 대한 데이터 분포를 나타내며, 그룹/범주 간의 비교, 왜도, 분산, 그리고 **이상치(Outlier) 파악**에 유용합니다.
- **이상치 정의:** 박스 플롯에서 **Q1 – 1.5*IQR ~ Q3 + 1.5*IQR 범위**를 벗어나는 값은 이상치로 표기됩니다.

### 2.3.6. 심슨의 역설 (Simpson's Paradox)

기술 통계적 분석 결과가 직관과 다를 수 있음을 보여주는 중요한 개념입니다.

- **정의:** **심슨의 역설(Simpson's Paradox)** 은 확률 및 통계에서 하나의 추세가 여러 데이터 그룹에서는 나타나지만, 그 그룹들을 **합쳤을 때 추세가 사라지거나 정반대로 나타나는 현상**을 말합니다.
- **발생 분야:** 주로 사회 과학 및 의료 통계에서 자주 발견됩니다.
- **시사점:** 단순하게 데이터를 분석하다 보면 잘못된 결과가 나올 수 있음을 의미하며, 데이터가 **만들어진 배경 지식(knowledge)**에 대한 깊은 이해가 필수적임을 강조합니다.

---
## 2.4 데이터 전처리 (Data Preprocessing)

통계학 개론에서 **데이터 전처리(Data Preprocessing)** 는 분석의 정확성과 신뢰도를 확보하기 위한 가장 중요하고 시간이 많이 소요되는 단계입니다. 실제 문제를 해결할 때 벤치마크 데이터와 달리 정제되지 않은 데이터(Garbage in)를 다루어야 하므로, 전처리는 올바른 결과(Garbage out 방지)를 얻기 위해 필수적입니다.

다음은 전처리의 개요와 핵심적인 기법들입니다.

### 2.4.1. 데이터 전처리 (Data Preprocessing) 개요

#### 2.4.1.1. 전처리의 정의와 필요성

데이터 전처리는 **원시 데이터(raw data)를 재구성하거나 구조화**하여 데이터 마이닝이 전략적 정보를 효율적이고 쉽게 검색할 수 있도록 하는 데이터 준비 기술입니다.

- **실제 데이터의 문제점:** 보통 프로그래밍 수업이나 튜토리얼에서 제공되는 데이터는 잘 정제되어 있지만, 실제 문제 해결을 위한 데이터는 값이 없거나(결측값), 중복이 있거나, 단위 통일이 안 되어 있거나, 노이즈가 존재하는 경우가 많습니다.
- **소요 시간:** 데이터 분석을 할 때 논리만큼이나 **빠르게 구현하고 처리할 수 있는 프로그래밍 능력**이 중요한데, 전처리 과정이 전체 분석 시간 중 **가장 오래 걸리는 부분** 중 하나입니다.

#### 2.4.1.2. 주요 전처리 기법

전처리에는 주로 다음 세 가지 주요 영역이 포함됩니다:

1. **결측값 처리 (Handling Missing Data)**
2. **데이터 변환 (Data Transformation)**
3. 문자열 조작 (String Manipulation)

### 2.4.2. 결측값 처리 (Handling Missing Data)

결측값(Missing Data)은 다양한 이유(응답 거부, 측정 불가능, 기기 오류, 기술적 한계 등)로 측정 데이터의 값이 비어 있는 경우를 말합니다. 결측값을 처리하지 않으면 평균, 분산 계산이나 추후 모델을 만들 때 오류가 발생할 수 있습니다.

#### 2.4.2.1. 결측값 발생 메커니즘

결측값은 발생 유형에 따라 세 가지로 구분될 수 있습니다:

1. **완전 무작위 결측 (Missing Completely At Random, MCAR):** 결측 패턴이 결측값 자체나 측정된 다른 변수의 값과 독립적인 경우 (예: 20개 음료 중 무작위로 4개만 평가하도록 한 맛 연구).
2. **무작위 결측 (Missing At Random, MAR):** 관찰된 변수들을 조건으로 할 때, 결측 패턴이 결측값 자체와 독립적인 경우 (예: 젊은 참가자가 은퇴 저축에 대한 질문에 덜 응답하지만, 이는 관찰된 변수(나이)와 관련 있고 실제 저축액과는 직접 관련이 없는 경우).
3. **비무작위 결측 (Missing Not At Random, MNAR):** 결측 패턴이 측정된 변수를 보정했음에도 불구하고 결측값 자체와 관련된 경우 (예: 고소득자가 소득 보고를 덜 하는 경우).

#### 2.4.2.2. 결측값 처리 방법 (Handling)

결측값 처리에는 크게 삭제(Deletion)와 대체(Imputation) 방법이 있습니다.

|처리 방법|세부 내용|장단점 및 고려 사항|
|:--|:--|:--|
|**삭제 (Deletion)**|결측값이 있는 행(sample) 또는 열(attribute)을 제거.|가장 쉽지만, 데이터셋 크기가 줄어들고, 데이터가 비싼 경우(예: 생물 실험 데이터) 사용하기 어려움. 입력 특징 공간에 편향을 유발할 수 있음.|
|**단일 대체 (Single Imputation)**|각 결측값을 하나의 숫자로 대체.|**숫자형:** 열의 평균(mean) 또는 중앙값(median)으로 대체. **범주형:** 열의 최빈값(most frequent value)으로 대체.|
|**다중 대체 (Multiple Imputation)**|관찰되지 않은 변수들의 분포를 가정하고 반복적으로 표본 추출하여 여러 개의 대체된 데이터셋을 만듦.|단일 대체보다 추정된 매개변수나 예측의 불확실성을 더 잘 설명할 수 있음.|
|**추론 기반 대체**|회귀(Regression) 또는 분류(Classification) 모델을 사용하여 다른 예측 변수들로부터 결측값을 추정.||

#### 2.4.2.3. Pandas를 활용한 결측값 처리

Pandas 라이브러리는 결측값 처리에 필수적입니다.

- **삭제 기능:**
    - `axis=0`: 결측값이 포함된 **행(Row)** 을 제거.
    - `axis=1`: 결측값이 포함된 **열(Column)** 을 제거.
    - `thresh`: 행이나 열이 적어도 $x$개 이상의 non-null 값을 가질 경우에만 유지(keep).
- **대체 기능:**
    - 숫자형 열의 결측값은 **평균 값(average value)** 으로, 범주형 열의 결측값은 **가장 빈번한 값(most frequent value)** 으로 대체할 수 있습니다.
- **Scikit-learn 활용:** Pandas와 함께 `sklearn.impute.SimpleImputer`를 사용하여 숫자형은 평균(mean), 범주형은 최빈값(most_frequent)으로 대체하는 작업도 가능합니다.

### 2.4.3. 데이터 변환 (Data Transformation)

데이터 변환은 효율적인 정보 검색을 위해 원시 데이터를 재구성하는 기술입니다.

- **중복 제거 (Removing Duplicates):** 중복되는 데이터를 확인하고 제거하는 기능.
- **함수나 매핑을 사용한 변환 (Transforming Data Using a Function or Mapping):** 데이터를 특정 규칙에 따라 변환하거나 값을 교체하는 작업.
- **축 인덱스 이름 변경 (Renaming Axis Indexes):** 행/열 인덱스의 이름을 변경.
- **이산화 및 Binning (Discretization and Binning):** 연속적인 데이터를 분석을 위해 이산적으로 나누거나 'Bin'으로 분할하는 작업입니다 (예: 사람들의 나이 데이터를 이산적인 연령 구간으로 그룹화).
- **지표/더미 변수 계산 (Computing Indicator/Dummy Variables):** 범주형 변수를 모델링에 사용하기 위해 0과 1로 구성된 더미 변수로 변환하는 작업.

### 2.4.4. 데이터 집계 및 정규화 (Data Aggregation and Normalization)

#### 2.4.4.1. 데이터 집계 (Data Aggregation)

데이터 집계는 여러 데이터 소스에서 얻은 데이터를 요약된 형식으로 저장하고 제시하는 방법입니다. 데이터 분석 통찰력의 정확도는 사용되는 데이터의 양과 품질에 크게 의존하기 때문에 이 단계는 중요합니다.

#### 2.4.4.2. 데이터 정규화 (Data Normalization)

데이터 정규화는 데이터 값들을 $[-1, 1]$이나 $[0.0, 1.0]$과 같은 **훨씬 작은 범위로 스케일링**하는 것을 의미합니다.

- **Min-Max 정규화 (Min-max normalization)**
- **Z-점수 정규화 (Z-score normalization)**

### 2.4.5. 이상 감지 및 필터링 (Detecting and Filtering Outliers)

#### 2.4.5.1. 이상치(Outlier)의 정의 및 발생

- **정의:** 이상치는 주어진 데이터셋에서 **나머지 관찰값들과 멀리 떨어져 있는 관찰값**입니다. 즉, 나머지 값들에 비해 월등히 크거나 작은 값입니다.
- **발생 원인:** 데이터의 변동성 또는 실험적/인적 오류로 인해 발생할 수 있습니다.
- **영향:** 통계 분석 결과에 큰 왜곡을 줄 수 있습니다.

#### 2.4.5.2. 이상치 감지 방법

이상치를 감지하는 방법에는 여러 가지가 있습니다.

1. **시각적 검사 (Visual inspection):** 기술 통계량을 보거나 **박스 플롯(Boxplot)**과 같은 그래픽 진단 도구를 사용하는 방법.
2. **Z-점수 (Z-score):** $(X_i - \text{평균}) / \text{표준편차}$ 공식을 사용하여 Z-점수를 계산하고, 임계값(예: 3)을 설정하여 절댓값이 임계값을 초과하는 데이터 포인트를 이상치로 표시.
3. **사분위수 범위 (Interquartile Range, IQR):**
    - 데이터를 오름차순으로 정렬하고 Q1(1사분위수)과 Q3(3사분위수)를 계산합니다.
    - $\text{IQR} = Q3 - Q1$을 계산합니다.
    - **하한($Q1 - 1.5 \times \text{IQR}$)보다 작거나 상한($Q3 + 1.5 \times \text{IQR}$)보다 큰 값**을 이상치로 표시.

#### 2.4.5.3. 이상치 처리 (Handling)

이상치가 감지되면 다음 방법으로 처리할 수 있습니다:

- **제거 (Trimming/removing):** 데이터셋에서 이상치를 완전히 제거합니다.
- **제한 (Flooring and Capping):** 이상치를 90번째 백분위수 이상 혹은 10번째 백분위수 이하의 특정 값으로 제한합니다 (예: 이상치가 너무 크면 90th percentile 값으로 대체).